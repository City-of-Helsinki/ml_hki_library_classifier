{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# default_exp model\\n# %load_ext lab_black\\n\\n# nb_black if running in jupyter\\n%load_ext nb_black\\n\\n%load_ext autoreload\\n# automatically reload python modules if there are changes in the\\n%autoreload 2\";\n",
       "                var nbb_formatted_code = \"# default_exp model\\n# %load_ext lab_black\\n\\n# nb_black if running in jupyter\\n%load_ext nb_black\\n\\n%load_ext autoreload\\n# automatically reload python modules if there are changes in the\\n%autoreload 2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# default_exp model\n",
    "# %load_ext lab_black\n",
    "\n",
    "# nb_black if running in jupyter\n",
    "%load_ext nb_black\n",
    "\n",
    "%load_ext autoreload\n",
    "# automatically reload python modules if there are changes in the\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# hide\\nfrom nbdev.showdoc import *\";\n",
       "                var nbb_formatted_code = \"# hide\\nfrom nbdev.showdoc import *\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "> In this notebook we create and test our machine learning model. The output should be a Python class, but we start by just creating general python functions that we use for our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***input***: toy dataset from data-notebook\n",
    "\n",
    "***output***: python module containing ML model class or a set of general Python functions\n",
    "\n",
    "***description:***\n",
    "\n",
    "In this notebook we hypothetize, explain and explore machine learning models to solve our problem.\n",
    "\n",
    "This notebook contains an example ML model for classifying the library classification dataset with Random Forest Classifier or some other sklearn-classifier.\n",
    "\n",
    "*Template notes:*\n",
    "*Adjust the running number, name, header and top cell `#default_exp module_name` of the notebooks accordingly. Remember to add `# export` to top of all cells containing functions or classes that you have defined and want to use outside this notebook.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# export\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.linear_model import LogisticRegression\\nfrom pandas.api.types import CategoricalDtype\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.metrics import f1_score\\nfrom sklearn.metrics import log_loss\\n\\nfrom sklearn.model_selection import (\\n    GridSearchCV,\\n    cross_val_score,\\n    train_test_split,\\n    StratifiedKFold,\\n)\\nfrom sklearn.pipeline import Pipeline, make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import mean_squared_error\\n\\n# imports\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.ensemble import ExtraTreesClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.svm import LinearSVC\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn import preprocessing\";\n",
       "                var nbb_formatted_code = \"# export\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.linear_model import LogisticRegression\\nfrom pandas.api.types import CategoricalDtype\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.metrics import f1_score\\nfrom sklearn.metrics import log_loss\\n\\nfrom sklearn.model_selection import (\\n    GridSearchCV,\\n    cross_val_score,\\n    train_test_split,\\n    StratifiedKFold,\\n)\\nfrom sklearn.pipeline import Pipeline, make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import mean_squared_error\\n\\n# imports\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.ensemble import ExtraTreesClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.svm import LinearSVC\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn import preprocessing\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# FIX THIS!!!!\\n\\n# Use 'nbdev_build_lib' shell command to update library\\n# from ml_project_template.plot import plot_trellis, plot_histogram\";\n",
       "                var nbb_formatted_code = \"# FIX THIS!!!!\\n\\n# Use 'nbdev_build_lib' shell command to update library\\n# from ml_project_template.plot import plot_trellis, plot_histogram\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FIX THIS!!!!\n",
    "\n",
    "# Use 'nbdev_build_lib' shell command to update library\n",
    "# from ml_project_template.plot import plot_trellis, plot_histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define notebook parameters\n",
    "\n",
    "Remember, only simple assignments here!\n",
    "\n",
    " - **toy_data_file** = File location of a small toy dataset file created when 00_dta-notebook is executed, e.g. *data/preprocessed_data/dataset_toy_all_classes.csv*\n",
    " - **all_classes_data_file** = File location of a full dataset for training the selected algorithm, e.g. *data/preprocessed_data/dataset_clean_all_classes.csv*\n",
    " - **input_data_file** = File location for actual input data for which we want do the predicting, e.g. *data/preprocessed_data/input_file.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# Parameters\\n\\n# this cell is tagged with 'parameters'\\ntoy_data_file = \\\"data/preprocessed_data/dataset_toy_all_classes.csv\\\"\\nall_classes_data_file = \\\"data/preprocessed_data/dataset_clean_all_classes.csv\\\"\\ninput_data_file = \\\"data/preprocessed_data/input_file.csv\\\"\\nseed = 0\";\n",
       "                var nbb_formatted_code = \"# Parameters\\n\\n# this cell is tagged with 'parameters'\\ntoy_data_file = \\\"data/preprocessed_data/dataset_toy_all_classes.csv\\\"\\nall_classes_data_file = \\\"data/preprocessed_data/dataset_clean_all_classes.csv\\\"\\ninput_data_file = \\\"data/preprocessed_data/input_file.csv\\\"\\nseed = 0\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters\n",
    "\n",
    "# this cell is tagged with 'parameters'\n",
    "toy_data_file = \"data/preprocessed_data/dataset_toy_all_classes.csv\"\n",
    "all_classes_data_file = \"data/preprocessed_data/dataset_clean_all_classes.csv\"\n",
    "input_data_file = \"data/preprocessed_data/input_file.csv\"\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make immediate derivations from the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"np.random.seed(seed)\";\n",
       "                var nbb_formatted_code = \"np.random.seed(seed)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import toy data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>084</th>\n",
       "      <th>092</th>\n",
       "      <th>093</th>\n",
       "      <th>094</th>\n",
       "      <th>095</th>\n",
       "      <th>650</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4601</th>\n",
       "      <td>420908822165</td>\n",
       "      <td>78.89110</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>788.330</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98805</th>\n",
       "      <td>420908631171</td>\n",
       "      <td>99.10000</td>\n",
       "      <td>99.1000</td>\n",
       "      <td>99.100</td>\n",
       "      <td>99.1000</td>\n",
       "      <td>990.100</td>\n",
       "      <td>suomalaiset,taidemaalarit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22333</th>\n",
       "      <td>420907981954</td>\n",
       "      <td>78.46200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>784.142</td>\n",
       "      <td>perinnemusiikki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>420908153948</td>\n",
       "      <td>69.30000</td>\n",
       "      <td>69.3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.3000</td>\n",
       "      <td>675.800</td>\n",
       "      <td>markkinointitutkimus,markkinointi,tietojärjest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59116</th>\n",
       "      <td>420908158377</td>\n",
       "      <td>68.20000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.2000</td>\n",
       "      <td>691.100</td>\n",
       "      <td>ruokaohjeet,ruoanvalmistus,pula-ajat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>420908925952</td>\n",
       "      <td>59.31000</td>\n",
       "      <td>59.3100</td>\n",
       "      <td>59.310</td>\n",
       "      <td>59.3100</td>\n",
       "      <td>696.100</td>\n",
       "      <td>hudvård,kvinnor,naturliga ämnen,näring,massage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71997</th>\n",
       "      <td>420908390431</td>\n",
       "      <td>15.90000</td>\n",
       "      <td>15.9000</td>\n",
       "      <td>15.900</td>\n",
       "      <td>15.9000</td>\n",
       "      <td>192.000</td>\n",
       "      <td>kummitukset,yliluonnolliset olennot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31844</th>\n",
       "      <td>420908970852</td>\n",
       "      <td>25.50000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>258.000</td>\n",
       "      <td>diakonia,järjestöt,historia,uskonnolliset järj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35847</th>\n",
       "      <td>420908722756</td>\n",
       "      <td>62.51100</td>\n",
       "      <td>62.5110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.5110</td>\n",
       "      <td>624.800</td>\n",
       "      <td>museoajoneuvot,järjestöt,entistäminen,autot,hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52747</th>\n",
       "      <td>420908582677</td>\n",
       "      <td>75.72000</td>\n",
       "      <td>75.7200</td>\n",
       "      <td>75.720</td>\n",
       "      <td>75.7200</td>\n",
       "      <td>756.200</td>\n",
       "      <td>fotografering,digitalteknik,digitalkameror,bil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12885</th>\n",
       "      <td>420908629131</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.8000</td>\n",
       "      <td>462.000</td>\n",
       "      <td>luonnonvalokuvaus,valokuvaajat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74909</th>\n",
       "      <td>420908138656</td>\n",
       "      <td>79.18100</td>\n",
       "      <td>79.1810</td>\n",
       "      <td>79.181</td>\n",
       "      <td>79.1810</td>\n",
       "      <td>793.400</td>\n",
       "      <td>karate,budolajit,zenbuddhalaisuus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28171</th>\n",
       "      <td>420908796321</td>\n",
       "      <td>89.62000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.620</td>\n",
       "      <td>89.6200</td>\n",
       "      <td>865.000</td>\n",
       "      <td>turism,italienska,svenska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65154</th>\n",
       "      <td>420908780438</td>\n",
       "      <td>92.70000</td>\n",
       "      <td>92.7000</td>\n",
       "      <td>92.700</td>\n",
       "      <td>92.7000</td>\n",
       "      <td>927.000</td>\n",
       "      <td>liput,historia,symboliikka,kansallisliput,merk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19300</th>\n",
       "      <td>420909205498</td>\n",
       "      <td>78.71300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>786.310</td>\n",
       "      <td>orkesterimusiikki,sarjat,valssit,1900-luku,sov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50159</th>\n",
       "      <td>420908332793</td>\n",
       "      <td>56.40000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.4000</td>\n",
       "      <td>576.000</td>\n",
       "      <td>genetik,genteknik,DNA,människan,evolution,evol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17925</th>\n",
       "      <td>420908813350</td>\n",
       "      <td>23.60000</td>\n",
       "      <td>23.6000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.6000</td>\n",
       "      <td>236.000</td>\n",
       "      <td>profetiat,eskatologia,näyt,saarnaajat,saarnat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41055</th>\n",
       "      <td>420907871403</td>\n",
       "      <td>89.64000</td>\n",
       "      <td>89.6400</td>\n",
       "      <td>89.640</td>\n",
       "      <td>89.6400</td>\n",
       "      <td>868.000</td>\n",
       "      <td>multimedia,portugalin kieli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59248</th>\n",
       "      <td>420908539161</td>\n",
       "      <td>48.10000</td>\n",
       "      <td>48.1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.1000</td>\n",
       "      <td>420.000</td>\n",
       "      <td>yhteiskuntakuvaus,tapakulttuuri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55736</th>\n",
       "      <td>420908918184</td>\n",
       "      <td>89.50000</td>\n",
       "      <td>89.5000</td>\n",
       "      <td>89.500</td>\n",
       "      <td>89.5000</td>\n",
       "      <td>875.000</td>\n",
       "      <td>englannin kieli,aikuisopiskelu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57553</th>\n",
       "      <td>420908642216</td>\n",
       "      <td>78.89110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>788.330</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37356</th>\n",
       "      <td>420908913237</td>\n",
       "      <td>55.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>490.000</td>\n",
       "      <td>maamagnetismi,tutkimus,geofysiikka,historia,me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41703</th>\n",
       "      <td>420909213463</td>\n",
       "      <td>84.23000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>971.230</td>\n",
       "      <td>ajankuvaus,elämäntapa,moskovalaiset,yhteiskunt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25850</th>\n",
       "      <td>420907895240</td>\n",
       "      <td>78.89000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>788.900</td>\n",
       "      <td>maailmanmusiikki,kokoelmat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104017</th>\n",
       "      <td>420909221106</td>\n",
       "      <td>40.80000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>788.322</td>\n",
       "      <td>viulumusiikki,elämäntapa,muusikot,viulu,musiik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26554</th>\n",
       "      <td>420909051062</td>\n",
       "      <td>78.99300</td>\n",
       "      <td>78.9930</td>\n",
       "      <td>78.993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>788.330</td>\n",
       "      <td>muusikot,popmusiikki,rock,yhtyeet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99224</th>\n",
       "      <td>420909198020</td>\n",
       "      <td>78.89223</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>788.211</td>\n",
       "      <td>jazz,hard bop,tenorisaksofoni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71089</th>\n",
       "      <td>420908494536</td>\n",
       "      <td>78.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0000</td>\n",
       "      <td>787.311</td>\n",
       "      <td>joululaulut,joulumusiikki,kokoelmat,viihdemusi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>420908892795</td>\n",
       "      <td>92.00000</td>\n",
       "      <td>92.7000</td>\n",
       "      <td>92.700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>927.000</td>\n",
       "      <td>Lapporörelsen,Mäntsäläupproret,finska inbördes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105566</th>\n",
       "      <td>420907852406</td>\n",
       "      <td>79.84000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.840</td>\n",
       "      <td>79.8400</td>\n",
       "      <td>797.000</td>\n",
       "      <td>leikit,laululeikit,roolileikit,ulkoleikit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           record_id       084      092     093      094      095  \\\n",
       "4601    420908822165  78.89110  78.8911     NaN  78.8911  788.330   \n",
       "98805   420908631171  99.10000  99.1000  99.100  99.1000  990.100   \n",
       "22333   420907981954  78.46200      NaN     NaN      NaN  784.142   \n",
       "2969    420908153948  69.30000  69.3000     NaN  69.3000  675.800   \n",
       "59116   420908158377  68.20000      NaN     NaN  68.2000  691.100   \n",
       "1545    420908925952  59.31000  59.3100  59.310  59.3100  696.100   \n",
       "71997   420908390431  15.90000  15.9000  15.900  15.9000  192.000   \n",
       "31844   420908970852  25.50000      NaN     NaN      NaN  258.000   \n",
       "35847   420908722756  62.51100  62.5110     NaN  62.5110  624.800   \n",
       "52747   420908582677  75.72000  75.7200  75.720  75.7200  756.200   \n",
       "12885   420908629131  40.00000      NaN     NaN  40.8000  462.000   \n",
       "74909   420908138656  79.18100  79.1810  79.181  79.1810  793.400   \n",
       "28171   420908796321  89.62000      NaN  89.620  89.6200  865.000   \n",
       "65154   420908780438  92.70000  92.7000  92.700  92.7000  927.000   \n",
       "19300   420909205498  78.71300      NaN     NaN      NaN  786.310   \n",
       "50159   420908332793  56.40000      NaN     NaN  56.4000  576.000   \n",
       "17925   420908813350  23.60000  23.6000     NaN  23.6000  236.000   \n",
       "41055   420907871403  89.64000  89.6400  89.640  89.6400  868.000   \n",
       "59248   420908539161  48.10000  48.1000     NaN  48.1000  420.000   \n",
       "55736   420908918184  89.50000  89.5000  89.500  89.5000  875.000   \n",
       "57553   420908642216  78.89110      NaN     NaN  78.8911  788.330   \n",
       "37356   420908913237  55.00000      NaN     NaN      NaN  490.000   \n",
       "41703   420909213463  84.23000      NaN     NaN      NaN  971.230   \n",
       "25850   420907895240  78.89000      NaN     NaN      NaN  788.900   \n",
       "104017  420909221106  40.80000      NaN     NaN      NaN  788.322   \n",
       "26554   420909051062  78.99300  78.9930  78.993      NaN  788.330   \n",
       "99224   420909198020  78.89223      NaN     NaN      NaN  788.211   \n",
       "71089   420908494536  78.00000      NaN     NaN  78.0000  787.311   \n",
       "2902    420908892795  92.00000  92.7000  92.700      NaN  927.000   \n",
       "105566  420907852406  79.84000      NaN  79.840  79.8400  797.000   \n",
       "\n",
       "                                                      650  \n",
       "4601                                                 rock  \n",
       "98805                           suomalaiset,taidemaalarit  \n",
       "22333                                     perinnemusiikki  \n",
       "2969    markkinointitutkimus,markkinointi,tietojärjest...  \n",
       "59116                ruokaohjeet,ruoanvalmistus,pula-ajat  \n",
       "1545    hudvård,kvinnor,naturliga ämnen,näring,massage...  \n",
       "71997                 kummitukset,yliluonnolliset olennot  \n",
       "31844   diakonia,järjestöt,historia,uskonnolliset järj...  \n",
       "35847   museoajoneuvot,järjestöt,entistäminen,autot,hi...  \n",
       "52747   fotografering,digitalteknik,digitalkameror,bil...  \n",
       "12885                      luonnonvalokuvaus,valokuvaajat  \n",
       "74909                   karate,budolajit,zenbuddhalaisuus  \n",
       "28171                           turism,italienska,svenska  \n",
       "65154   liput,historia,symboliikka,kansallisliput,merk...  \n",
       "19300   orkesterimusiikki,sarjat,valssit,1900-luku,sov...  \n",
       "50159   genetik,genteknik,DNA,människan,evolution,evol...  \n",
       "17925       profetiat,eskatologia,näyt,saarnaajat,saarnat  \n",
       "41055                         multimedia,portugalin kieli  \n",
       "59248                     yhteiskuntakuvaus,tapakulttuuri  \n",
       "55736                      englannin kieli,aikuisopiskelu  \n",
       "57553                                                rock  \n",
       "37356   maamagnetismi,tutkimus,geofysiikka,historia,me...  \n",
       "41703   ajankuvaus,elämäntapa,moskovalaiset,yhteiskunt...  \n",
       "25850                          maailmanmusiikki,kokoelmat  \n",
       "104017  viulumusiikki,elämäntapa,muusikot,viulu,musiik...  \n",
       "26554                   muusikot,popmusiikki,rock,yhtyeet  \n",
       "99224                       jazz,hard bop,tenorisaksofoni  \n",
       "71089   joululaulut,joulumusiikki,kokoelmat,viihdemusi...  \n",
       "2902    Lapporörelsen,Mäntsäläupproret,finska inbördes...  \n",
       "105566          leikit,laululeikit,roolileikit,ulkoleikit  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"toy_df = pd.read_csv(toy_data_file, index_col=0)\\ntoy_df.head(30)\";\n",
       "                var nbb_formatted_code = \"toy_df = pd.read_csv(toy_data_file, index_col=0)\\ntoy_df.head(30)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_df = pd.read_csv(toy_data_file, index_col=0)\n",
    "toy_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting the model\n",
    "\n",
    "Some useful links for choosing the estimator\n",
    " - [Classifier comparison](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    " - [Choosing the right estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
    " \n",
    "We tested the following models both with toy dataset and actual dataset:\n",
    " - [Random forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforest#sklearn.ensemble.RandomForestClassifier)\n",
    " - [Support Vector(SVC)](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    " - [ExtraTrees](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)\n",
    " - [KNeighbors classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=kneighbors#sklearn.neighbors.KNeighborsClassifier)\n",
    " - [Decision tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontree#sklearn.tree.DecisionTreeClassifier)\n",
    " - [SGD classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html?highlight=sgd%20classifier#sklearn.linear_model.SGDClassifier)\n",
    " - [linear SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
    " - [Naive Bayes](https://scikit-learn.org/stable/modules/classes.html?highlight=naive%20bayes#module-sklearn.naive_bayes) \n",
    "\n",
    "\n",
    "The best result (see the later input) were obtained with Random Forest Classifier.\n",
    "## The math behind Random Forest Classifier:\n",
    "Random Forest constructs a multitude of decision trees and does the ckassification by selecting the class with most trees.\n",
    "\n",
    "$$\n",
    "Pr(Y_i=1|X_i) = {\\frac{exp(\\beta_0 + \\beta_1X_i + \\dots + \\beta_nX_n)}{1 + exp (\\beta_0 + \\beta_1X_i + \\dots + \\beta_nX_n)}}\n",
    "$$\n",
    "\n",
    "\n",
    "Worst result were obtained with Gaussian Naive Bayes Classifier.\n",
    "#### Naive Bayes:\n",
    "\n",
    "Naive Bayes is classifier assumes that features are independent of each other. Naive Bayes is used to calculate posterior probability P(c|x) from class prior probability P(c), predictor prior probability P(x) and likelihood P(x|c)\n",
    "\n",
    "$$\n",
    "P(c|x)= {\\frac{P(x|c)P(c)}{P(x)}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General python functions before constructing the model class\n",
    "\n",
    "First we create some general functions for splitting data into training and validation set, preprocessing data, fitting the data with selected model algorithm an, predicting and printing out the results etc. We use f1_score for loss function and pipe for predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gather": {
     "logged": 1635766019951
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# export\\n\\n# FIX THIS IMPORT!!!!\\n# These functions won't work if we don't define these also in this code block\\n# from lib_classification.plot import plot_trellis, plot_histogram\\n\\n\\nseed = 0\\n\\n\\n\\\"\\\"\\\"\\nLabels can't be of type float for classification. Thus we multiply floats\\nso that there are no decimals. When printing the results we do the opposite.operation\\n\\nmax_decimals tells number of possible decimals in library classification.\\nSet max_decimals to 0 if you want to omit decimals alltogether\\n\\\"\\\"\\\"\\nmax_decimals = 6\\nmultiply_factor = 10 ** max_decimals\\n\\n\\\"\\\"\\\"\\nSklearn models can't handle NaN values, replace them with suitable value, defaul = 0\\n\\\"\\\"\\\"\\nreplace_nan = 0\\n\\n\\ndef keywords_to_features(df_to_parse, keywords=[]):\\n    \\\"\\\"\\\"\\n    Parse dataframe keywords and create features of them\\n    \\\"\\\"\\\"\\n    # print(f\\\"KEYWORDS: {len(keywords)}\\\")\\n\\n    # Create keyword list\\n    if len(keywords) == 0:\\n        for i in range(len(df_to_parse)):\\n            item_keywords_str = (str)(df_to_parse.iloc[(i), 6])\\n            item_keywords_lst = item_keywords_str.split(\\\",\\\")\\n\\n            for word in item_keywords_lst:\\n                word = word.strip().lower()\\n                if word not in keywords:\\n                    keywords.append(word)\\n\\n    # Add keyword columns with keyword as a title (value will be 0 or 1 depending if the keyword belongs to the volume or not)\\n    # NOTE: \\\"A Pandas Series is like a column in a table\\\" (https://www.w3schools.com/python/pandas/pandas_series.asp)\\n    for i in range(len(keywords)):\\n        df_to_parse[keywords[i]] = pd.Series([], dtype=\\\"int64\\\")\\n        df_to_parse = df_to_parse.reset_index(drop=True)\\n\\n    # Fill features with keywords attached to item with value \\\"1\\\"\\n    for i in range(len(df_to_parse)):\\n        item_keywords_str = (str)(df_to_parse.iloc[(i), 6])\\n        item_keywords_lst = item_keywords_str.split(\\\",\\\")\\n\\n        for word in item_keywords_lst:\\n            word = word.strip().lower()\\n            if word in keywords:\\n                df_to_parse.at[i, word] = 1\\n\\n    # Drop column with comma-separated keywords and fill NaN with 0\\n    df_to_parse = df_to_parse.drop([\\\"650\\\"], axis=1)\\n    df_to_parse = df_to_parse.fillna(0)\\n\\n    # print(f\\\"DF TO PARSE SHAPE: {df_to_parse.shape}\\\")\\n    # print(f\\\"KEYWORDS: {len(keywords)}\\\")\\n    return df_to_parse, keywords\\n\\n\\ndef split_X_y(df):\\n    \\\"\\\"\\\"\\n    Split dataframe into features and labels\\n    \\\"\\\"\\\"\\n    # X = df.iloc[:, :-1]  # .to_numpy()\\n    # y = df.iloc[:, -1]  # .to_numpy()\\n\\n    # for col in df.columns:\\n    #    print(f\\\"*{col}*\\\")\\n\\n    X = df.copy().reset_index(drop=True)\\n    y = X.pop(\\\"095\\\").reset_index(drop=True)\\n\\n    return X, y\\n\\n\\ndef modify_lib_data(X, y=None):\\n    \\\"\\\"\\\"\\n    Do the needed modification for library data\\n    \\\"\\\"\\\"\\n\\n    # Sklearn GaussianNB doesn't handle NaN-values in input.\\n    # We fill the NaN values with 0.\\n    X = X.fillna(replace_nan)\\n\\n    # Change datatypes for features and labels\\n    X = X.astype(\\n        {\\n            \\\"record_id\\\": \\\"int\\\",\\n            \\\"084\\\": \\\"category\\\",\\n            \\\"092\\\": \\\"category\\\",\\n            \\\"093\\\": \\\"category\\\",\\n            \\\"094\\\": \\\"category\\\",\\n        }\\n    )\\n\\n    # for some reason y is of type Series\\n    # We need dataframe\\n    # y = y.to_frame()\\n\\n    # Convert labels from float to big integers,\\n    # Note: Type 'Category' won't work with categorization models (at least not with GaussianNB)\\n    if y is not None:\\n        y = y.multiply(multiply_factor)\\n        y = y.astype({\\\"095\\\": \\\"int\\\"})\\n\\n    return X, y\\n\\n\\ndef reverse_mod_lib_data(X, y_pred, y=None):\\n    \\\"\\\"\\\"\\n    Reverse the library data back to original format\\n    \\\"\\\"\\\"\\n\\n    # Sklearn GaussianNB doesn't handle NaN-values in input.\\n    # We fill the NaN values with 0 and now change it back\\n    X = X.replace(0, np.nan)\\n\\n    # Change datatypes for features and labels back to\\n    X = X.astype(\\n        {\\n            \\\"record_id\\\": \\\"int\\\",\\n            \\\"084\\\": \\\"float\\\",\\n            \\\"092\\\": \\\"float\\\",\\n            \\\"093\\\": \\\"float\\\",\\n            \\\"094\\\": \\\"float\\\",\\n        }\\n    )\\n\\n    # Convert labels from category back to int\\n    if y is not None:\\n        y = y.multiply(1 / multiply_factor)\\n    y_pred = y_pred.multiply(1 / multiply_factor)\\n\\n    return X, y, y_pred\\n\\n\\ndef get_train_test_data(X, y, seed, stratify=True, test_size=0.2, shuffle=True):\\n    \\\"\\\"\\\"\\n    Split the data into training and test sets\\n    \\\"\\\"\\\"\\n\\n    # Stratify won't work with all datasets, it requires at least 2 rows for each label value\\n    if stratify:\\n        return train_test_split(\\n            X, y, test_size=test_size, shuffle=shuffle, stratify=y, random_state=seed\\n        )\\n\\n    else:\\n        return train_test_split(\\n            X, y, test_size=test_size, shuffle=shuffle, random_state=seed\\n        )\\n\\n\\ndef fit(model, scaler, X_train, X_test, y_train, y_test):\\n    \\\"\\\"\\\"\\n    Fit the model\\n    \\\"\\\"\\\"\\n\\n    pipe = Pipeline([(\\\"scaler\\\", scaler), (\\\"model\\\", model)])\\n    pipe.fit(X_train, y_train)\\n    err_train = pipe.score(X_train, y_train)\\n    err_test = pipe.score(X_test, y_test)\\n\\n    return pipe\\n\\n\\ndef predict(pipe, X):\\n    \\\"\\\"\\\"\\n    Use the model (pipe object) to predict labels\\n    \\\"\\\"\\\"\\n\\n    y_pred = pipe.predict(X)\\n    # pred_probabilities = pipe.predict_proba(X)\\n\\n    # Print probabilities for first data point only\\n    # print(\\n    #    f\\\"\\\\nPredicted probability of each label for first data point:\\\\n{pred_probabilities[0]}\\\"\\n    # )\\n\\n    return y_pred\\n\\n\\ndef get_train_loss(pipe, X_train, y_train):\\n    \\\"\\\"\\\"\\n    Return train loss of fitted model\\n    \\\"\\\"\\\"\\n\\n    return pipe.score(X_train, y_train)\\n\\n\\ndef get_test_loss(pipe, X_test, y_test):\\n    \\\"\\\"\\\"\\n    Return test loss of fitted model\\n    \\\"\\\"\\\"\\n    return pipe.score(X_test, y_test)\\n\\n\\ndef loss(pipe, X, y):\\n    \\\"\\\"\\\"\\n    Return loss (model quality metric)\\n\\n    Note that this may be a different metric than the one that the model optimizer is using (scoring method).\\n    For example for LogisticRegression the scoring method is mean accuracy,\\n    but we might want to track for example f1-score for loss because it is better balanced.\\n    \\\"\\\"\\\"\\n\\n    # return mean_squared_error(predict(pipe, X), y)\\n    return f1_score(y, predict(pipe, X), average=\\\"macro\\\")\\n\\n\\ndef print_loss(pipe, X, y, X_train, y_train, X_test, y_test, model_name, dataset_name):\\n    \\\"\\\"\\\"\\n    Print training and validation errors\\n    \\\"\\\"\\\"\\n    print(\\\"\\\\n******************************************************************\\\")\\n    print(f\\\"  Results for {model_name} with {dataset_name}:\\\")\\n    print(\\\"******************************************************************\\\")\\n\\n    print(f\\\"Training error: {get_train_loss(pipe, X_train, y_train)}\\\")\\n    print(f\\\"Validation error: {get_test_loss(pipe, X_test, y_test)}\\\")\\n    print(f\\\"Loss: {loss(pipe, X, y)}\\\")\\n\\n    # train_test_df = X_train.iloc[:,1:].copy()\\n    # train_test_df[\\\"prediction_correct\\\"] = (predict(pipe, X_train) - y_train.values == 0)\\n    # display(train_test_df.head)\\n    # _ = plot_trellis(train_test_df, legend_title=\\\"prediction\\\", true_label=\\\"correct\\\")\\n\\n\\ndef print_details(X, y, y_pred, label_name=\\\"label\\\", pred_column_name=\\\"pred\\\", n_rows=10):\\n    \\\"\\\"\\\"\\n    Print the results for observation\\n    \\\"\\\"\\\"\\n\\n    y_compare = pd.concat([y, y_pred], axis=1)\\n\\n    print(f\\\"\\\\nOriginal and predicted labels (first {n_rows} rows):\\\")\\n    display(y_compare.head(n_rows))\\n\\n    X_compare = pd.concat([X, y_compare], axis=1)\\n    false_preds = X_compare[X_compare[label_name] != X_compare[pred_column_name]]\\n    n_false_preds = len(false_preds)\\n    n_right_preds = len(X_compare) - n_false_preds\\n    print(f\\\"Number of false predictions: {n_false_preds}\\\")\\n    print(f\\\"Number of right predictions: {n_right_preds}\\\")\\n    print(\\\"\\\\n\\\\nAll false predictions in dataset:\\\")\\n    display(false_preds)\\n\\n    # print(\\n    #    \\\"\\\\nHow different classifications correlate with each other on true and false predictions:\\\"\\n    # )\\n    X_compare[\\\"prediction_correct\\\"] = (\\n        X_compare[label_name] - X_compare[pred_column_name] == 0\\n    )\\n    # display(X_compare.head())\\n    # FIX THIS IMPORT!!!\\n    # _ = plot_trellis(X_compare.iloc[:,1:], legend_title=\\\"prediction\\\", true_label=\\\"correct\\\")\\n\\n\\ndef test_model(\\n    model, scaler, df, model_name, dataset_name, test_size=0.2, verbose=True\\n):\\n    \\\"\\\"\\\"\\n    Test the model with the help of functions above\\n    \\\"\\\"\\\"\\n\\n    label_name = \\\"095\\\"\\n    pred_column_name = \\\"095_PRED\\\"\\n\\n    # Create features and labels\\n    X, y = split_X_y(df)\\n\\n    # Modify library data as needed\\n    X, y = modify_lib_data(X, y)\\n\\n    # Split data into training and test sets\\n    X_train, X_test, y_train, y_test = get_train_test_data(\\n        X, y, seed, stratify=False, test_size=test_size\\n    )\\n\\n    # Fit and predict\\n    pipe = fit(model, scaler, X_train, X_test, y_train, y_test)\\n    y_pred = predict(pipe, X)\\n    score = loss(pipe, X, y)\\n\\n    # convert predictions from numpy to dataframe and set an easy column name\\n    y_pred = pd.DataFrame(y_pred)\\n    y_pred = y_pred.rename(columns={y_pred.columns[0]: pred_column_name})\\n\\n    if verbose:\\n        print_loss(\\n            pipe, X, y, X_train, y_train, X_test, y_test, model_name, dataset_name\\n        )\\n\\n    # Modify the library data back to original format\\n    X, y, y_pred = reverse_mod_lib_data(X, y_pred, y)\\n\\n    # Print the results with desired column names\\n    if verbose:\\n        print_details(X, y, y_pred, label_name, pred_column_name, 30)\\n\\n    # print(score)\\n    return pipe, score\\n\\n\\ndef predict_hkl_class(pipe, items_to_classify, info=\\\"\\\", y=None):\\n    \\\"\\\"\\\"\\n    Predict the actual HKL class\\n    \\\"\\\"\\\"\\n    pred_column_name = \\\"095_PRED\\\"\\n\\n    # Modify library data as needed\\n    X, y = modify_lib_data(items_to_classify, y)\\n\\n    y_pred = predict(pipe, X)\\n\\n    # convert predictions from numpy to dataframe and set an easy column name\\n    y_pred = pd.DataFrame(y_pred)\\n    y_pred = y_pred.rename(columns={y_pred.columns[0]: pred_column_name})\\n\\n    # Modify the library data back to original format\\n    X, y, y_pred = reverse_mod_lib_data(X, y_pred, y)\\n\\n    result = pd.concat([X, y_pred], axis=1)\\n    result[\\\"Info\\\"] = info\\n\\n    return result\";\n",
       "                var nbb_formatted_code = \"# export\\n\\n# FIX THIS IMPORT!!!!\\n# These functions won't work if we don't define these also in this code block\\n# from lib_classification.plot import plot_trellis, plot_histogram\\n\\n\\nseed = 0\\n\\n\\n\\\"\\\"\\\"\\nLabels can't be of type float for classification. Thus we multiply floats\\nso that there are no decimals. When printing the results we do the opposite.operation\\n\\nmax_decimals tells number of possible decimals in library classification.\\nSet max_decimals to 0 if you want to omit decimals alltogether\\n\\\"\\\"\\\"\\nmax_decimals = 6\\nmultiply_factor = 10 ** max_decimals\\n\\n\\\"\\\"\\\"\\nSklearn models can't handle NaN values, replace them with suitable value, defaul = 0\\n\\\"\\\"\\\"\\nreplace_nan = 0\\n\\n\\ndef keywords_to_features(df_to_parse, keywords=[]):\\n    \\\"\\\"\\\"\\n    Parse dataframe keywords and create features of them\\n    \\\"\\\"\\\"\\n    # print(f\\\"KEYWORDS: {len(keywords)}\\\")\\n\\n    # Create keyword list\\n    if len(keywords) == 0:\\n        for i in range(len(df_to_parse)):\\n            item_keywords_str = (str)(df_to_parse.iloc[(i), 6])\\n            item_keywords_lst = item_keywords_str.split(\\\",\\\")\\n\\n            for word in item_keywords_lst:\\n                word = word.strip().lower()\\n                if word not in keywords:\\n                    keywords.append(word)\\n\\n    # Add keyword columns with keyword as a title (value will be 0 or 1 depending if the keyword belongs to the volume or not)\\n    # NOTE: \\\"A Pandas Series is like a column in a table\\\" (https://www.w3schools.com/python/pandas/pandas_series.asp)\\n    for i in range(len(keywords)):\\n        df_to_parse[keywords[i]] = pd.Series([], dtype=\\\"int64\\\")\\n        df_to_parse = df_to_parse.reset_index(drop=True)\\n\\n    # Fill features with keywords attached to item with value \\\"1\\\"\\n    for i in range(len(df_to_parse)):\\n        item_keywords_str = (str)(df_to_parse.iloc[(i), 6])\\n        item_keywords_lst = item_keywords_str.split(\\\",\\\")\\n\\n        for word in item_keywords_lst:\\n            word = word.strip().lower()\\n            if word in keywords:\\n                df_to_parse.at[i, word] = 1\\n\\n    # Drop column with comma-separated keywords and fill NaN with 0\\n    df_to_parse = df_to_parse.drop([\\\"650\\\"], axis=1)\\n    df_to_parse = df_to_parse.fillna(0)\\n\\n    # print(f\\\"DF TO PARSE SHAPE: {df_to_parse.shape}\\\")\\n    # print(f\\\"KEYWORDS: {len(keywords)}\\\")\\n    return df_to_parse, keywords\\n\\n\\ndef split_X_y(df):\\n    \\\"\\\"\\\"\\n    Split dataframe into features and labels\\n    \\\"\\\"\\\"\\n    # X = df.iloc[:, :-1]  # .to_numpy()\\n    # y = df.iloc[:, -1]  # .to_numpy()\\n\\n    # for col in df.columns:\\n    #    print(f\\\"*{col}*\\\")\\n\\n    X = df.copy().reset_index(drop=True)\\n    y = X.pop(\\\"095\\\").reset_index(drop=True)\\n\\n    return X, y\\n\\n\\ndef modify_lib_data(X, y=None):\\n    \\\"\\\"\\\"\\n    Do the needed modification for library data\\n    \\\"\\\"\\\"\\n\\n    # Sklearn GaussianNB doesn't handle NaN-values in input.\\n    # We fill the NaN values with 0.\\n    X = X.fillna(replace_nan)\\n\\n    # Change datatypes for features and labels\\n    X = X.astype(\\n        {\\n            \\\"record_id\\\": \\\"int\\\",\\n            \\\"084\\\": \\\"category\\\",\\n            \\\"092\\\": \\\"category\\\",\\n            \\\"093\\\": \\\"category\\\",\\n            \\\"094\\\": \\\"category\\\",\\n        }\\n    )\\n\\n    # for some reason y is of type Series\\n    # We need dataframe\\n    # y = y.to_frame()\\n\\n    # Convert labels from float to big integers,\\n    # Note: Type 'Category' won't work with categorization models (at least not with GaussianNB)\\n    if y is not None:\\n        y = y.multiply(multiply_factor)\\n        y = y.astype({\\\"095\\\": \\\"int\\\"})\\n\\n    return X, y\\n\\n\\ndef reverse_mod_lib_data(X, y_pred, y=None):\\n    \\\"\\\"\\\"\\n    Reverse the library data back to original format\\n    \\\"\\\"\\\"\\n\\n    # Sklearn GaussianNB doesn't handle NaN-values in input.\\n    # We fill the NaN values with 0 and now change it back\\n    X = X.replace(0, np.nan)\\n\\n    # Change datatypes for features and labels back to\\n    X = X.astype(\\n        {\\n            \\\"record_id\\\": \\\"int\\\",\\n            \\\"084\\\": \\\"float\\\",\\n            \\\"092\\\": \\\"float\\\",\\n            \\\"093\\\": \\\"float\\\",\\n            \\\"094\\\": \\\"float\\\",\\n        }\\n    )\\n\\n    # Convert labels from category back to int\\n    if y is not None:\\n        y = y.multiply(1 / multiply_factor)\\n    y_pred = y_pred.multiply(1 / multiply_factor)\\n\\n    return X, y, y_pred\\n\\n\\ndef get_train_test_data(X, y, seed, stratify=True, test_size=0.2, shuffle=True):\\n    \\\"\\\"\\\"\\n    Split the data into training and test sets\\n    \\\"\\\"\\\"\\n\\n    # Stratify won't work with all datasets, it requires at least 2 rows for each label value\\n    if stratify:\\n        return train_test_split(\\n            X, y, test_size=test_size, shuffle=shuffle, stratify=y, random_state=seed\\n        )\\n\\n    else:\\n        return train_test_split(\\n            X, y, test_size=test_size, shuffle=shuffle, random_state=seed\\n        )\\n\\n\\ndef fit(model, scaler, X_train, X_test, y_train, y_test):\\n    \\\"\\\"\\\"\\n    Fit the model\\n    \\\"\\\"\\\"\\n\\n    pipe = Pipeline([(\\\"scaler\\\", scaler), (\\\"model\\\", model)])\\n    pipe.fit(X_train, y_train)\\n    err_train = pipe.score(X_train, y_train)\\n    err_test = pipe.score(X_test, y_test)\\n\\n    return pipe\\n\\n\\ndef predict(pipe, X):\\n    \\\"\\\"\\\"\\n    Use the model (pipe object) to predict labels\\n    \\\"\\\"\\\"\\n\\n    y_pred = pipe.predict(X)\\n    # pred_probabilities = pipe.predict_proba(X)\\n\\n    # Print probabilities for first data point only\\n    # print(\\n    #    f\\\"\\\\nPredicted probability of each label for first data point:\\\\n{pred_probabilities[0]}\\\"\\n    # )\\n\\n    return y_pred\\n\\n\\ndef get_train_loss(pipe, X_train, y_train):\\n    \\\"\\\"\\\"\\n    Return train loss of fitted model\\n    \\\"\\\"\\\"\\n\\n    return pipe.score(X_train, y_train)\\n\\n\\ndef get_test_loss(pipe, X_test, y_test):\\n    \\\"\\\"\\\"\\n    Return test loss of fitted model\\n    \\\"\\\"\\\"\\n    return pipe.score(X_test, y_test)\\n\\n\\ndef loss(pipe, X, y):\\n    \\\"\\\"\\\"\\n    Return loss (model quality metric)\\n\\n    Note that this may be a different metric than the one that the model optimizer is using (scoring method).\\n    For example for LogisticRegression the scoring method is mean accuracy,\\n    but we might want to track for example f1-score for loss because it is better balanced.\\n    \\\"\\\"\\\"\\n\\n    # return mean_squared_error(predict(pipe, X), y)\\n    return f1_score(y, predict(pipe, X), average=\\\"macro\\\")\\n\\n\\ndef print_loss(pipe, X, y, X_train, y_train, X_test, y_test, model_name, dataset_name):\\n    \\\"\\\"\\\"\\n    Print training and validation errors\\n    \\\"\\\"\\\"\\n    print(\\\"\\\\n******************************************************************\\\")\\n    print(f\\\"  Results for {model_name} with {dataset_name}:\\\")\\n    print(\\\"******************************************************************\\\")\\n\\n    print(f\\\"Training error: {get_train_loss(pipe, X_train, y_train)}\\\")\\n    print(f\\\"Validation error: {get_test_loss(pipe, X_test, y_test)}\\\")\\n    print(f\\\"Loss: {loss(pipe, X, y)}\\\")\\n\\n    # train_test_df = X_train.iloc[:,1:].copy()\\n    # train_test_df[\\\"prediction_correct\\\"] = (predict(pipe, X_train) - y_train.values == 0)\\n    # display(train_test_df.head)\\n    # _ = plot_trellis(train_test_df, legend_title=\\\"prediction\\\", true_label=\\\"correct\\\")\\n\\n\\ndef print_details(X, y, y_pred, label_name=\\\"label\\\", pred_column_name=\\\"pred\\\", n_rows=10):\\n    \\\"\\\"\\\"\\n    Print the results for observation\\n    \\\"\\\"\\\"\\n\\n    y_compare = pd.concat([y, y_pred], axis=1)\\n\\n    print(f\\\"\\\\nOriginal and predicted labels (first {n_rows} rows):\\\")\\n    display(y_compare.head(n_rows))\\n\\n    X_compare = pd.concat([X, y_compare], axis=1)\\n    false_preds = X_compare[X_compare[label_name] != X_compare[pred_column_name]]\\n    n_false_preds = len(false_preds)\\n    n_right_preds = len(X_compare) - n_false_preds\\n    print(f\\\"Number of false predictions: {n_false_preds}\\\")\\n    print(f\\\"Number of right predictions: {n_right_preds}\\\")\\n    print(\\\"\\\\n\\\\nAll false predictions in dataset:\\\")\\n    display(false_preds)\\n\\n    # print(\\n    #    \\\"\\\\nHow different classifications correlate with each other on true and false predictions:\\\"\\n    # )\\n    X_compare[\\\"prediction_correct\\\"] = (\\n        X_compare[label_name] - X_compare[pred_column_name] == 0\\n    )\\n    # display(X_compare.head())\\n    # FIX THIS IMPORT!!!\\n    # _ = plot_trellis(X_compare.iloc[:,1:], legend_title=\\\"prediction\\\", true_label=\\\"correct\\\")\\n\\n\\ndef test_model(\\n    model, scaler, df, model_name, dataset_name, test_size=0.2, verbose=True\\n):\\n    \\\"\\\"\\\"\\n    Test the model with the help of functions above\\n    \\\"\\\"\\\"\\n\\n    label_name = \\\"095\\\"\\n    pred_column_name = \\\"095_PRED\\\"\\n\\n    # Create features and labels\\n    X, y = split_X_y(df)\\n\\n    # Modify library data as needed\\n    X, y = modify_lib_data(X, y)\\n\\n    # Split data into training and test sets\\n    X_train, X_test, y_train, y_test = get_train_test_data(\\n        X, y, seed, stratify=False, test_size=test_size\\n    )\\n\\n    # Fit and predict\\n    pipe = fit(model, scaler, X_train, X_test, y_train, y_test)\\n    y_pred = predict(pipe, X)\\n    score = loss(pipe, X, y)\\n\\n    # convert predictions from numpy to dataframe and set an easy column name\\n    y_pred = pd.DataFrame(y_pred)\\n    y_pred = y_pred.rename(columns={y_pred.columns[0]: pred_column_name})\\n\\n    if verbose:\\n        print_loss(\\n            pipe, X, y, X_train, y_train, X_test, y_test, model_name, dataset_name\\n        )\\n\\n    # Modify the library data back to original format\\n    X, y, y_pred = reverse_mod_lib_data(X, y_pred, y)\\n\\n    # Print the results with desired column names\\n    if verbose:\\n        print_details(X, y, y_pred, label_name, pred_column_name, 30)\\n\\n    # print(score)\\n    return pipe, score\\n\\n\\ndef predict_hkl_class(pipe, items_to_classify, info=\\\"\\\", y=None):\\n    \\\"\\\"\\\"\\n    Predict the actual HKL class\\n    \\\"\\\"\\\"\\n    pred_column_name = \\\"095_PRED\\\"\\n\\n    # Modify library data as needed\\n    X, y = modify_lib_data(items_to_classify, y)\\n\\n    y_pred = predict(pipe, X)\\n\\n    # convert predictions from numpy to dataframe and set an easy column name\\n    y_pred = pd.DataFrame(y_pred)\\n    y_pred = y_pred.rename(columns={y_pred.columns[0]: pred_column_name})\\n\\n    # Modify the library data back to original format\\n    X, y, y_pred = reverse_mod_lib_data(X, y_pred, y)\\n\\n    result = pd.concat([X, y_pred], axis=1)\\n    result[\\\"Info\\\"] = info\\n\\n    return result\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "# FIX THIS IMPORT!!!!\n",
    "# These functions won't work if we don't define these also in this code block\n",
    "# from lib_classification.plot import plot_trellis, plot_histogram\n",
    "\n",
    "\n",
    "seed = 0\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Labels can't be of type float for classification. Thus we multiply floats\n",
    "so that there are no decimals. When printing the results we do the opposite.operation\n",
    "\n",
    "max_decimals tells number of possible decimals in library classification.\n",
    "Set max_decimals to 0 if you want to omit decimals alltogether\n",
    "\"\"\"\n",
    "max_decimals = 6\n",
    "multiply_factor = 10 ** max_decimals\n",
    "\n",
    "\"\"\"\n",
    "Sklearn models can't handle NaN values, replace them with suitable value, defaul = 0\n",
    "\"\"\"\n",
    "replace_nan = 0\n",
    "\n",
    "\n",
    "def keywords_to_features(df_to_parse, keywords=[]):\n",
    "    \"\"\"\n",
    "    Parse dataframe keywords and create features of them\n",
    "    \"\"\"\n",
    "    # print(f\"KEYWORDS: {len(keywords)}\")\n",
    "\n",
    "    # Create keyword list\n",
    "    if len(keywords) == 0:\n",
    "        for i in range(len(df_to_parse)):\n",
    "            item_keywords_str = (str)(df_to_parse.iloc[(i), 6])\n",
    "            item_keywords_lst = item_keywords_str.split(\",\")\n",
    "\n",
    "            for word in item_keywords_lst:\n",
    "                word = word.strip().lower()\n",
    "                if word not in keywords:\n",
    "                    keywords.append(word)\n",
    "\n",
    "    # Add keyword columns with keyword as a title (value will be 0 or 1 depending if the keyword belongs to the volume or not)\n",
    "    # NOTE: \"A Pandas Series is like a column in a table\" (https://www.w3schools.com/python/pandas/pandas_series.asp)\n",
    "    for i in range(len(keywords)):\n",
    "        df_to_parse[keywords[i]] = pd.Series([], dtype=\"int64\")\n",
    "        df_to_parse = df_to_parse.reset_index(drop=True)\n",
    "\n",
    "    # Fill features with keywords attached to item with value \"1\"\n",
    "    for i in range(len(df_to_parse)):\n",
    "        item_keywords_str = (str)(df_to_parse.iloc[(i), 6])\n",
    "        item_keywords_lst = item_keywords_str.split(\",\")\n",
    "\n",
    "        for word in item_keywords_lst:\n",
    "            word = word.strip().lower()\n",
    "            if word in keywords:\n",
    "                df_to_parse.at[i, word] = 1\n",
    "\n",
    "    # Drop column with comma-separated keywords and fill NaN with 0\n",
    "    df_to_parse = df_to_parse.drop([\"650\"], axis=1)\n",
    "    df_to_parse = df_to_parse.fillna(0)\n",
    "\n",
    "    # print(f\"DF TO PARSE SHAPE: {df_to_parse.shape}\")\n",
    "    # print(f\"KEYWORDS: {len(keywords)}\")\n",
    "    return df_to_parse, keywords\n",
    "\n",
    "\n",
    "def split_X_y(df):\n",
    "    \"\"\"\n",
    "    Split dataframe into features and labels\n",
    "    \"\"\"\n",
    "    # X = df.iloc[:, :-1]  # .to_numpy()\n",
    "    # y = df.iloc[:, -1]  # .to_numpy()\n",
    "\n",
    "    # for col in df.columns:\n",
    "    #    print(f\"*{col}*\")\n",
    "\n",
    "    X = df.copy().reset_index(drop=True)\n",
    "    y = X.pop(\"095\").reset_index(drop=True)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def modify_lib_data(X, y=None):\n",
    "    \"\"\"\n",
    "    Do the needed modification for library data\n",
    "    \"\"\"\n",
    "\n",
    "    # Sklearn GaussianNB doesn't handle NaN-values in input.\n",
    "    # We fill the NaN values with 0.\n",
    "    X = X.fillna(replace_nan)\n",
    "\n",
    "    # Change datatypes for features and labels\n",
    "    X = X.astype(\n",
    "        {\n",
    "            \"record_id\": \"int\",\n",
    "            \"084\": \"category\",\n",
    "            \"092\": \"category\",\n",
    "            \"093\": \"category\",\n",
    "            \"094\": \"category\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # for some reason y is of type Series\n",
    "    # We need dataframe\n",
    "    # y = y.to_frame()\n",
    "\n",
    "    # Convert labels from float to big integers,\n",
    "    # Note: Type 'Category' won't work with categorization models (at least not with GaussianNB)\n",
    "    if y is not None:\n",
    "        y = y.multiply(multiply_factor)\n",
    "        y = y.astype({\"095\": \"int\"})\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def reverse_mod_lib_data(X, y_pred, y=None):\n",
    "    \"\"\"\n",
    "    Reverse the library data back to original format\n",
    "    \"\"\"\n",
    "\n",
    "    # Sklearn GaussianNB doesn't handle NaN-values in input.\n",
    "    # We fill the NaN values with 0 and now change it back\n",
    "    X = X.replace(0, np.nan)\n",
    "\n",
    "    # Change datatypes for features and labels back to\n",
    "    X = X.astype(\n",
    "        {\n",
    "            \"record_id\": \"int\",\n",
    "            \"084\": \"float\",\n",
    "            \"092\": \"float\",\n",
    "            \"093\": \"float\",\n",
    "            \"094\": \"float\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Convert labels from category back to int\n",
    "    if y is not None:\n",
    "        y = y.multiply(1 / multiply_factor)\n",
    "    y_pred = y_pred.multiply(1 / multiply_factor)\n",
    "\n",
    "    return X, y, y_pred\n",
    "\n",
    "\n",
    "def get_train_test_data(X, y, seed, stratify=True, test_size=0.2, shuffle=True):\n",
    "    \"\"\"\n",
    "    Split the data into training and test sets\n",
    "    \"\"\"\n",
    "\n",
    "    # Stratify won't work with all datasets, it requires at least 2 rows for each label value\n",
    "    if stratify:\n",
    "        return train_test_split(\n",
    "            X, y, test_size=test_size, shuffle=shuffle, stratify=y, random_state=seed\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        return train_test_split(\n",
    "            X, y, test_size=test_size, shuffle=shuffle, random_state=seed\n",
    "        )\n",
    "\n",
    "\n",
    "def fit(model, scaler, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Fit the model\n",
    "    \"\"\"\n",
    "\n",
    "    pipe = Pipeline([(\"scaler\", scaler), (\"model\", model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    err_train = pipe.score(X_train, y_train)\n",
    "    err_test = pipe.score(X_test, y_test)\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def predict(pipe, X):\n",
    "    \"\"\"\n",
    "    Use the model (pipe object) to predict labels\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = pipe.predict(X)\n",
    "    # pred_probabilities = pipe.predict_proba(X)\n",
    "\n",
    "    # Print probabilities for first data point only\n",
    "    # print(\n",
    "    #    f\"\\nPredicted probability of each label for first data point:\\n{pred_probabilities[0]}\"\n",
    "    # )\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def get_train_loss(pipe, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Return train loss of fitted model\n",
    "    \"\"\"\n",
    "\n",
    "    return pipe.score(X_train, y_train)\n",
    "\n",
    "\n",
    "def get_test_loss(pipe, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Return test loss of fitted model\n",
    "    \"\"\"\n",
    "    return pipe.score(X_test, y_test)\n",
    "\n",
    "\n",
    "def loss(pipe, X, y):\n",
    "    \"\"\"\n",
    "    Return loss (model quality metric)\n",
    "\n",
    "    Note that this may be a different metric than the one that the model optimizer is using (scoring method).\n",
    "    For example for LogisticRegression the scoring method is mean accuracy,\n",
    "    but we might want to track for example f1-score for loss because it is better balanced.\n",
    "    \"\"\"\n",
    "\n",
    "    # return mean_squared_error(predict(pipe, X), y)\n",
    "    return f1_score(y, predict(pipe, X), average=\"macro\")\n",
    "\n",
    "\n",
    "def print_loss(pipe, X, y, X_train, y_train, X_test, y_test, model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Print training and validation errors\n",
    "    \"\"\"\n",
    "    print(\"\\n******************************************************************\")\n",
    "    print(f\"  Results for {model_name} with {dataset_name}:\")\n",
    "    print(\"******************************************************************\")\n",
    "\n",
    "    print(f\"Training error: {get_train_loss(pipe, X_train, y_train)}\")\n",
    "    print(f\"Validation error: {get_test_loss(pipe, X_test, y_test)}\")\n",
    "    print(f\"Loss: {loss(pipe, X, y)}\")\n",
    "\n",
    "    # train_test_df = X_train.iloc[:,1:].copy()\n",
    "    # train_test_df[\"prediction_correct\"] = (predict(pipe, X_train) - y_train.values == 0)\n",
    "    # display(train_test_df.head)\n",
    "    # _ = plot_trellis(train_test_df, legend_title=\"prediction\", true_label=\"correct\")\n",
    "\n",
    "\n",
    "def print_details(X, y, y_pred, label_name=\"label\", pred_column_name=\"pred\", n_rows=10):\n",
    "    \"\"\"\n",
    "    Print the results for observation\n",
    "    \"\"\"\n",
    "\n",
    "    y_compare = pd.concat([y, y_pred], axis=1)\n",
    "\n",
    "    print(f\"\\nOriginal and predicted labels (first {n_rows} rows):\")\n",
    "    display(y_compare.head(n_rows))\n",
    "\n",
    "    X_compare = pd.concat([X, y_compare], axis=1)\n",
    "    false_preds = X_compare[X_compare[label_name] != X_compare[pred_column_name]]\n",
    "    n_false_preds = len(false_preds)\n",
    "    n_right_preds = len(X_compare) - n_false_preds\n",
    "    print(f\"Number of false predictions: {n_false_preds}\")\n",
    "    print(f\"Number of right predictions: {n_right_preds}\")\n",
    "    print(\"\\n\\nAll false predictions in dataset:\")\n",
    "    display(false_preds)\n",
    "\n",
    "    # print(\n",
    "    #    \"\\nHow different classifications correlate with each other on true and false predictions:\"\n",
    "    # )\n",
    "    X_compare[\"prediction_correct\"] = (\n",
    "        X_compare[label_name] - X_compare[pred_column_name] == 0\n",
    "    )\n",
    "    # display(X_compare.head())\n",
    "    # FIX THIS IMPORT!!!\n",
    "    # _ = plot_trellis(X_compare.iloc[:,1:], legend_title=\"prediction\", true_label=\"correct\")\n",
    "\n",
    "\n",
    "def test_model(\n",
    "    model, scaler, df, model_name, dataset_name, test_size=0.2, verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Test the model with the help of functions above\n",
    "    \"\"\"\n",
    "\n",
    "    label_name = \"095\"\n",
    "    pred_column_name = \"095_PRED\"\n",
    "\n",
    "    # Create features and labels\n",
    "    X, y = split_X_y(df)\n",
    "\n",
    "    # Modify library data as needed\n",
    "    X, y = modify_lib_data(X, y)\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = get_train_test_data(\n",
    "        X, y, seed, stratify=False, test_size=test_size\n",
    "    )\n",
    "\n",
    "    # Fit and predict\n",
    "    pipe = fit(model, scaler, X_train, X_test, y_train, y_test)\n",
    "    y_pred = predict(pipe, X)\n",
    "    score = loss(pipe, X, y)\n",
    "\n",
    "    # convert predictions from numpy to dataframe and set an easy column name\n",
    "    y_pred = pd.DataFrame(y_pred)\n",
    "    y_pred = y_pred.rename(columns={y_pred.columns[0]: pred_column_name})\n",
    "\n",
    "    if verbose:\n",
    "        print_loss(\n",
    "            pipe, X, y, X_train, y_train, X_test, y_test, model_name, dataset_name\n",
    "        )\n",
    "\n",
    "    # Modify the library data back to original format\n",
    "    X, y, y_pred = reverse_mod_lib_data(X, y_pred, y)\n",
    "\n",
    "    # Print the results with desired column names\n",
    "    if verbose:\n",
    "        print_details(X, y, y_pred, label_name, pred_column_name, 30)\n",
    "\n",
    "    # print(score)\n",
    "    return pipe, score\n",
    "\n",
    "\n",
    "def predict_hkl_class(pipe, items_to_classify, info=\"\", y=None):\n",
    "    \"\"\"\n",
    "    Predict the actual HKL class\n",
    "    \"\"\"\n",
    "    pred_column_name = \"095_PRED\"\n",
    "\n",
    "    # Modify library data as needed\n",
    "    X, y = modify_lib_data(items_to_classify, y)\n",
    "\n",
    "    y_pred = predict(pipe, X)\n",
    "\n",
    "    # convert predictions from numpy to dataframe and set an easy column name\n",
    "    y_pred = pd.DataFrame(y_pred)\n",
    "    y_pred = y_pred.rename(columns={y_pred.columns[0]: pred_column_name})\n",
    "\n",
    "    # Modify the library data back to original format\n",
    "    X, y, y_pred = reverse_mod_lib_data(X, y_pred, y)\n",
    "\n",
    "    result = pd.concat([X, y_pred], axis=1)\n",
    "    result[\"Info\"] = info\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keywords in input dataset is: 1917 should be equal with 1917\n",
      "Input data: 500 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>084</th>\n",
       "      <th>092</th>\n",
       "      <th>093</th>\n",
       "      <th>094</th>\n",
       "      <th>095</th>\n",
       "      <th>rock</th>\n",
       "      <th>suomalaiset</th>\n",
       "      <th>taidemaalarit</th>\n",
       "      <th>perinnemusiikki</th>\n",
       "      <th>...</th>\n",
       "      <th>laskentatoimi</th>\n",
       "      <th>toimintolaskenta</th>\n",
       "      <th>kaupunkiarkeologia</th>\n",
       "      <th>virkatalot</th>\n",
       "      <th>kauppiaat</th>\n",
       "      <th>liikemiehet</th>\n",
       "      <th>pormestarit</th>\n",
       "      <th>maaherrat</th>\n",
       "      <th>kenraalikuvernöörit</th>\n",
       "      <th>talot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>420908822165</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>788.330</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>420908631171</td>\n",
       "      <td>99.1000</td>\n",
       "      <td>99.1000</td>\n",
       "      <td>99.1</td>\n",
       "      <td>99.1000</td>\n",
       "      <td>990.100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>420907981954</td>\n",
       "      <td>78.4620</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>784.142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>420908153948</td>\n",
       "      <td>69.3000</td>\n",
       "      <td>69.3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.3000</td>\n",
       "      <td>675.800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>420908158377</td>\n",
       "      <td>68.2000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.2000</td>\n",
       "      <td>691.100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1923 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      record_id      084      092   093      094      095  rock  suomalaiset  \\\n",
       "0  420908822165  78.8911  78.8911   0.0  78.8911  788.330   1.0          0.0   \n",
       "1  420908631171  99.1000  99.1000  99.1  99.1000  990.100   0.0          1.0   \n",
       "2  420907981954  78.4620   0.0000   0.0   0.0000  784.142   0.0          0.0   \n",
       "3  420908153948  69.3000  69.3000   0.0  69.3000  675.800   0.0          0.0   \n",
       "4  420908158377  68.2000   0.0000   0.0  68.2000  691.100   0.0          0.0   \n",
       "\n",
       "   taidemaalarit  perinnemusiikki  ...  laskentatoimi  toimintolaskenta  \\\n",
       "0            0.0              0.0  ...            0.0               0.0   \n",
       "1            1.0              0.0  ...            0.0               0.0   \n",
       "2            0.0              1.0  ...            0.0               0.0   \n",
       "3            0.0              0.0  ...            0.0               0.0   \n",
       "4            0.0              0.0  ...            0.0               0.0   \n",
       "\n",
       "   kaupunkiarkeologia  virkatalot  kauppiaat  liikemiehet  pormestarit  \\\n",
       "0                 0.0         0.0        0.0          0.0          0.0   \n",
       "1                 0.0         0.0        0.0          0.0          0.0   \n",
       "2                 0.0         0.0        0.0          0.0          0.0   \n",
       "3                 0.0         0.0        0.0          0.0          0.0   \n",
       "4                 0.0         0.0        0.0          0.0          0.0   \n",
       "\n",
       "   maaherrat  kenraalikuvernöörit  talot  \n",
       "0        0.0                  0.0    0.0  \n",
       "1        0.0                  0.0    0.0  \n",
       "2        0.0                  0.0    0.0  \n",
       "3        0.0                  0.0    0.0  \n",
       "4        0.0                  0.0    0.0  \n",
       "\n",
       "[5 rows x 1923 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"toy_df, keywords = keywords_to_features(toy_df)\\n\\n# printing out the number of keywords and rows in our final dataset for training and validating the model\\nprint(f\\\"Number of keywords in input dataset is: {toy_df.shape[1] - 6} should be equal with {len(keywords)}\\\")\\nprint(f\\\"Input data: {toy_df.shape[0]} rows.\\\")\\ntoy_df.head()\";\n",
       "                var nbb_formatted_code = \"toy_df, keywords = keywords_to_features(toy_df)\\n\\n# printing out the number of keywords and rows in our final dataset for training and validating the model\\nprint(\\n    f\\\"Number of keywords in input dataset is: {toy_df.shape[1] - 6} should be equal with {len(keywords)}\\\"\\n)\\nprint(f\\\"Input data: {toy_df.shape[0]} rows.\\\")\\ntoy_df.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_df, keywords = keywords_to_features(toy_df)\n",
    "\n",
    "# printing out the number of keywords and rows in our final dataset for training and validating the model\n",
    "print(f\"Number of keywords in input dataset is: {toy_df.shape[1] - 6} should be equal with {len(keywords)}\")\n",
    "print(f\"Input data: {toy_df.shape[0]} rows.\")\n",
    "toy_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start testing\n",
    "\n",
    "Now we can test and print the results with one single function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************\n",
      "  Results for RANDOM FOREST CLASSIFIER with TOY DATASET:\n",
      "******************************************************************\n",
      "Training error: 0.85\n",
      "Validation error: 0.1\n",
      "Loss: 0.6364728419944833\n",
      "\n",
      "Original and predicted labels (first 30 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>095</th>\n",
       "      <th>095_PRED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>788.330</td>\n",
       "      <td>788.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>990.100</td>\n",
       "      <td>788.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784.142</td>\n",
       "      <td>784.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>675.800</td>\n",
       "      <td>675.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>691.100</td>\n",
       "      <td>691.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>696.100</td>\n",
       "      <td>696.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>192.000</td>\n",
       "      <td>788.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>258.000</td>\n",
       "      <td>788.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>624.800</td>\n",
       "      <td>624.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>756.200</td>\n",
       "      <td>756.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>462.000</td>\n",
       "      <td>788.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>793.400</td>\n",
       "      <td>788.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>865.000</td>\n",
       "      <td>788.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>927.000</td>\n",
       "      <td>927.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>786.310</td>\n",
       "      <td>786.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>576.000</td>\n",
       "      <td>788.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>236.000</td>\n",
       "      <td>236.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>868.000</td>\n",
       "      <td>788.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>420.000</td>\n",
       "      <td>420.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>875.000</td>\n",
       "      <td>875.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>788.330</td>\n",
       "      <td>788.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>490.000</td>\n",
       "      <td>788.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>971.230</td>\n",
       "      <td>798.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>788.900</td>\n",
       "      <td>788.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>788.322</td>\n",
       "      <td>788.322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>788.330</td>\n",
       "      <td>788.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>788.211</td>\n",
       "      <td>788.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>787.311</td>\n",
       "      <td>787.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>927.000</td>\n",
       "      <td>927.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>797.000</td>\n",
       "      <td>797.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        095  095_PRED\n",
       "0   788.330   788.330\n",
       "1   990.100   788.330\n",
       "2   784.142   784.142\n",
       "3   675.800   675.800\n",
       "4   691.100   691.100\n",
       "5   696.100   696.100\n",
       "6   192.000   788.330\n",
       "7   258.000   788.330\n",
       "8   624.800   624.800\n",
       "9   756.200   756.200\n",
       "10  462.000   788.330\n",
       "11  793.400   788.330\n",
       "12  865.000   788.330\n",
       "13  927.000   927.000\n",
       "14  786.310   786.310\n",
       "15  576.000   788.330\n",
       "16  236.000   236.000\n",
       "17  868.000   788.330\n",
       "18  420.000   420.000\n",
       "19  875.000   875.000\n",
       "20  788.330   788.330\n",
       "21  490.000   788.330\n",
       "22  971.230   798.200\n",
       "23  788.900   788.330\n",
       "24  788.322   788.322\n",
       "25  788.330   788.330\n",
       "26  788.211   788.211\n",
       "27  787.311   787.311\n",
       "28  927.000   927.000\n",
       "29  797.000   797.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of false predictions: 150\n",
      "Number of right predictions: 350\n",
      "\n",
      "\n",
      "All false predictions in dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>084</th>\n",
       "      <th>092</th>\n",
       "      <th>093</th>\n",
       "      <th>094</th>\n",
       "      <th>rock</th>\n",
       "      <th>suomalaiset</th>\n",
       "      <th>taidemaalarit</th>\n",
       "      <th>perinnemusiikki</th>\n",
       "      <th>markkinointitutkimus</th>\n",
       "      <th>...</th>\n",
       "      <th>kaupunkiarkeologia</th>\n",
       "      <th>virkatalot</th>\n",
       "      <th>kauppiaat</th>\n",
       "      <th>liikemiehet</th>\n",
       "      <th>pormestarit</th>\n",
       "      <th>maaherrat</th>\n",
       "      <th>kenraalikuvernöörit</th>\n",
       "      <th>talot</th>\n",
       "      <th>095</th>\n",
       "      <th>095_PRED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>420908631171</td>\n",
       "      <td>99.10000</td>\n",
       "      <td>99.1000</td>\n",
       "      <td>99.100</td>\n",
       "      <td>99.100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>990.100</td>\n",
       "      <td>788.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>420908390431</td>\n",
       "      <td>15.90000</td>\n",
       "      <td>15.9000</td>\n",
       "      <td>15.900</td>\n",
       "      <td>15.900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>192.000</td>\n",
       "      <td>788.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>420908970852</td>\n",
       "      <td>25.50000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>258.000</td>\n",
       "      <td>788.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>420908629131</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>462.000</td>\n",
       "      <td>788.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>420908138656</td>\n",
       "      <td>79.18100</td>\n",
       "      <td>79.1810</td>\n",
       "      <td>79.181</td>\n",
       "      <td>79.181</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>793.400</td>\n",
       "      <td>788.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>420909116320</td>\n",
       "      <td>88.50000</td>\n",
       "      <td>88.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>882.000</td>\n",
       "      <td>788.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>420908591076</td>\n",
       "      <td>33.25000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>332.230</td>\n",
       "      <td>788.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>420908562236</td>\n",
       "      <td>78.89150</td>\n",
       "      <td>78.8915</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>788.140</td>\n",
       "      <td>788.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>420909000824</td>\n",
       "      <td>79.37000</td>\n",
       "      <td>79.3700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>796.200</td>\n",
       "      <td>788.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>420908246880</td>\n",
       "      <td>78.89111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>788.331</td>\n",
       "      <td>788.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 1924 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        record_id       084      092     093     094  rock  suomalaiset  \\\n",
       "1    420908631171  99.10000  99.1000  99.100  99.100   NaN          1.0   \n",
       "6    420908390431  15.90000  15.9000  15.900  15.900   NaN          NaN   \n",
       "7    420908970852  25.50000      NaN     NaN     NaN   NaN          NaN   \n",
       "10   420908629131  40.00000      NaN     NaN  40.800   NaN          NaN   \n",
       "11   420908138656  79.18100  79.1810  79.181  79.181   NaN          NaN   \n",
       "..            ...       ...      ...     ...     ...   ...          ...   \n",
       "483  420909116320  88.50000  88.5000     NaN  88.500   NaN          NaN   \n",
       "484  420908591076  33.25000      NaN     NaN  33.250   NaN          NaN   \n",
       "489  420908562236  78.89150  78.8915     NaN     NaN   NaN          NaN   \n",
       "492  420909000824  79.37000  79.3700     NaN  79.370   NaN          NaN   \n",
       "498  420908246880  78.89111      NaN     NaN     NaN   1.0          NaN   \n",
       "\n",
       "     taidemaalarit  perinnemusiikki  markkinointitutkimus  ...  \\\n",
       "1              1.0              NaN                   NaN  ...   \n",
       "6              NaN              NaN                   NaN  ...   \n",
       "7              NaN              NaN                   NaN  ...   \n",
       "10             NaN              NaN                   NaN  ...   \n",
       "11             NaN              NaN                   NaN  ...   \n",
       "..             ...              ...                   ...  ...   \n",
       "483            NaN              NaN                   NaN  ...   \n",
       "484            NaN              NaN                   NaN  ...   \n",
       "489            NaN              NaN                   NaN  ...   \n",
       "492            NaN              NaN                   NaN  ...   \n",
       "498            NaN              NaN                   NaN  ...   \n",
       "\n",
       "     kaupunkiarkeologia  virkatalot  kauppiaat  liikemiehet  pormestarit  \\\n",
       "1                   NaN         NaN        NaN          NaN          NaN   \n",
       "6                   NaN         NaN        NaN          NaN          NaN   \n",
       "7                   NaN         NaN        NaN          NaN          NaN   \n",
       "10                  NaN         NaN        NaN          NaN          NaN   \n",
       "11                  NaN         NaN        NaN          NaN          NaN   \n",
       "..                  ...         ...        ...          ...          ...   \n",
       "483                 NaN         NaN        NaN          NaN          NaN   \n",
       "484                 NaN         NaN        NaN          NaN          NaN   \n",
       "489                 NaN         NaN        NaN          NaN          NaN   \n",
       "492                 NaN         NaN        NaN          NaN          NaN   \n",
       "498                 NaN         NaN        NaN          NaN          NaN   \n",
       "\n",
       "     maaherrat  kenraalikuvernöörit  talot      095  095_PRED  \n",
       "1          NaN                  NaN    NaN  990.100    788.33  \n",
       "6          NaN                  NaN    NaN  192.000    788.33  \n",
       "7          NaN                  NaN    NaN  258.000    788.33  \n",
       "10         NaN                  NaN    NaN  462.000    788.33  \n",
       "11         NaN                  NaN    NaN  793.400    788.33  \n",
       "..         ...                  ...    ...      ...       ...  \n",
       "483        NaN                  NaN    NaN  882.000    788.33  \n",
       "484        NaN                  NaN    NaN  332.230    788.33  \n",
       "489        NaN                  NaN    NaN  788.140    788.33  \n",
       "492        NaN                  NaN    NaN  796.200    788.33  \n",
       "498        NaN                  NaN    NaN  788.331    788.33  \n",
       "\n",
       "[150 rows x 1924 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"# Define some initial params\\nk = 5\\ntest_size = 0.2\\n\\n#######################################\\n# Test SVC Classifier\\n#######################################\\n\\nfrom sklearn.svm import SVC\\n\\n_ = test_model(\\n    RandomForestClassifier(max_depth=15),\\n    StandardScaler(),\\n    toy_df,\\n    \\\"RANDOM FOREST CLASSIFIER\\\",\\n    \\\"TOY DATASET\\\",\\n    test_size,\\n)\\n\\n#\\n# Skip hyperparameter tuning for now, maybe implement this later\\n#\\n\\n#cv = StratifiedKFold(n_splits=k)\\n#print(cross_val_score(pipe, X_train, y_train, cv=cv))\\n\\n## optimize\\n#param_grid = {\\n#    \\\"estimator__C\\\": np.logspace(-4, 4, 10),\\n#}\\n\\n# make_pipeline(Imputer(),StandardScaler(),PCA(n_components=2),SVC(random_state=1))\\n\\n# cv = StratifiedKFold(n_splits=5)\\n#gs = GridSearchCV(\\n#    estimator=pipe,\\n#    param_grid=param_grid,\\n#    scoring=\\\"accuracy\\\",\\n#    cv=cv,\\n#    return_train_score=True,\\n#)\\n#gs.fit(X_train, y_train)\\n#\\n#print(\\\"Best Estimator: \\\\n{}\\\\n\\\".format(gs.best_estimator_))\\n#print(\\\"Best Parameters: \\\\n{}\\\\n\\\".format(gs.best_params_))\\n#print(\\\"Best Test Score: \\\\n{}\\\\n\\\".format(gs.best_score_))\\n#print(\\n#    \\\"Best Training Score: \\\\n{}\\\\n\\\".format(\\n#        gs.cv_results_[\\\"mean_train_score\\\"][gs.best_index_]\\n#    )\\n#)\\n#print(\\\"All Training Scores: \\\\n{}\\\\n\\\".format(gs.cv_results_[\\\"mean_train_score\\\"]))\\n#print(\\\"All Test Scores: \\\\n{}\\\\n\\\".format(gs.cv_results_[\\\"mean_test_score\\\"]))\\n# # This prints out all results during Cross-Validation in details\\n# print(\\\"All Meta Results During CV Search: \\\\n{}\\\\n\\\".format(gs.cv_results_))\\n\\n# Reset pipeline with best params\\n#pipe.set_params(estimator__C=gs.best_params_[\\\"estimator__C\\\"])\\n#pipe.fit(X_train, y_train)\\n#print(\\\"Test score with best params (should equal to Best Test Score above)\\\")\\n#print(pipe.score(X_test, y_test))\";\n",
       "                var nbb_formatted_code = \"# Define some initial params\\nk = 5\\ntest_size = 0.2\\n\\n#######################################\\n# Test SVC Classifier\\n#######################################\\n\\nfrom sklearn.svm import SVC\\n\\n_ = test_model(\\n    RandomForestClassifier(max_depth=15),\\n    StandardScaler(),\\n    toy_df,\\n    \\\"RANDOM FOREST CLASSIFIER\\\",\\n    \\\"TOY DATASET\\\",\\n    test_size,\\n)\\n\\n#\\n# Skip hyperparameter tuning for now, maybe implement this later\\n#\\n\\n# cv = StratifiedKFold(n_splits=k)\\n# print(cross_val_score(pipe, X_train, y_train, cv=cv))\\n\\n## optimize\\n# param_grid = {\\n#    \\\"estimator__C\\\": np.logspace(-4, 4, 10),\\n# }\\n\\n# make_pipeline(Imputer(),StandardScaler(),PCA(n_components=2),SVC(random_state=1))\\n\\n# cv = StratifiedKFold(n_splits=5)\\n# gs = GridSearchCV(\\n#    estimator=pipe,\\n#    param_grid=param_grid,\\n#    scoring=\\\"accuracy\\\",\\n#    cv=cv,\\n#    return_train_score=True,\\n# )\\n# gs.fit(X_train, y_train)\\n#\\n# print(\\\"Best Estimator: \\\\n{}\\\\n\\\".format(gs.best_estimator_))\\n# print(\\\"Best Parameters: \\\\n{}\\\\n\\\".format(gs.best_params_))\\n# print(\\\"Best Test Score: \\\\n{}\\\\n\\\".format(gs.best_score_))\\n# print(\\n#    \\\"Best Training Score: \\\\n{}\\\\n\\\".format(\\n#        gs.cv_results_[\\\"mean_train_score\\\"][gs.best_index_]\\n#    )\\n# )\\n# print(\\\"All Training Scores: \\\\n{}\\\\n\\\".format(gs.cv_results_[\\\"mean_train_score\\\"]))\\n# print(\\\"All Test Scores: \\\\n{}\\\\n\\\".format(gs.cv_results_[\\\"mean_test_score\\\"]))\\n# # This prints out all results during Cross-Validation in details\\n# print(\\\"All Meta Results During CV Search: \\\\n{}\\\\n\\\".format(gs.cv_results_))\\n\\n# Reset pipeline with best params\\n# pipe.set_params(estimator__C=gs.best_params_[\\\"estimator__C\\\"])\\n# pipe.fit(X_train, y_train)\\n# print(\\\"Test score with best params (should equal to Best Test Score above)\\\")\\n# print(pipe.score(X_test, y_test))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define some initial params\n",
    "k = 5\n",
    "test_size = 0.2\n",
    "\n",
    "#######################################\n",
    "# Test SVC Classifier\n",
    "#######################################\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "_ = test_model(\n",
    "    RandomForestClassifier(max_depth=15),\n",
    "    StandardScaler(),\n",
    "    toy_df,\n",
    "    \"RANDOM FOREST CLASSIFIER\",\n",
    "    \"TOY DATASET\",\n",
    "    test_size,\n",
    ")\n",
    "\n",
    "#\n",
    "# Skip hyperparameter tuning for now, maybe implement this later\n",
    "#\n",
    "\n",
    "#cv = StratifiedKFold(n_splits=k)\n",
    "#print(cross_val_score(pipe, X_train, y_train, cv=cv))\n",
    "\n",
    "## optimize\n",
    "#param_grid = {\n",
    "#    \"estimator__C\": np.logspace(-4, 4, 10),\n",
    "#}\n",
    "\n",
    "# make_pipeline(Imputer(),StandardScaler(),PCA(n_components=2),SVC(random_state=1))\n",
    "\n",
    "# cv = StratifiedKFold(n_splits=5)\n",
    "#gs = GridSearchCV(\n",
    "#    estimator=pipe,\n",
    "#    param_grid=param_grid,\n",
    "#    scoring=\"accuracy\",\n",
    "#    cv=cv,\n",
    "#    return_train_score=True,\n",
    "#)\n",
    "#gs.fit(X_train, y_train)\n",
    "#\n",
    "#print(\"Best Estimator: \\n{}\\n\".format(gs.best_estimator_))\n",
    "#print(\"Best Parameters: \\n{}\\n\".format(gs.best_params_))\n",
    "#print(\"Best Test Score: \\n{}\\n\".format(gs.best_score_))\n",
    "#print(\n",
    "#    \"Best Training Score: \\n{}\\n\".format(\n",
    "#        gs.cv_results_[\"mean_train_score\"][gs.best_index_]\n",
    "#    )\n",
    "#)\n",
    "#print(\"All Training Scores: \\n{}\\n\".format(gs.cv_results_[\"mean_train_score\"]))\n",
    "#print(\"All Test Scores: \\n{}\\n\".format(gs.cv_results_[\"mean_test_score\"]))\n",
    "# # This prints out all results during Cross-Validation in details\n",
    "# print(\"All Meta Results During CV Search: \\n{}\\n\".format(gs.cv_results_))\n",
    "\n",
    "# Reset pipeline with best params\n",
    "#pipe.set_params(estimator__C=gs.best_params_[\"estimator__C\"])\n",
    "#pipe.fit(X_train, y_train)\n",
    "#print(\"Test score with best params (should equal to Best Test Score above)\")\n",
    "#print(pipe.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test results with toy dataset\n",
    "\n",
    " - 355/500 right (70%)\n",
    " - Training error: 0.8575\n",
    " - Validation error: 0.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTUAL PREDICTING\n",
    "\n",
    "Because the results with toy dataset were not very encouriging (compare training error and validation error) and because dataset for fitting the model is huge we came up with the idea of limitting the training/validation set to only to those rows that have at least some of the features exactly the same as in input data (the data we actually want to predict from).\n",
    "\n",
    "*Note: We should refactor this next cell into separate functions and do Python class implementation and move the actual predicting to \"02_Loss.ipynb\"-notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: PREDICTOR=RandomForest, SCORE=0.43, SET SIZE=220    \n",
      "1: PREDICTOR=RandomForest, SCORE=0.42, SET SIZE=950     WARNING: Over 20 possible classes (30)\n",
      "2: PREDICTOR=RandomForest, SCORE=0.17, SET SIZE=1523     WARNING: Over 20 possible classes (43)\n",
      "3: PREDICTOR=RandomForest, SCORE=0.43, SET SIZE=1857     WARNING: Over 20 possible classes (40)\n",
      "4: PREDICTOR=RandomForest, SCORE=0.58, SET SIZE=477     WARNING: Over 20 possible classes (50)\n",
      "5: PREDICTOR=RandomForest, SCORE=0.09, SET SIZE=1465     WARNING: Over 20 possible classes (99)\n",
      "6: PREDICTOR=RandomForest, SCORE=0.17, SET SIZE=1766     WARNING: Over 20 possible classes (99)\n",
      "7: PREDICTOR=RandomForest, SCORE=0.08, SET SIZE=1465     WARNING: Over 20 possible classes (99)\n",
      "8: PREDICTOR=RandomForest, SCORE=0.36, SET SIZE=842     WARNING: Over 20 possible classes (33)\n",
      "9: PREDICTOR=RandomForest, SCORE=1.03, SET SIZE=359    \n",
      "10: PREDICTOR=RandomForest, SCORE=0.04, SET SIZE=2000     WARNING: Over 20 possible classes (129)\n",
      "11: PREDICTOR=RandomForest, SCORE=0.89, SET SIZE=422    \n",
      "12: PREDICTOR=RandomForest, SCORE=0.74, SET SIZE=233    \n",
      "13: PREDICTOR=RandomForest, SCORE=0.7, SET SIZE=1172    \n",
      "14: PREDICTOR=RandomForest, SCORE=0.33, SET SIZE=1161     WARNING: Over 20 possible classes (61)\n",
      "15: PREDICTOR=RandomForest, SCORE=1.07, SET SIZE=362    \n",
      "16: PREDICTOR=RandomForest, SCORE=0.85, SET SIZE=408    \n",
      "17: PREDICTOR=RandomForest, SCORE=0.28, SET SIZE=668     WARNING: Over 20 possible classes (50)\n",
      "18: PREDICTOR=RandomForest, SCORE=0.52, SET SIZE=842     WARNING: Over 20 possible classes (53)\n",
      "19: PREDICTOR=RandomForest, SCORE=0.08, SET SIZE=2000     WARNING: Over 20 possible classes (251)\n",
      "\n",
      "Number of false predictions: 1\n",
      "Number of right predictions: 19\n",
      "\n",
      "\n",
      "All false predictions in dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>084</th>\n",
       "      <th>092</th>\n",
       "      <th>093</th>\n",
       "      <th>094</th>\n",
       "      <th>rock</th>\n",
       "      <th>suomalaiset</th>\n",
       "      <th>taidemaalarit</th>\n",
       "      <th>perinnemusiikki</th>\n",
       "      <th>markkinointitutkimus</th>\n",
       "      <th>...</th>\n",
       "      <th>liikemiehet</th>\n",
       "      <th>pormestarit</th>\n",
       "      <th>maaherrat</th>\n",
       "      <th>kenraalikuvernöörit</th>\n",
       "      <th>talot</th>\n",
       "      <th>095_PRED</th>\n",
       "      <th>Info</th>\n",
       "      <th>score</th>\n",
       "      <th>predictor</th>\n",
       "      <th>095_CORRECT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>420909105645</td>\n",
       "      <td>99.13</td>\n",
       "      <td>99.13</td>\n",
       "      <td>99.13</td>\n",
       "      <td>99.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>993.10</td>\n",
       "      <td>WARNING: Over 20 possible classes (99)</td>\n",
       "      <td>0.08</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 1927 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      record_id   084   092   093   094  rock  suomalaiset  taidemaalarit  \\\n",
       "0  420909105645 99.13 99.13 99.13 99.13   NaN          NaN            NaN   \n",
       "\n",
       "   perinnemusiikki  markkinointitutkimus  ...  liikemiehet  pormestarit  \\\n",
       "0              NaN                   NaN  ...          NaN          NaN   \n",
       "\n",
       "   maaherrat  kenraalikuvernöörit  talot  095_PRED  \\\n",
       "0        NaN                  NaN    NaN    993.10   \n",
       "\n",
       "                                      Info  score     predictor  095_CORRECT  \n",
       "0   WARNING: Over 20 possible classes (99)   0.08  RandomForest          993  \n",
       "\n",
       "[1 rows x 1927 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RandomForest    1\n",
       "Name: predictor, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>095_PRED</th>\n",
       "      <th>095_CORRECT</th>\n",
       "      <th>predictor</th>\n",
       "      <th>score</th>\n",
       "      <th>Info</th>\n",
       "      <th>084</th>\n",
       "      <th>092</th>\n",
       "      <th>093</th>\n",
       "      <th>094</th>\n",
       "      <th>record_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>821.10</td>\n",
       "      <td>821.10</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.43</td>\n",
       "      <td></td>\n",
       "      <td>86.22</td>\n",
       "      <td>86.22</td>\n",
       "      <td>86.22</td>\n",
       "      <td>86.22</td>\n",
       "      <td>420908943150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>691.12</td>\n",
       "      <td>691.12</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.42</td>\n",
       "      <td>WARNING: Over 20 possible classes (30)</td>\n",
       "      <td>59.34</td>\n",
       "      <td>68.22</td>\n",
       "      <td>68.22</td>\n",
       "      <td>68.22</td>\n",
       "      <td>420909061903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>691.10</td>\n",
       "      <td>691.10</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.17</td>\n",
       "      <td>WARNING: Over 20 possible classes (43)</td>\n",
       "      <td>68.20</td>\n",
       "      <td>68.20</td>\n",
       "      <td>68.20</td>\n",
       "      <td>68.20</td>\n",
       "      <td>420909016138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>788.11</td>\n",
       "      <td>788.11</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.43</td>\n",
       "      <td>WARNING: Over 20 possible classes (40)</td>\n",
       "      <td>78.89</td>\n",
       "      <td>78.89</td>\n",
       "      <td>78.89</td>\n",
       "      <td>78.89</td>\n",
       "      <td>420908852805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>798</td>\n",
       "      <td>798</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.58</td>\n",
       "      <td>WARNING: Over 20 possible classes (50)</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>420908697168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>993.10</td>\n",
       "      <td>993.10</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.09</td>\n",
       "      <td>WARNING: Over 20 possible classes (99)</td>\n",
       "      <td>99.13</td>\n",
       "      <td>99.13</td>\n",
       "      <td>99.13</td>\n",
       "      <td>99.13</td>\n",
       "      <td>420908579965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188.10</td>\n",
       "      <td>188.10</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.17</td>\n",
       "      <td>WARNING: Over 20 possible classes (99)</td>\n",
       "      <td>17.30</td>\n",
       "      <td>17.30</td>\n",
       "      <td>17.30</td>\n",
       "      <td>17.30</td>\n",
       "      <td>420908946672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>993.10</td>\n",
       "      <td>993</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.08</td>\n",
       "      <td>WARNING: Over 20 possible classes (99)</td>\n",
       "      <td>99.13</td>\n",
       "      <td>99.13</td>\n",
       "      <td>99.13</td>\n",
       "      <td>99.13</td>\n",
       "      <td>420909105645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>613.10</td>\n",
       "      <td>613.10</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.36</td>\n",
       "      <td>WARNING: Over 20 possible classes (33)</td>\n",
       "      <td>59.34</td>\n",
       "      <td>59.34</td>\n",
       "      <td>59.34</td>\n",
       "      <td>59.34</td>\n",
       "      <td>420909072938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>788.60</td>\n",
       "      <td>788.60</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1.03</td>\n",
       "      <td></td>\n",
       "      <td>78.31</td>\n",
       "      <td>78.31</td>\n",
       "      <td>78.31</td>\n",
       "      <td>78.31</td>\n",
       "      <td>420907850892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>990.10</td>\n",
       "      <td>990.10</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.04</td>\n",
       "      <td>WARNING: Over 20 possible classes (129)</td>\n",
       "      <td>99.10</td>\n",
       "      <td>99.10</td>\n",
       "      <td>99.10</td>\n",
       "      <td>99.10</td>\n",
       "      <td>420908603653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>764.60</td>\n",
       "      <td>764.60</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.89</td>\n",
       "      <td></td>\n",
       "      <td>65.42</td>\n",
       "      <td>65.43</td>\n",
       "      <td>65.43</td>\n",
       "      <td>65.43</td>\n",
       "      <td>420909256733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>597.93</td>\n",
       "      <td>597.93</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.74</td>\n",
       "      <td></td>\n",
       "      <td>58.11</td>\n",
       "      <td>58.11</td>\n",
       "      <td>58.11</td>\n",
       "      <td>58.11</td>\n",
       "      <td>420909261695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>788.33</td>\n",
       "      <td>788.33</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.70</td>\n",
       "      <td></td>\n",
       "      <td>78.89</td>\n",
       "      <td>78.89</td>\n",
       "      <td>78.89</td>\n",
       "      <td>78.89</td>\n",
       "      <td>420908580663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>788.21</td>\n",
       "      <td>788.21</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.33</td>\n",
       "      <td>WARNING: Over 20 possible classes (61)</td>\n",
       "      <td>78.89</td>\n",
       "      <td>78.89</td>\n",
       "      <td>78.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>420909155148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>796.10</td>\n",
       "      <td>796.10</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1.07</td>\n",
       "      <td></td>\n",
       "      <td>79.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>420908921552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>691.14</td>\n",
       "      <td>691.14</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.85</td>\n",
       "      <td></td>\n",
       "      <td>68.25</td>\n",
       "      <td>68.25</td>\n",
       "      <td>68.25</td>\n",
       "      <td>68.25</td>\n",
       "      <td>420909209064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>811</td>\n",
       "      <td>811</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.28</td>\n",
       "      <td>WARNING: Over 20 possible classes (50)</td>\n",
       "      <td>86.20</td>\n",
       "      <td>86.20</td>\n",
       "      <td>86.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>420908024930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>928.10</td>\n",
       "      <td>928.10</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.52</td>\n",
       "      <td>WARNING: Over 20 possible classes (53)</td>\n",
       "      <td>99.14</td>\n",
       "      <td>99.14</td>\n",
       "      <td>92.73</td>\n",
       "      <td>99.14</td>\n",
       "      <td>420909224089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>990</td>\n",
       "      <td>990</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.08</td>\n",
       "      <td>WARNING: Over 20 possible classes (251)</td>\n",
       "      <td>99.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.10</td>\n",
       "      <td>99.10</td>\n",
       "      <td>420909067148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   095_PRED  095_CORRECT     predictor  score  \\\n",
       "0    821.10       821.10  RandomForest   0.43   \n",
       "0    691.12       691.12  RandomForest   0.42   \n",
       "0    691.10       691.10  RandomForest   0.17   \n",
       "0    788.11       788.11  RandomForest   0.43   \n",
       "0       798          798  RandomForest   0.58   \n",
       "0    993.10       993.10  RandomForest   0.09   \n",
       "0    188.10       188.10  RandomForest   0.17   \n",
       "0    993.10          993  RandomForest   0.08   \n",
       "0    613.10       613.10  RandomForest   0.36   \n",
       "0    788.60       788.60  RandomForest   1.03   \n",
       "0    990.10       990.10  RandomForest   0.04   \n",
       "0    764.60       764.60  RandomForest   0.89   \n",
       "0    597.93       597.93  RandomForest   0.74   \n",
       "0    788.33       788.33  RandomForest   0.70   \n",
       "0    788.21       788.21  RandomForest   0.33   \n",
       "0    796.10       796.10  RandomForest   1.07   \n",
       "0    691.14       691.14  RandomForest   0.85   \n",
       "0       811          811  RandomForest   0.28   \n",
       "0    928.10       928.10  RandomForest   0.52   \n",
       "0       990          990  RandomForest   0.08   \n",
       "\n",
       "                                       Info   084   092   093   094  \\\n",
       "0                                           86.22 86.22 86.22 86.22   \n",
       "0    WARNING: Over 20 possible classes (30) 59.34 68.22 68.22 68.22   \n",
       "0    WARNING: Over 20 possible classes (43) 68.20 68.20 68.20 68.20   \n",
       "0    WARNING: Over 20 possible classes (40) 78.89 78.89 78.89 78.89   \n",
       "0    WARNING: Over 20 possible classes (50)    65    65    65    65   \n",
       "0    WARNING: Over 20 possible classes (99) 99.13 99.13 99.13 99.13   \n",
       "0    WARNING: Over 20 possible classes (99) 17.30 17.30 17.30 17.30   \n",
       "0    WARNING: Over 20 possible classes (99) 99.13 99.13 99.13 99.13   \n",
       "0    WARNING: Over 20 possible classes (33) 59.34 59.34 59.34 59.34   \n",
       "0                                           78.31 78.31 78.31 78.31   \n",
       "0   WARNING: Over 20 possible classes (129) 99.10 99.10 99.10 99.10   \n",
       "0                                           65.42 65.43 65.43 65.43   \n",
       "0                                           58.11 58.11 58.11 58.11   \n",
       "0                                           78.89 78.89 78.89 78.89   \n",
       "0    WARNING: Over 20 possible classes (61) 78.89 78.89 78.89   NaN   \n",
       "0                                           79.31   NaN 79.31 79.31   \n",
       "0                                           68.25 68.25 68.25 68.25   \n",
       "0    WARNING: Over 20 possible classes (50) 86.20 86.20 86.20   NaN   \n",
       "0    WARNING: Over 20 possible classes (53) 99.14 99.14 92.73 99.14   \n",
       "0   WARNING: Over 20 possible classes (251) 99.10   NaN 99.10 99.10   \n",
       "\n",
       "      record_id  \n",
       "0  420908943150  \n",
       "0  420909061903  \n",
       "0  420909016138  \n",
       "0  420908852805  \n",
       "0  420908697168  \n",
       "0  420908579965  \n",
       "0  420908946672  \n",
       "0  420909105645  \n",
       "0  420909072938  \n",
       "0  420907850892  \n",
       "0  420908603653  \n",
       "0  420909256733  \n",
       "0  420909261695  \n",
       "0  420908580663  \n",
       "0  420909155148  \n",
       "0  420908921552  \n",
       "0  420909209064  \n",
       "0  420908024930  \n",
       "0  420909224089  \n",
       "0  420909067148  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF ERRORS: 0\n",
      "SCORE MEAN: 0.46299999999999997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForest    20\n",
       "Name: predictor, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 180\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"import time\\n\\nstart = time.time()\\npd.options.display.float_format = (\\n    lambda x: \\\"{:.0f}\\\".format(x) if int(x) == x else \\\"{:,.2f}\\\".format(x)\\n)\\n\\n\\n############################################################\\n#\\n# HERE WE DO THE ACTUAL PREDICTING\\n#   - We should probably refactor this to \\\"02_Loss.ipynb\\\"\\n#\\n###########################################################\\n\\n\\n# Define some initial params\\nk = 5\\ntest_size = 0.2\\nMAX_TRAININGSET_SIZE = 2000  # We never use bigger training set than this\\nMIN_TRAININGSET_SIZE = 50  # Warn if training set is smaller than this\\nMAX_POSSIBLE_CLASSES = 20  # Warn if number of possible classes is bigger than this\\n\\n\\n# Read input data (generated in '00_data.inpynb')\\ninput_df = pd.read_csv(\\n    input_data_file, index_col=0\\n)  # simulated input data from whole trainng set\\nINPUT_ROWS = 20 # len(input_df) to handle full input file\\n\\nall_classes_df = pd.read_csv(all_classes_data_file, index_col=0)  # whole training set\\nvalidation_df = input_df  # we have real 095 values in simulated input file\\nerrors_df = pd.DataFrame()  # append try/catch errors here\\nfinal_result = pd.DataFrame()  # append prediction results in this dataframe\\n\\n\\n# UNCOMMENT THESE IF WE HAVE A REAL INPUT FILE\\n#\\n# input_data_file = \\\"data/preprocessed_data/testiaineisto.csv\\\"\\n# output_data_file = \\\"data/preprocessed_data/testiaineisto_output.csv\\\"\\n# input_df = pd.read_csv(input_data_file, index_col=False)\\n# validation_df = None\\n# display(input_df.head())\\n\\n\\n# Choose what predictors you want to use\\n# Score means is the wverage score for each predictor with 1000 library items\\n# This doesn't improve results significantly so you should prbably just omit it\\n# Try it out if you want\\npredictors = [\\n    RandomForestClassifier(max_depth=15),\\n    SVC(probability=True),\\n    ExtraTreesClassifier(max_depth=15),\\n    KNeighborsClassifier(n_neighbors=3),\\n    DecisionTreeClassifier(max_depth=15),\\n    SGDClassifier(max_iter=1000),\\n    LinearSVC(dual=False),\\n    GaussianNB(),\\n]\\npredictor_names = [\\n    \\\"RandomForest\\\",\\n    \\\"SVC\\\",\\n    \\\"ExtraTrees\\\",\\n    \\\"KNeighbors\\\",\\n    \\\"DecisionTree\\\",\\n    \\\"SGD\\\",\\n    \\\"LinearSVC\\\",\\n    \\\"GaussianNB\\\",\\n]\\nscore_means = [\\n    0.78,  # RandomForest:  681/977 RIGHT, TIME: 4046\\n    0.47,  # SVC:           668/977 RIGHT, TIME: 4657\\n    0.78,  # Extratrees:    660/977 RIGHT, TIME: 3996\\n    0.35,  # KNeighbors:    658/977 RIGHT, TIME: 3859\\n    0.8,  # Desiciontree:  632/977 RIGHT, TIME: 'decent'\\n    0.68,  # SGD:           623/977 RIGHT, TIME: 4088\\n    0.69,  # LinearSVC:     623/977 RIGHT, TIME: 8460\\n    0.74,  # GaussianNB:    572/977 RIGHT, TIME: 3958\\n]\\nprecisions = [\\n    681 / 977,  # RandomForest\\n    668 / 977,  # SVC\\n    660 / 977,  # Extratrees\\n    658 / 977,  # KNeighbors\\n    632 / 977,  # Desiciontree\\n    623 / 977,  # SGD\\n    623 / 977,  # LinearSVC\\n    572 / 977,  # GaussianNB\\n]\\n\\n\\n# Uncomment these if you want to try just one predictor\\n#\\npredictors = [RandomForestClassifier(max_depth=15)]\\npredictor_names = [\\\"RandomForest\\\"]\\nscore_means = [0.78]\\n\\n\\n#\\n# LOOP THROUGH NEW LIBRARY ITEMS, CLASSIFY THEM AND ADD TO RESULT DATAFRAME\\n#\\nfor i in range(20):\\n    # for i in range(len(input_df)):\\n\\n    # Get one new item from input\\n    item_df = input_df.iloc[[i]]\\n    item_df = item_df.fillna(\\n        -1\\n    )  # replace NaN with -1 to omit class matching when selecting best training set\\n\\n    # parse new item keywords and get known classes of new item\\n    item_df, keywords_lst = keywords_to_features(item_df.copy())\\n    item_df = item_df.drop([\\\"095\\\"], axis=1)\\n    class_084 = item_df.loc[0, \\\"084\\\"]\\n    class_092 = item_df.loc[0, \\\"092\\\"]\\n    class_093 = item_df.loc[0, \\\"093\\\"]\\n    class_094 = item_df.loc[0, \\\"094\\\"]\\n\\n    #\\n    # GET BEST TRAINING SET: until large enough and one of these conditions match\\n    # 1. all classes match to new items's classes\\n    # 2. 093 class match\\n    # 3. 093 OR 092 class matche\\n    # 4. 093 OR 092 OR 094 class match\\n    # 5. any of the classes match\\n    #\\n    training_set_df = all_classes_df[\\n        (all_classes_df[\\\"093\\\"] == class_093)\\n        & (all_classes_df[\\\"092\\\"] == class_092)\\n        & (all_classes_df[\\\"094\\\"] == class_094)\\n        & (all_classes_df[\\\"084\\\"] == class_084)\\n    ]\\n\\n    if len(training_set_df) < MAX_TRAININGSET_SIZE:\\n        training_set_df = all_classes_df[\\n            (all_classes_df[\\\"093\\\"] == class_093) | (all_classes_df[\\\"092\\\"] == class_092)\\n        ]\\n\\n    if len(training_set_df) < MAX_TRAININGSET_SIZE:\\n        training_set_df = all_classes_df[\\n            (all_classes_df[\\\"093\\\"] == class_093)\\n            | (all_classes_df[\\\"092\\\"] == class_092)\\n            | (all_classes_df[\\\"094\\\"] == class_094)\\n        ]\\n\\n    if len(training_set_df) < MAX_TRAININGSET_SIZE:\\n        training_set_df = all_classes_df[\\n            (all_classes_df[\\\"093\\\"] == class_093)\\n            | (all_classes_df[\\\"092\\\"] == class_092)\\n            | (all_classes_df[\\\"094\\\"] == class_094)\\n            | (all_classes_df[\\\"084\\\"] == class_084)\\n        ]\\n\\n    # If training set grew too large then limit it here to MAX_TRAININGSET_SIZE\\n    if len(training_set_df) > MAX_TRAININGSET_SIZE:\\n        training_set_df = training_set_df.sample(MAX_TRAININGSET_SIZE)\\n\\n    # PARSE KEYWORDS AND INITIALIZE SOME VARIABLES\\n    training_set_df, items_keywords_lst = keywords_to_features(\\n        training_set_df.copy(), keywords_lst\\n    )\\n    num_possibles = training_set_df[\\\"095\\\"].nunique()\\n    training_set_size = len(training_set_df)\\n    info = \\\"\\\"\\n    warning = False\\n\\n    # Handle some exceptions\\n    if training_set_size < 1 or num_possibles == 1:\\n        info = \\\"\\\"\\n        prediction = np.nan\\n        predictor = \\\"None\\\"\\n\\n        if training_set_size < 1:\\n            info = \\\"No training data.\\\"\\n            score = -1\\n        elif num_possibles == 1:\\n            info = \\\"Only one class in training set.\\\"\\n            prediction = training_set_df[\\\"095\\\"]\\n            predictor = \\\"One class, no predictor\\\"\\n            score = 1\\n\\n        result_df = item_df.copy()\\n        result_df[\\\"095_PRED\\\"] = prediction\\n        result_df[\\\"Info\\\"] = info\\n        result_df[\\\"score\\\"] = score\\n        result_df[\\\"predictor\\\"] = predictor\\n        final_result = final_result.append(result_df)\\n        print(\\n            f\\\"{i}: PREDICTOR={result_df.loc[0]['predictor']}, SCORE={result_df.loc[0]['score']}, SETSIZE={training_set_size}\\\"\\n        )\\n        continue\\n\\n    # Warn if there are many possible classes or training set is too small\\n    if num_possibles > MAX_POSSIBLE_CLASSES:\\n        info = (\\n            info\\n            + \\\" WARNING: Over \\\"\\n            + str(MAX_POSSIBLE_CLASSES)\\n            + \\\" possible classes (\\\"\\n            + str(num_possibles)\\n            + \\\")\\\"\\n        )\\n        warning = True\\n    if training_set_size < MIN_TRAININGSET_SIZE:\\n        info = info + \\\" WARNING: training set smaller than \\\" + str(MIN_TRAININGSET_SIZE)\\n        warning = True\\n\\n    # print(item_df.shape)\\n    # print(training_set_df.shape)\\n    # print(item_df.loc[0, \\\"084\\\"])\\n    # print(training_set_size)\\n    # print(\\\"Uniques:\\\", num_possibles)\\n    # print(training_set_df[\\\"095\\\"].unique())\\n\\n    #\\n    # INNER LOOP: FIND A DECENT ESTIMATOR FROM PREDICTORS LIST\\n    #\\n    prev_score_estimator = 0\\n    for j in range(len(predictors)):\\n        predictor = predictors[j]\\n        meanscore = score_means[j]\\n        # print(str(i) + \\\": CLASS=\\\" + str(item_df.loc[0, \\\"084\\\"]))\\n        # print(str(type(predictor)) + \\\": \\\" + str(meanscore))\\n\\n        try:\\n            pipe, score = test_model(\\n                predictor,\\n                StandardScaler(),\\n                training_set_df,\\n                str(type(predictor)),\\n                \\\"WHOLE TRAINING SET\\\" + str(len(training_set_df)),\\n                test_size,\\n                verbose=False,\\n            )\\n        except:\\n            errors_df.append(item_df)\\n            continue\\n\\n        # compare score for this item with predictor's average score\\n        score_estimator = score / meanscore\\n\\n        # Add prediction into results if first predictor OR better estimator found\\n        if j == 0 or score_estimator > prev_score_estimator:\\n            result_df = predict_hkl_class(pipe, item_df, info)\\n            result_df[\\\"score\\\"] = round(score_estimator, 2)\\n            result_df[\\\"predictor\\\"] = predictor_names[j]\\n            prev_score_estimator = score_estimator\\n\\n        if score_estimator < 1.2:\\n            continue\\n        else:\\n            break\\n\\n    # APPEND NEW ITEM WITH PREDICTED 095 CLASS TO FINAL RESULT DATAFRAME\\n    result_df[\\\"095_PRED\\\"] = result_df[\\\"095_PRED\\\"].round(6)\\n    final_result = final_result.append(result_df)\\n    print(\\n        f\\\"{i}: PREDICTOR={result_df.loc[0]['predictor']}, SCORE={result_df.loc[0]['score']}, SET SIZE={training_set_size}    {info}\\\"\\n    )\\n\\n# replace -1 back to nan\\nfinal_result = final_result.replace(-1, np.nan)\\n\\n\\n#\\n# IF WE KNOW THE REAL 095 VALUES WE PRINT SOME EXTRA INFO\\n#\\nif validation_df is not None:\\n    final_result[\\\"095_CORRECT\\\"] = np.nan\\n    for rec_id in final_result[\\\"record_id\\\"].tolist():\\n        right_hkl_class = float(\\n            validation_df.loc[validation_df[\\\"record_id\\\"] == rec_id][\\\"095\\\"]\\n        )\\n        final_result.loc[\\n            final_result.record_id == rec_id, \\\"095_CORRECT\\\"\\n        ] = right_hkl_class\\n\\n    # For float comparison we need to use the 6 decimal round function\\n    false_preds = final_result[\\n        final_result[\\\"095_PRED\\\"].round(6) != final_result[\\\"095_CORRECT\\\"].round(6)\\n    ]\\n    n_false_preds = len(false_preds)\\n    n_right_preds = len(final_result) - n_false_preds\\n\\n    print(f\\\"\\\\nNumber of false predictions: {n_false_preds}\\\")\\n    print(f\\\"Number of right predictions: {n_right_preds}\\\")\\n    print(\\\"\\\\n\\\\nAll false predictions in dataset:\\\")\\n    display(false_preds)\\n    display(false_preds[\\\"predictor\\\"].value_counts())\\n\\n\\n#\\n# RE-ARRANGE THE FINAL RESULSET AND DISPLAY RESULTS\\n#\\ncolumn_names = [\\n    \\\"095_PRED\\\",\\n    \\\"095_CORRECT\\\",\\n    \\\"predictor\\\",\\n    \\\"score\\\",\\n    \\\"Info\\\",\\n    \\\"084\\\",\\n    \\\"092\\\",\\n    \\\"093\\\",\\n    \\\"094\\\",\\n    \\\"record_id\\\",\\n]\\nfinal_result = final_result.reindex(columns=column_names)\\ndisplay(final_result.head(100))\\n\\n\\n# Uncomment these if you want debug data\\nprint(\\\"NUMBER OF ERRORS: \\\" + str(len(errors_df)))\\nprint(\\\"SCORE MEAN:\\\", final_result[\\\"score\\\"].mean())\\ndisplay(final_result[\\\"predictor\\\"].value_counts())\\ndisplay(errors_df.head(100))\\nprint(\\\"Execution time:\\\", int((time.time() - start)))\\n\\n# output_data_file = \\\"data/preprocessed_data/output_file.csv\\\"\\n# final_result.to_csv(output_data_file)\";\n",
       "                var nbb_formatted_code = \"import time\\n\\nstart = time.time()\\npd.options.display.float_format = (\\n    lambda x: \\\"{:.0f}\\\".format(x) if int(x) == x else \\\"{:,.2f}\\\".format(x)\\n)\\n\\n\\n############################################################\\n#\\n# HERE WE DO THE ACTUAL PREDICTING\\n#   - We should probably refactor this to \\\"02_Loss.ipynb\\\"\\n#\\n###########################################################\\n\\n\\n# Define some initial params\\nk = 5\\ntest_size = 0.2\\nMAX_TRAININGSET_SIZE = 2000  # We never use bigger training set than this\\nMIN_TRAININGSET_SIZE = 50  # Warn if training set is smaller than this\\nMAX_POSSIBLE_CLASSES = 20  # Warn if number of possible classes is bigger than this\\n\\n\\n# Read input data (generated in '00_data.inpynb')\\ninput_df = pd.read_csv(\\n    input_data_file, index_col=0\\n)  # simulated input data from whole trainng set\\nINPUT_ROWS = 20  # len(input_df) to handle full input file\\n\\nall_classes_df = pd.read_csv(all_classes_data_file, index_col=0)  # whole training set\\nvalidation_df = input_df  # we have real 095 values in simulated input file\\nerrors_df = pd.DataFrame()  # append try/catch errors here\\nfinal_result = pd.DataFrame()  # append prediction results in this dataframe\\n\\n\\n# UNCOMMENT THESE IF WE HAVE A REAL INPUT FILE\\n#\\n# input_data_file = \\\"data/preprocessed_data/testiaineisto.csv\\\"\\n# output_data_file = \\\"data/preprocessed_data/testiaineisto_output.csv\\\"\\n# input_df = pd.read_csv(input_data_file, index_col=False)\\n# validation_df = None\\n# display(input_df.head())\\n\\n\\n# Choose what predictors you want to use\\n# Score means is the wverage score for each predictor with 1000 library items\\n# This doesn't improve results significantly so you should prbably just omit it\\n# Try it out if you want\\npredictors = [\\n    RandomForestClassifier(max_depth=15),\\n    SVC(probability=True),\\n    ExtraTreesClassifier(max_depth=15),\\n    KNeighborsClassifier(n_neighbors=3),\\n    DecisionTreeClassifier(max_depth=15),\\n    SGDClassifier(max_iter=1000),\\n    LinearSVC(dual=False),\\n    GaussianNB(),\\n]\\npredictor_names = [\\n    \\\"RandomForest\\\",\\n    \\\"SVC\\\",\\n    \\\"ExtraTrees\\\",\\n    \\\"KNeighbors\\\",\\n    \\\"DecisionTree\\\",\\n    \\\"SGD\\\",\\n    \\\"LinearSVC\\\",\\n    \\\"GaussianNB\\\",\\n]\\nscore_means = [\\n    0.78,  # RandomForest:  681/977 RIGHT, TIME: 4046\\n    0.47,  # SVC:           668/977 RIGHT, TIME: 4657\\n    0.78,  # Extratrees:    660/977 RIGHT, TIME: 3996\\n    0.35,  # KNeighbors:    658/977 RIGHT, TIME: 3859\\n    0.8,  # Desiciontree:  632/977 RIGHT, TIME: 'decent'\\n    0.68,  # SGD:           623/977 RIGHT, TIME: 4088\\n    0.69,  # LinearSVC:     623/977 RIGHT, TIME: 8460\\n    0.74,  # GaussianNB:    572/977 RIGHT, TIME: 3958\\n]\\nprecisions = [\\n    681 / 977,  # RandomForest\\n    668 / 977,  # SVC\\n    660 / 977,  # Extratrees\\n    658 / 977,  # KNeighbors\\n    632 / 977,  # Desiciontree\\n    623 / 977,  # SGD\\n    623 / 977,  # LinearSVC\\n    572 / 977,  # GaussianNB\\n]\\n\\n\\n# Uncomment these if you want to try just one predictor\\n#\\npredictors = [RandomForestClassifier(max_depth=15)]\\npredictor_names = [\\\"RandomForest\\\"]\\nscore_means = [0.78]\\n\\n\\n#\\n# LOOP THROUGH NEW LIBRARY ITEMS, CLASSIFY THEM AND ADD TO RESULT DATAFRAME\\n#\\nfor i in range(20):\\n    # for i in range(len(input_df)):\\n\\n    # Get one new item from input\\n    item_df = input_df.iloc[[i]]\\n    item_df = item_df.fillna(\\n        -1\\n    )  # replace NaN with -1 to omit class matching when selecting best training set\\n\\n    # parse new item keywords and get known classes of new item\\n    item_df, keywords_lst = keywords_to_features(item_df.copy())\\n    item_df = item_df.drop([\\\"095\\\"], axis=1)\\n    class_084 = item_df.loc[0, \\\"084\\\"]\\n    class_092 = item_df.loc[0, \\\"092\\\"]\\n    class_093 = item_df.loc[0, \\\"093\\\"]\\n    class_094 = item_df.loc[0, \\\"094\\\"]\\n\\n    #\\n    # GET BEST TRAINING SET: until large enough and one of these conditions match\\n    # 1. all classes match to new items's classes\\n    # 2. 093 class match\\n    # 3. 093 OR 092 class matche\\n    # 4. 093 OR 092 OR 094 class match\\n    # 5. any of the classes match\\n    #\\n    training_set_df = all_classes_df[\\n        (all_classes_df[\\\"093\\\"] == class_093)\\n        & (all_classes_df[\\\"092\\\"] == class_092)\\n        & (all_classes_df[\\\"094\\\"] == class_094)\\n        & (all_classes_df[\\\"084\\\"] == class_084)\\n    ]\\n\\n    if len(training_set_df) < MAX_TRAININGSET_SIZE:\\n        training_set_df = all_classes_df[\\n            (all_classes_df[\\\"093\\\"] == class_093) | (all_classes_df[\\\"092\\\"] == class_092)\\n        ]\\n\\n    if len(training_set_df) < MAX_TRAININGSET_SIZE:\\n        training_set_df = all_classes_df[\\n            (all_classes_df[\\\"093\\\"] == class_093)\\n            | (all_classes_df[\\\"092\\\"] == class_092)\\n            | (all_classes_df[\\\"094\\\"] == class_094)\\n        ]\\n\\n    if len(training_set_df) < MAX_TRAININGSET_SIZE:\\n        training_set_df = all_classes_df[\\n            (all_classes_df[\\\"093\\\"] == class_093)\\n            | (all_classes_df[\\\"092\\\"] == class_092)\\n            | (all_classes_df[\\\"094\\\"] == class_094)\\n            | (all_classes_df[\\\"084\\\"] == class_084)\\n        ]\\n\\n    # If training set grew too large then limit it here to MAX_TRAININGSET_SIZE\\n    if len(training_set_df) > MAX_TRAININGSET_SIZE:\\n        training_set_df = training_set_df.sample(MAX_TRAININGSET_SIZE)\\n\\n    # PARSE KEYWORDS AND INITIALIZE SOME VARIABLES\\n    training_set_df, items_keywords_lst = keywords_to_features(\\n        training_set_df.copy(), keywords_lst\\n    )\\n    num_possibles = training_set_df[\\\"095\\\"].nunique()\\n    training_set_size = len(training_set_df)\\n    info = \\\"\\\"\\n    warning = False\\n\\n    # Handle some exceptions\\n    if training_set_size < 1 or num_possibles == 1:\\n        info = \\\"\\\"\\n        prediction = np.nan\\n        predictor = \\\"None\\\"\\n\\n        if training_set_size < 1:\\n            info = \\\"No training data.\\\"\\n            score = -1\\n        elif num_possibles == 1:\\n            info = \\\"Only one class in training set.\\\"\\n            prediction = training_set_df[\\\"095\\\"]\\n            predictor = \\\"One class, no predictor\\\"\\n            score = 1\\n\\n        result_df = item_df.copy()\\n        result_df[\\\"095_PRED\\\"] = prediction\\n        result_df[\\\"Info\\\"] = info\\n        result_df[\\\"score\\\"] = score\\n        result_df[\\\"predictor\\\"] = predictor\\n        final_result = final_result.append(result_df)\\n        print(\\n            f\\\"{i}: PREDICTOR={result_df.loc[0]['predictor']}, SCORE={result_df.loc[0]['score']}, SETSIZE={training_set_size}\\\"\\n        )\\n        continue\\n\\n    # Warn if there are many possible classes or training set is too small\\n    if num_possibles > MAX_POSSIBLE_CLASSES:\\n        info = (\\n            info\\n            + \\\" WARNING: Over \\\"\\n            + str(MAX_POSSIBLE_CLASSES)\\n            + \\\" possible classes (\\\"\\n            + str(num_possibles)\\n            + \\\")\\\"\\n        )\\n        warning = True\\n    if training_set_size < MIN_TRAININGSET_SIZE:\\n        info = info + \\\" WARNING: training set smaller than \\\" + str(MIN_TRAININGSET_SIZE)\\n        warning = True\\n\\n    # print(item_df.shape)\\n    # print(training_set_df.shape)\\n    # print(item_df.loc[0, \\\"084\\\"])\\n    # print(training_set_size)\\n    # print(\\\"Uniques:\\\", num_possibles)\\n    # print(training_set_df[\\\"095\\\"].unique())\\n\\n    #\\n    # INNER LOOP: FIND A DECENT ESTIMATOR FROM PREDICTORS LIST\\n    #\\n    prev_score_estimator = 0\\n    for j in range(len(predictors)):\\n        predictor = predictors[j]\\n        meanscore = score_means[j]\\n        # print(str(i) + \\\": CLASS=\\\" + str(item_df.loc[0, \\\"084\\\"]))\\n        # print(str(type(predictor)) + \\\": \\\" + str(meanscore))\\n\\n        try:\\n            pipe, score = test_model(\\n                predictor,\\n                StandardScaler(),\\n                training_set_df,\\n                str(type(predictor)),\\n                \\\"WHOLE TRAINING SET\\\" + str(len(training_set_df)),\\n                test_size,\\n                verbose=False,\\n            )\\n        except:\\n            errors_df.append(item_df)\\n            continue\\n\\n        # compare score for this item with predictor's average score\\n        score_estimator = score / meanscore\\n\\n        # Add prediction into results if first predictor OR better estimator found\\n        if j == 0 or score_estimator > prev_score_estimator:\\n            result_df = predict_hkl_class(pipe, item_df, info)\\n            result_df[\\\"score\\\"] = round(score_estimator, 2)\\n            result_df[\\\"predictor\\\"] = predictor_names[j]\\n            prev_score_estimator = score_estimator\\n\\n        if score_estimator < 1.2:\\n            continue\\n        else:\\n            break\\n\\n    # APPEND NEW ITEM WITH PREDICTED 095 CLASS TO FINAL RESULT DATAFRAME\\n    result_df[\\\"095_PRED\\\"] = result_df[\\\"095_PRED\\\"].round(6)\\n    final_result = final_result.append(result_df)\\n    print(\\n        f\\\"{i}: PREDICTOR={result_df.loc[0]['predictor']}, SCORE={result_df.loc[0]['score']}, SET SIZE={training_set_size}    {info}\\\"\\n    )\\n\\n# replace -1 back to nan\\nfinal_result = final_result.replace(-1, np.nan)\\n\\n\\n#\\n# IF WE KNOW THE REAL 095 VALUES WE PRINT SOME EXTRA INFO\\n#\\nif validation_df is not None:\\n    final_result[\\\"095_CORRECT\\\"] = np.nan\\n    for rec_id in final_result[\\\"record_id\\\"].tolist():\\n        right_hkl_class = float(\\n            validation_df.loc[validation_df[\\\"record_id\\\"] == rec_id][\\\"095\\\"]\\n        )\\n        final_result.loc[\\n            final_result.record_id == rec_id, \\\"095_CORRECT\\\"\\n        ] = right_hkl_class\\n\\n    # For float comparison we need to use the 6 decimal round function\\n    false_preds = final_result[\\n        final_result[\\\"095_PRED\\\"].round(6) != final_result[\\\"095_CORRECT\\\"].round(6)\\n    ]\\n    n_false_preds = len(false_preds)\\n    n_right_preds = len(final_result) - n_false_preds\\n\\n    print(f\\\"\\\\nNumber of false predictions: {n_false_preds}\\\")\\n    print(f\\\"Number of right predictions: {n_right_preds}\\\")\\n    print(\\\"\\\\n\\\\nAll false predictions in dataset:\\\")\\n    display(false_preds)\\n    display(false_preds[\\\"predictor\\\"].value_counts())\\n\\n\\n#\\n# RE-ARRANGE THE FINAL RESULSET AND DISPLAY RESULTS\\n#\\ncolumn_names = [\\n    \\\"095_PRED\\\",\\n    \\\"095_CORRECT\\\",\\n    \\\"predictor\\\",\\n    \\\"score\\\",\\n    \\\"Info\\\",\\n    \\\"084\\\",\\n    \\\"092\\\",\\n    \\\"093\\\",\\n    \\\"094\\\",\\n    \\\"record_id\\\",\\n]\\nfinal_result = final_result.reindex(columns=column_names)\\ndisplay(final_result.head(100))\\n\\n\\n# Uncomment these if you want debug data\\nprint(\\\"NUMBER OF ERRORS: \\\" + str(len(errors_df)))\\nprint(\\\"SCORE MEAN:\\\", final_result[\\\"score\\\"].mean())\\ndisplay(final_result[\\\"predictor\\\"].value_counts())\\ndisplay(errors_df.head(100))\\nprint(\\\"Execution time:\\\", int((time.time() - start)))\\n\\n# output_data_file = \\\"data/preprocessed_data/output_file.csv\\\"\\n# final_result.to_csv(output_data_file)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "pd.options.display.float_format = (\n",
    "    lambda x: \"{:.0f}\".format(x) if int(x) == x else \"{:,.2f}\".format(x)\n",
    ")\n",
    "\n",
    "\n",
    "############################################################\n",
    "#\n",
    "# HERE WE DO THE ACTUAL PREDICTING\n",
    "#   - We should probably refactor this to \"02_Loss.ipynb\"\n",
    "#\n",
    "###########################################################\n",
    "\n",
    "\n",
    "# Define some initial params\n",
    "k = 5\n",
    "test_size = 0.2\n",
    "MAX_TRAININGSET_SIZE = 2000  # We never use bigger training set than this\n",
    "MIN_TRAININGSET_SIZE = 50  # Warn if training set is smaller than this\n",
    "MAX_POSSIBLE_CLASSES = 20  # Warn if number of possible classes is bigger than this\n",
    "\n",
    "\n",
    "# Read input data (generated in '00_data.inpynb')\n",
    "input_df = pd.read_csv(\n",
    "    input_data_file, index_col=0\n",
    ")  # simulated input data from whole trainng set\n",
    "INPUT_ROWS = 20  # len(input_df) to handle full input file\n",
    "\n",
    "all_classes_df = pd.read_csv(all_classes_data_file, index_col=0)  # whole training set\n",
    "validation_df = input_df  # we have real 095 values in simulated input file\n",
    "errors_df = pd.DataFrame()  # append try/catch errors here\n",
    "final_result = pd.DataFrame()  # append prediction results in this dataframe\n",
    "\n",
    "\n",
    "# UNCOMMENT THESE IF WE HAVE A REAL INPUT FILE\n",
    "#\n",
    "# input_data_file = \"data/preprocessed_data/testiaineisto.csv\"\n",
    "# output_data_file = \"data/preprocessed_data/testiaineisto_output.csv\"\n",
    "# input_df = pd.read_csv(input_data_file, index_col=False)\n",
    "# validation_df = None\n",
    "# display(input_df.head())\n",
    "\n",
    "\n",
    "# Choose what predictors you want to use\n",
    "# Score means is the wverage score for each predictor with 1000 library items\n",
    "# This doesn't improve results significantly so you should prbably just omit it\n",
    "# Try it out if you want\n",
    "predictors = [\n",
    "    RandomForestClassifier(max_depth=15),\n",
    "    SVC(probability=True),\n",
    "    ExtraTreesClassifier(max_depth=15),\n",
    "    KNeighborsClassifier(n_neighbors=3),\n",
    "    DecisionTreeClassifier(max_depth=15),\n",
    "    SGDClassifier(max_iter=1000),\n",
    "    LinearSVC(dual=False),\n",
    "    GaussianNB(),\n",
    "]\n",
    "predictor_names = [\n",
    "    \"RandomForest\",\n",
    "    \"SVC\",\n",
    "    \"ExtraTrees\",\n",
    "    \"KNeighbors\",\n",
    "    \"DecisionTree\",\n",
    "    \"SGD\",\n",
    "    \"LinearSVC\",\n",
    "    \"GaussianNB\",\n",
    "]\n",
    "score_means = [\n",
    "    0.78,  # RandomForest:  681/977 RIGHT, TIME: 4046\n",
    "    0.47,  # SVC:           668/977 RIGHT, TIME: 4657\n",
    "    0.78,  # Extratrees:    660/977 RIGHT, TIME: 3996\n",
    "    0.35,  # KNeighbors:    658/977 RIGHT, TIME: 3859\n",
    "    0.8,  # Desiciontree:  632/977 RIGHT, TIME: 'decent'\n",
    "    0.68,  # SGD:           623/977 RIGHT, TIME: 4088\n",
    "    0.69,  # LinearSVC:     623/977 RIGHT, TIME: 8460\n",
    "    0.74,  # GaussianNB:    572/977 RIGHT, TIME: 3958\n",
    "]\n",
    "precisions = [\n",
    "    681 / 977,  # RandomForest\n",
    "    668 / 977,  # SVC\n",
    "    660 / 977,  # Extratrees\n",
    "    658 / 977,  # KNeighbors\n",
    "    632 / 977,  # Desiciontree\n",
    "    623 / 977,  # SGD\n",
    "    623 / 977,  # LinearSVC\n",
    "    572 / 977,  # GaussianNB\n",
    "]\n",
    "\n",
    "\n",
    "# Uncomment these if you want to try just one predictor\n",
    "#\n",
    "predictors = [RandomForestClassifier(max_depth=15)]\n",
    "predictor_names = [\"RandomForest\"]\n",
    "score_means = [0.78]\n",
    "\n",
    "\n",
    "#\n",
    "# LOOP THROUGH NEW LIBRARY ITEMS, CLASSIFY THEM AND ADD TO RESULT DATAFRAME\n",
    "#\n",
    "for i in range(20):\n",
    "    # for i in range(len(input_df)):\n",
    "\n",
    "    # Get one new item from input\n",
    "    item_df = input_df.iloc[[i]]\n",
    "    item_df = item_df.fillna(\n",
    "        -1\n",
    "    )  # replace NaN with -1 to omit class matching when selecting best training set\n",
    "\n",
    "    # parse new item keywords and get known classes of new item\n",
    "    item_df, keywords_lst = keywords_to_features(item_df.copy())\n",
    "    item_df = item_df.drop([\"095\"], axis=1)\n",
    "    class_084 = item_df.loc[0, \"084\"]\n",
    "    class_092 = item_df.loc[0, \"092\"]\n",
    "    class_093 = item_df.loc[0, \"093\"]\n",
    "    class_094 = item_df.loc[0, \"094\"]\n",
    "\n",
    "    #\n",
    "    # GET BEST TRAINING SET: until large enough and one of these conditions match\n",
    "    # 1. all classes match to new items's classes\n",
    "    # 2. 093 class match\n",
    "    # 3. 093 OR 092 class matche\n",
    "    # 4. 093 OR 092 OR 094 class match\n",
    "    # 5. any of the classes match\n",
    "    #\n",
    "    training_set_df = all_classes_df[\n",
    "        (all_classes_df[\"093\"] == class_093)\n",
    "        & (all_classes_df[\"092\"] == class_092)\n",
    "        & (all_classes_df[\"094\"] == class_094)\n",
    "        & (all_classes_df[\"084\"] == class_084)\n",
    "    ]\n",
    "\n",
    "    if len(training_set_df) < MAX_TRAININGSET_SIZE:\n",
    "        training_set_df = all_classes_df[\n",
    "            (all_classes_df[\"093\"] == class_093) | (all_classes_df[\"092\"] == class_092)\n",
    "        ]\n",
    "\n",
    "    if len(training_set_df) < MAX_TRAININGSET_SIZE:\n",
    "        training_set_df = all_classes_df[\n",
    "            (all_classes_df[\"093\"] == class_093)\n",
    "            | (all_classes_df[\"092\"] == class_092)\n",
    "            | (all_classes_df[\"094\"] == class_094)\n",
    "        ]\n",
    "\n",
    "    if len(training_set_df) < MAX_TRAININGSET_SIZE:\n",
    "        training_set_df = all_classes_df[\n",
    "            (all_classes_df[\"093\"] == class_093)\n",
    "            | (all_classes_df[\"092\"] == class_092)\n",
    "            | (all_classes_df[\"094\"] == class_094)\n",
    "            | (all_classes_df[\"084\"] == class_084)\n",
    "        ]\n",
    "\n",
    "    # If training set grew too large then limit it here to MAX_TRAININGSET_SIZE\n",
    "    if len(training_set_df) > MAX_TRAININGSET_SIZE:\n",
    "        training_set_df = training_set_df.sample(MAX_TRAININGSET_SIZE)\n",
    "\n",
    "    # PARSE KEYWORDS AND INITIALIZE SOME VARIABLES\n",
    "    training_set_df, items_keywords_lst = keywords_to_features(\n",
    "        training_set_df.copy(), keywords_lst\n",
    "    )\n",
    "    num_possibles = training_set_df[\"095\"].nunique()\n",
    "    training_set_size = len(training_set_df)\n",
    "    info = \"\"\n",
    "    warning = False\n",
    "\n",
    "    # Handle some exceptions\n",
    "    if training_set_size < 1 or num_possibles == 1:\n",
    "        info = \"\"\n",
    "        prediction = np.nan\n",
    "        predictor = \"None\"\n",
    "\n",
    "        if training_set_size < 1:\n",
    "            info = \"No training data.\"\n",
    "            score = -1\n",
    "        elif num_possibles == 1:\n",
    "            info = \"Only one class in training set.\"\n",
    "            prediction = training_set_df[\"095\"]\n",
    "            predictor = \"One class, no predictor\"\n",
    "            score = 1\n",
    "\n",
    "        result_df = item_df.copy()\n",
    "        result_df[\"095_PRED\"] = prediction\n",
    "        result_df[\"Info\"] = info\n",
    "        result_df[\"score\"] = score\n",
    "        result_df[\"predictor\"] = predictor\n",
    "        final_result = final_result.append(result_df)\n",
    "        print(\n",
    "            f\"{i}: PREDICTOR={result_df.loc[0]['predictor']}, SCORE={result_df.loc[0]['score']}, SETSIZE={training_set_size}\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # Warn if there are many possible classes or training set is too small\n",
    "    if num_possibles > MAX_POSSIBLE_CLASSES:\n",
    "        info = (\n",
    "            info\n",
    "            + \" WARNING: Over \"\n",
    "            + str(MAX_POSSIBLE_CLASSES)\n",
    "            + \" possible classes (\"\n",
    "            + str(num_possibles)\n",
    "            + \")\"\n",
    "        )\n",
    "        warning = True\n",
    "    if training_set_size < MIN_TRAININGSET_SIZE:\n",
    "        info = info + \" WARNING: training set smaller than \" + str(MIN_TRAININGSET_SIZE)\n",
    "        warning = True\n",
    "\n",
    "    # print(item_df.shape)\n",
    "    # print(training_set_df.shape)\n",
    "    # print(item_df.loc[0, \"084\"])\n",
    "    # print(training_set_size)\n",
    "    # print(\"Uniques:\", num_possibles)\n",
    "    # print(training_set_df[\"095\"].unique())\n",
    "\n",
    "    #\n",
    "    # INNER LOOP: FIND A DECENT ESTIMATOR FROM PREDICTORS LIST\n",
    "    #\n",
    "    prev_score_estimator = 0\n",
    "    for j in range(len(predictors)):\n",
    "        predictor = predictors[j]\n",
    "        meanscore = score_means[j]\n",
    "        # print(str(i) + \": CLASS=\" + str(item_df.loc[0, \"084\"]))\n",
    "        # print(str(type(predictor)) + \": \" + str(meanscore))\n",
    "\n",
    "        try:\n",
    "            pipe, score = test_model(\n",
    "                predictor,\n",
    "                StandardScaler(),\n",
    "                training_set_df,\n",
    "                str(type(predictor)),\n",
    "                \"WHOLE TRAINING SET\" + str(len(training_set_df)),\n",
    "                test_size,\n",
    "                verbose=False,\n",
    "            )\n",
    "        except:\n",
    "            errors_df.append(item_df)\n",
    "            continue\n",
    "\n",
    "        # compare score for this item with predictor's average score\n",
    "        score_estimator = score / meanscore\n",
    "\n",
    "        # Add prediction into results if first predictor OR better estimator found\n",
    "        if j == 0 or score_estimator > prev_score_estimator:\n",
    "            result_df = predict_hkl_class(pipe, item_df, info)\n",
    "            result_df[\"score\"] = round(score_estimator, 2)\n",
    "            result_df[\"predictor\"] = predictor_names[j]\n",
    "            prev_score_estimator = score_estimator\n",
    "\n",
    "        if score_estimator < 1.2:\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # APPEND NEW ITEM WITH PREDICTED 095 CLASS TO FINAL RESULT DATAFRAME\n",
    "    result_df[\"095_PRED\"] = result_df[\"095_PRED\"].round(6)\n",
    "    final_result = final_result.append(result_df)\n",
    "    print(\n",
    "        f\"{i}: PREDICTOR={result_df.loc[0]['predictor']}, SCORE={result_df.loc[0]['score']}, SET SIZE={training_set_size}    {info}\"\n",
    "    )\n",
    "\n",
    "# replace -1 back to nan\n",
    "final_result = final_result.replace(-1, np.nan)\n",
    "\n",
    "\n",
    "#\n",
    "# IF WE KNOW THE REAL 095 VALUES WE PRINT SOME EXTRA INFO\n",
    "#\n",
    "if validation_df is not None:\n",
    "    final_result[\"095_CORRECT\"] = np.nan\n",
    "    for rec_id in final_result[\"record_id\"].tolist():\n",
    "        right_hkl_class = float(\n",
    "            validation_df.loc[validation_df[\"record_id\"] == rec_id][\"095\"]\n",
    "        )\n",
    "        final_result.loc[\n",
    "            final_result.record_id == rec_id, \"095_CORRECT\"\n",
    "        ] = right_hkl_class\n",
    "\n",
    "    # For float comparison we need to use the 6 decimal round function\n",
    "    false_preds = final_result[\n",
    "        final_result[\"095_PRED\"].round(6) != final_result[\"095_CORRECT\"].round(6)\n",
    "    ]\n",
    "    n_false_preds = len(false_preds)\n",
    "    n_right_preds = len(final_result) - n_false_preds\n",
    "\n",
    "    print(f\"\\nNumber of false predictions: {n_false_preds}\")\n",
    "    print(f\"Number of right predictions: {n_right_preds}\")\n",
    "    print(\"\\n\\nAll false predictions in dataset:\")\n",
    "    display(false_preds)\n",
    "    display(false_preds[\"predictor\"].value_counts())\n",
    "\n",
    "\n",
    "#\n",
    "# RE-ARRANGE THE FINAL RESULSET AND DISPLAY RESULTS\n",
    "#\n",
    "column_names = [\n",
    "    \"095_PRED\",\n",
    "    \"095_CORRECT\",\n",
    "    \"predictor\",\n",
    "    \"score\",\n",
    "    \"Info\",\n",
    "    \"084\",\n",
    "    \"092\",\n",
    "    \"093\",\n",
    "    \"094\",\n",
    "    \"record_id\",\n",
    "]\n",
    "final_result = final_result.reindex(columns=column_names)\n",
    "display(final_result.head(100))\n",
    "\n",
    "\n",
    "# Uncomment these if you want debug data\n",
    "print(\"NUMBER OF ERRORS: \" + str(len(errors_df)))\n",
    "print(\"SCORE MEAN:\", final_result[\"score\"].mean())\n",
    "display(final_result[\"predictor\"].value_counts())\n",
    "display(errors_df.head(100))\n",
    "print(\"Execution time:\", int((time.time() - start)))\n",
    "\n",
    "# output_data_file = \"data/preprocessed_data/output_file.csv\"\n",
    "# final_result.to_csv(output_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "We noticed that if there are no shortages in input data (that is there are no missing values in features) the random forest predictor does very well, even if there are tens of possible label classes. For example trying to predict 20 labels the model got 18 right and even in the last two the main class was correct (the error was in decimals).\n",
    " \n",
    "We did huge amount of tests with the 100 row input dataset. Here are main findings: \n",
    " \n",
    " - Trying to select the best model on the fly based on testing with all classes with 1000 items and the average lose-function value, we got 675/977 right, elapsed time: 9442ms\n",
    " - Using mixture of RandomForestClassifier, SVC, ExtraTreesClassifier and KNeighborsClassifier we got 682/977, elapsed time: 5568ms\n",
    " - Results for all models:\n",
    "     - RandomForestClassifier(max_depth=15), 681/977, SCORE MEAN 0.78, TIME: 4046ms\n",
    "     - SVC(probability=True), 668/977, SCORE MEAN: 0.47, TIME 4657ms\n",
    "     - ExtraTreesClassifier(max_depth=15), 660/977, SCORE MEAN: 0.78, TIME: 3996ms\n",
    "     - KNeighborsClassifier(n_neighbors=3), 658/977, SCORE MEAN: 0.35, TIME: 3859ms\n",
    "     - DecisionTreeClassifier(max_depth=15), 632/977, SCORE MEAN: 0.8ms\n",
    "     - SGDClassifier(max_iter=1000),  623/977, SCORE MEAN: 0.68, TIME: 4088ms\n",
    "     - LinearSVC(dual=False), 623/977, SCORE MEAN: 0.69, TIME: 8460ms\n",
    "     - GaussianNB(), 572/977, SCORE MEAN 0.74, TIME: 3958ms\n",
    "  \n",
    "### Some other observations about data and models:\n",
    "\n",
    "#### Gaussian Naive Bayes\n",
    " - We can't use float as a type in label (even the casting to type 'Category' didn't help)\n",
    "    - Workaround: multiply the HKLJ-CLASS with a big number and cast it to int\n",
    " - Sklearn libraries do not accept NaN values\n",
    "    - Workaround: replace NaN-values with 0\n",
    " - There are lots of rows where there is no info at all about other library classification\n",
    "    - Use keywords as additional info\n",
    "    - Omit the the rows where there is no class-information at all from other classification systems\n",
    "\n",
    "#### Complement Naive Bayes\n",
    " - This is an enhancement of Multinomial Naive Bayes\n",
    " - We can't use StandardScaler\n",
    "     - For unknown reason algorithm returns error \"negative values in input\"\n",
    "     - Workaround: use MinMaxScaler\n",
    " - Quick testing show much worse results than Gaussian Naive Bayes\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define base class for your ML model\n",
    "\n",
    "We should implement this next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output of this notebook\n",
    "\n",
    "The result of this notebook is a collection methods ready for evaluation with the real data.\n",
    "\n",
    "You should export classes and functions to `model.py` with `# nbdev_build_lib` (workflows will do this automatically)."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "ml-sami-libclas"
  },
  "kernelspec": {
   "display_name": "Python 3.8 (ml-sami-libclas)",
   "language": "python",
   "name": "ml-sami-libclas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
