{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 127;\n",
       "                var nbb_unformatted_code = \"# default_exp model\\n# %load_ext lab_black\\n\\n# nb_black if running in jupyter\\n%load_ext nb_black\\n\\n%load_ext autoreload\\n# automatically reload python modules if there are changes in the\\n%autoreload 2\";\n",
       "                var nbb_formatted_code = \"# default_exp model\\n# %load_ext lab_black\\n\\n# nb_black if running in jupyter\\n%load_ext nb_black\\n\\n%load_ext autoreload\\n# automatically reload python modules if there are changes in the\\n%autoreload 2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# default_exp model\n",
    "# %load_ext lab_black\n",
    "\n",
    "# nb_black if running in jupyter\n",
    "%load_ext nb_black\n",
    "\n",
    "%load_ext autoreload\n",
    "# automatically reload python modules if there are changes in the\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 128;\n",
       "                var nbb_unformatted_code = \"# hide\\nfrom nbdev.showdoc import *\";\n",
       "                var nbb_formatted_code = \"# hide\\nfrom nbdev.showdoc import *\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "> In this notebook you create and test a Python class to hold your machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***input***: toy dataset from data-notebook\n",
    "\n",
    "***output***: python module containing ML model class \n",
    "\n",
    "***description:***\n",
    "\n",
    "In this notebook you hypothetize, explain and explore machine learning models to solve your problem.\n",
    "\n",
    "Then, you should encapsulate the model inside a Python class to be exported into `your_repository/your_module/model.py`,\n",
    "so that it can be evaluated in the loss notebook, and intergrated with your target application.\n",
    "Repository name and module name are the same by default.\n",
    "You should also unit test the classes created in this notebook with the toy data created in data notebook.\n",
    "\n",
    "You should probably have a person in your team familiar with object oriented programming with python and unit testing, but if not, don't worry.\n",
    "If you can explore by scripting in the cells and create a draft of the properties and functions you want to have, and that's a great start.\n",
    "Then, any Python developer can easily build the model class for you.\n",
    "However, we encourage you to learn more on [object oriented programming with Python](https://realpython.com/python3-object-oriented-programming/)\n",
    "and [getting started with unit testing in Python](https://realpython.com/python-testing/).\n",
    "You can also follow the example here to create your own simple machine learning Python class.\n",
    "\n",
    "This notebook contains an example ML model for classifying the heart disease dataset with logistic regression.\n",
    "\n",
    "The example is split into a base class and a subclass for demonstrating class inheritance of Python.\n",
    "You can probably just write one class that contains all the attributes and functions you need, without inheriting anything.\n",
    "However, alternative implementations of a model might be implemented in separate subclasses, for example.\n",
    "You can also define multiple classes, for example one for ML model and another for optimization.\n",
    "If the methods are complicated or you are comparing multiple methods that don't share common functions, \n",
    "you can also separate models or subclasses to different notebooks similar to this.\n",
    "Adjust the running number, name, header and top cell `#default_exp module_name` of the notebooks accordingly.\n",
    "\n",
    "Remember to add `# export` to top of all cells containing functions or classes that you have defined and want to use outside this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 129;\n",
       "                var nbb_unformatted_code = \"# export\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.linear_model import LogisticRegression\\nfrom pandas.api.types import CategoricalDtype\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.metrics import f1_score\\nfrom sklearn.metrics import log_loss\\n\\nfrom sklearn.model_selection import (\\n    GridSearchCV,\\n    cross_val_score,\\n    train_test_split,\\n    StratifiedKFold,\\n)\\nfrom sklearn.pipeline import Pipeline, make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import mean_squared_error\";\n",
       "                var nbb_formatted_code = \"# export\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.linear_model import LogisticRegression\\nfrom pandas.api.types import CategoricalDtype\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.metrics import f1_score\\nfrom sklearn.metrics import log_loss\\n\\nfrom sklearn.model_selection import (\\n    GridSearchCV,\\n    cross_val_score,\\n    train_test_split,\\n    StratifiedKFold,\\n)\\nfrom sklearn.pipeline import Pipeline, make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import mean_squared_error\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 130;\n",
       "                var nbb_unformatted_code = \"# FIX THIS!!!!\\n\\n# Use 'nbdev_build_lib' shell command to update library\\n# from ml_project_template.plot import plot_trellis, plot_histogram\";\n",
       "                var nbb_formatted_code = \"# FIX THIS!!!!\\n\\n# Use 'nbdev_build_lib' shell command to update library\\n# from ml_project_template.plot import plot_trellis, plot_histogram\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FIX THIS!!!!\n",
    "\n",
    "# Use 'nbdev_build_lib' shell command to update library\n",
    "# from ml_project_template.plot import plot_trellis, plot_histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define notebook parameters\n",
    "\n",
    "Remember, only simple assignments here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 131;\n",
       "                var nbb_unformatted_code = \"# Parameters\\n\\n# this cell is tagged with 'parameters'\\ntoy_data_filename = \\\"data/preprocessed_data/dataset_toy_hki_lib_book_classification.csv\\\"\\nseed = 0\";\n",
       "                var nbb_formatted_code = \"# Parameters\\n\\n# this cell is tagged with 'parameters'\\ntoy_data_filename = \\\"data/preprocessed_data/dataset_toy_hki_lib_book_classification.csv\\\"\\nseed = 0\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters\n",
    "\n",
    "# this cell is tagged with 'parameters'\n",
    "toy_data_filename = \"data/preprocessed_data/dataset_toy_hki_lib_book_classification.csv\"\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make immediate derivations from the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 132;\n",
       "                var nbb_unformatted_code = \"np.random.seed(seed)\";\n",
       "                var nbb_formatted_code = \"np.random.seed(seed)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import toy data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>084</th>\n",
       "      <th>092</th>\n",
       "      <th>093</th>\n",
       "      <th>094</th>\n",
       "      <th>095</th>\n",
       "      <th>jalkapallo</th>\n",
       "      <th>historia</th>\n",
       "      <th>urheilijat</th>\n",
       "      <th>urheiluseurat</th>\n",
       "      <th>...</th>\n",
       "      <th>unelmat</th>\n",
       "      <th>uskollisuus</th>\n",
       "      <th>skandaalit</th>\n",
       "      <th>epärehellisyys</th>\n",
       "      <th>vertailu</th>\n",
       "      <th>uppslagsverk</th>\n",
       "      <th>yhteiskunta</th>\n",
       "      <th>onnistuminen</th>\n",
       "      <th>statistik</th>\n",
       "      <th>homoseksuaalisuus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>420909071778</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>420908876765</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>420909142663</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>420908551075</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>420909123493</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>420908962392</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>420908780772</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>420909198340</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>420908940998</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>420907837999</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>420909023505</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>420908554914</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>420909170048</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>420909025015</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>420909225368</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>420908964232</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>420909015193</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>420909075256</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>420908983556</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>420909149325</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>420908976786</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>420908993791</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>796.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>420908040399</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>420908947455</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>420908001241</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>420908932839</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>420909212330</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>420909165546</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>420908975453</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>796.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>420909028818</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>79.31</td>\n",
       "      <td>796.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 297 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        record_id    084    092    093    094    095  jalkapallo  historia  \\\n",
       "256  420909071778  79.31  79.31   0.00  79.31  796.1         0.0       0.0   \n",
       "4    420908876765  79.31  79.31   0.00  79.31  796.1         1.0       0.0   \n",
       "23   420909142663  79.31  79.31  79.31   0.00  796.1         1.0       1.0   \n",
       "170  420908551075  79.31  79.31  79.31  79.31  796.1         1.0       0.0   \n",
       "50   420909123493  79.31  79.31   0.00   0.00  796.1         1.0       1.0   \n",
       "286  420908962392  79.31  79.31  79.31  79.31  796.1         0.0       0.0   \n",
       "113  420908780772  79.31  79.31   0.00  79.31  796.1         1.0       0.0   \n",
       "203  420909198340  79.31  79.31  79.31  79.31  796.1         0.0       0.0   \n",
       "301  420908940998  79.31  79.31  79.31  79.31  796.1         0.0       0.0   \n",
       "314  420907837999  79.31   0.00   0.00   0.00  796.1         1.0       0.0   \n",
       "327  420909023505  79.31  79.31  79.31  79.31  796.1         1.0       0.0   \n",
       "86   420908554914  79.31  79.31  79.31  79.31  796.1         1.0       0.0   \n",
       "245  420909170048  79.31  79.31   0.00   0.00  796.1         1.0       0.0   \n",
       "35   420909025015  79.31   0.00   0.00  79.31  796.1         1.0       0.0   \n",
       "148  420909225368  79.31   0.00   0.00  79.31  796.1         1.0       0.0   \n",
       "83   420908964232  79.31  79.31   0.00   0.00  796.1         1.0       0.0   \n",
       "61   420909015193  79.31  79.31  79.31  79.31  796.1         0.0       0.0   \n",
       "25   420909075256  79.31   0.00  79.31   0.00  796.1         1.0       0.0   \n",
       "88   420908983556  79.31  79.31  79.31  79.31  796.1         1.0       1.0   \n",
       "108  420909149325  79.31   0.00   0.00  79.31  796.1         1.0       1.0   \n",
       "41   420908976786  79.31  79.31  79.31  79.31  796.3         1.0       1.0   \n",
       "43   420908993791  79.31   0.00  79.31   0.00  796.1         0.0       0.0   \n",
       "6    420908040399  79.31   0.00   0.00   0.00  796.1         1.0       0.0   \n",
       "126  420908947455  79.31  79.31   0.00  79.31  796.1         1.0       0.0   \n",
       "22   420908001241  79.31  79.31   0.00   0.00  796.1         1.0       0.0   \n",
       "216  420908932839  79.31  79.31   0.00  79.31  796.1         1.0       1.0   \n",
       "38   420909212330  79.31  79.31   0.00  79.31  796.1         1.0       0.0   \n",
       "57   420909165546  79.31   0.00   0.00   0.00  796.1         1.0       1.0   \n",
       "260  420908975453  79.31   0.00   0.00   0.00  796.2         0.0       0.0   \n",
       "188  420909028818  79.31  79.31  79.31  79.31  796.1         1.0       1.0   \n",
       "\n",
       "     urheilijat  urheiluseurat  ...  unelmat  uskollisuus  skandaalit  \\\n",
       "256         0.0            0.0  ...      0.0          0.0         0.0   \n",
       "4           1.0            0.0  ...      0.0          0.0         0.0   \n",
       "23          0.0            0.0  ...      0.0          0.0         0.0   \n",
       "170         0.0            0.0  ...      0.0          0.0         0.0   \n",
       "50          0.0            0.0  ...      0.0          0.0         0.0   \n",
       "286         0.0            0.0  ...      0.0          0.0         0.0   \n",
       "113         0.0            0.0  ...      0.0          0.0         0.0   \n",
       "203         0.0            0.0  ...      0.0          0.0         0.0   \n",
       "301         0.0            0.0  ...      0.0          0.0         0.0   \n",
       "314         0.0            0.0  ...      0.0          0.0         0.0   \n",
       "327         1.0            0.0  ...      0.0          0.0         0.0   \n",
       "86          0.0            0.0  ...      0.0          0.0         0.0   \n",
       "245         0.0            0.0  ...      0.0          0.0         0.0   \n",
       "35          0.0            0.0  ...      0.0          0.0         0.0   \n",
       "148         0.0            0.0  ...      0.0          0.0         0.0   \n",
       "83          1.0            0.0  ...      0.0          0.0         0.0   \n",
       "61          0.0            0.0  ...      0.0          0.0         0.0   \n",
       "25          1.0            0.0  ...      0.0          0.0         0.0   \n",
       "88          0.0            0.0  ...      0.0          0.0         0.0   \n",
       "108         1.0            0.0  ...      0.0          0.0         0.0   \n",
       "41          0.0            0.0  ...      0.0          0.0         0.0   \n",
       "43          0.0            0.0  ...      0.0          0.0         0.0   \n",
       "6           0.0            0.0  ...      0.0          0.0         0.0   \n",
       "126         1.0            0.0  ...      0.0          0.0         0.0   \n",
       "22          0.0            0.0  ...      0.0          0.0         0.0   \n",
       "216         1.0            1.0  ...      0.0          0.0         0.0   \n",
       "38          1.0            0.0  ...      0.0          0.0         0.0   \n",
       "57          0.0            0.0  ...      0.0          0.0         0.0   \n",
       "260         0.0            0.0  ...      0.0          0.0         0.0   \n",
       "188         0.0            0.0  ...      0.0          0.0         0.0   \n",
       "\n",
       "     epärehellisyys  vertailu  uppslagsverk  yhteiskunta  onnistuminen  \\\n",
       "256             0.0       0.0           0.0          0.0           0.0   \n",
       "4               0.0       0.0           0.0          0.0           0.0   \n",
       "23              0.0       0.0           0.0          0.0           0.0   \n",
       "170             0.0       0.0           0.0          0.0           0.0   \n",
       "50              0.0       0.0           0.0          0.0           0.0   \n",
       "286             0.0       0.0           0.0          0.0           0.0   \n",
       "113             0.0       0.0           0.0          0.0           0.0   \n",
       "203             0.0       0.0           0.0          0.0           0.0   \n",
       "301             0.0       0.0           0.0          0.0           0.0   \n",
       "314             0.0       0.0           0.0          0.0           0.0   \n",
       "327             0.0       0.0           0.0          0.0           0.0   \n",
       "86              0.0       0.0           0.0          0.0           0.0   \n",
       "245             0.0       0.0           0.0          0.0           0.0   \n",
       "35              0.0       0.0           0.0          0.0           0.0   \n",
       "148             0.0       0.0           0.0          0.0           0.0   \n",
       "83              0.0       0.0           0.0          0.0           0.0   \n",
       "61              0.0       0.0           0.0          0.0           0.0   \n",
       "25              0.0       0.0           0.0          0.0           0.0   \n",
       "88              0.0       0.0           0.0          0.0           0.0   \n",
       "108             0.0       0.0           0.0          0.0           0.0   \n",
       "41              0.0       0.0           0.0          0.0           0.0   \n",
       "43              0.0       0.0           0.0          0.0           0.0   \n",
       "6               0.0       0.0           0.0          0.0           0.0   \n",
       "126             0.0       0.0           0.0          0.0           0.0   \n",
       "22              0.0       0.0           0.0          0.0           0.0   \n",
       "216             0.0       0.0           0.0          0.0           0.0   \n",
       "38              0.0       0.0           0.0          0.0           0.0   \n",
       "57              0.0       0.0           0.0          0.0           0.0   \n",
       "260             0.0       0.0           0.0          0.0           0.0   \n",
       "188             0.0       0.0           0.0          1.0           0.0   \n",
       "\n",
       "     statistik  homoseksuaalisuus  \n",
       "256        0.0                0.0  \n",
       "4          0.0                0.0  \n",
       "23         0.0                0.0  \n",
       "170        0.0                0.0  \n",
       "50         0.0                0.0  \n",
       "286        0.0                0.0  \n",
       "113        0.0                0.0  \n",
       "203        0.0                0.0  \n",
       "301        0.0                0.0  \n",
       "314        0.0                0.0  \n",
       "327        0.0                0.0  \n",
       "86         0.0                0.0  \n",
       "245        0.0                0.0  \n",
       "35         0.0                0.0  \n",
       "148        0.0                0.0  \n",
       "83         0.0                0.0  \n",
       "61         0.0                0.0  \n",
       "25         0.0                0.0  \n",
       "88         0.0                0.0  \n",
       "108        0.0                0.0  \n",
       "41         0.0                0.0  \n",
       "43         0.0                0.0  \n",
       "6          0.0                0.0  \n",
       "126        0.0                0.0  \n",
       "22         0.0                0.0  \n",
       "216        0.0                0.0  \n",
       "38         0.0                0.0  \n",
       "57         0.0                0.0  \n",
       "260        0.0                0.0  \n",
       "188        0.0                0.0  \n",
       "\n",
       "[30 rows x 297 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 133;\n",
       "                var nbb_unformatted_code = \"toy_df = pd.read_csv(toy_data_filename, index_col=0)\\ntoy_df.head(30)\";\n",
       "                var nbb_formatted_code = \"toy_df = pd.read_csv(toy_data_filename, index_col=0)\\ntoy_df.head(30)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_df = pd.read_csv(toy_data_filename, index_col=0)\n",
    "toy_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note, that depending on your choice of file format and your variables, you might have to redefine data types once you load data! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO!\n",
    "\n",
    "## Explain the math behind\n",
    "\n",
    "Give a short explanation of how the algorithms work that you are planning to use.\n",
    "\n",
    "For industrial use, you can keep this light and simple: you can provide links to external sources for further reading.\n",
    "\n",
    "For research, you might want to dig deeper - this is your core documentation, after all!\n",
    "\n",
    "You can use $\\LaTeX$ notation to write math symbols and equations:\n",
    "\n",
    "$$\n",
    "Pr(Y_i=1|X_i) = {\\frac{exp(\\beta_0 + \\beta_1X_i + \\dots + \\beta_nX_n)}{1 + exp (\\beta_0 + \\beta_1X_i + \\dots + \\beta_nX_n)}}\n",
    "$$\n",
    "\n",
    "You can also draft algorithms:\n",
    "\n",
    "    ALGORITHM\n",
    "    input: X\n",
    "    output: y\n",
    "\n",
    "    while: condition\n",
    "        do thing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin with simple scripts before constructing the model class\n",
    "\n",
    "This is the place where you can explore and play around with different machine learning operations.\n",
    "Your goal is to plan and demonstrate the features and functions you want your machine learning class to have.\n",
    "The good thing is, that you don't need to think about object oriented programming here.\n",
    "Just assign variables and call functions. \n",
    "\n",
    "It's good to define at least the following steps:\n",
    "\n",
    "1. Splitting data into training and testing data\n",
    "2. Preprocess the data (scale, dimension reduction, convolutions etc.)\n",
    "3. Define your model algorithm and fit it with toy data\n",
    "4. Define your loss function - how do you evaluate your model?\n",
    "5. Consider hyperparameter optimization\n",
    "6. Try to pipe the previous steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 134;\n",
       "                var nbb_unformatted_code = \"# export\\n\\n# FIX THIS IMPORT!!!!\\n# These functions won't work if we don't define these also in this code block\\n#from lib_classification.plot import plot_trellis, plot_histogram\\n\\n\\nseed = 0\\n\\n\\n'''\\nLabels can't be of type float for classification. Thus we multiply floats\\nso that there are no decimals. When printing the results we do the opposite.operation\\n\\nmax_decimals tells number of possible decimals in library classification.\\nSet max_decimals to 0 if you want to omit decimals alltogether\\n'''\\nmax_decimals = 6  \\nmultiply_factor = 10 ** max_decimals\\n\\n'''\\nSklearn models can\\u00e4t handle NaN values, replace them with suitable value, defaul = 0\\n'''\\nreplace_nan = 0\\n\\ndef split_X_y(df):\\n    \\\"\\\"\\\"\\n    Split dataframe into features and labels\\n    \\\"\\\"\\\"\\n    #X = df.iloc[:, :-1]  # .to_numpy()\\n    #y = df.iloc[:, -1]  # .to_numpy()\\n    \\n    #for col in df.columns:\\n    #    print(f\\\"*{col}*\\\")\\n    \\n    X = df.copy().reset_index(drop=True)\\n    y = X.pop(\\\"095\\\").reset_index(drop=True)\\n    \\n    return X, y\\n\\n\\ndef modify_lib_data(X, y):\\n    \\\"\\\"\\\"\\n    Do the needed modification for library data\\n    \\\"\\\"\\\"\\n\\n    # Sklearn GaussianNB doesn't handle NaN-values in input.\\n    # We fill the NaN values with 0.\\n    X = X.fillna(replace_nan)\\n\\n    # Change datatypes for features and labels\\n    X = X.astype(\\n        {\\n            \\\"record_id\\\": \\\"int\\\",\\n            \\\"084\\\": \\\"category\\\",\\n            \\\"092\\\": \\\"category\\\",\\n            \\\"093\\\": \\\"category\\\",\\n            \\\"094\\\": \\\"category\\\",\\n        }\\n    )\\n\\n    # for some reason y is of type Series\\n    # We need dataframe\\n    # y = y.to_frame()\\n\\n    # Convert labels from float to big integers, \\n    # Note: Type 'Category' won't work with categorization models (at least not with GaussianNB)\\n    y = y.multiply(multiply_factor)\\n    y = y.astype({\\\"095\\\": \\\"int\\\"})\\n\\n    return X, y\\n\\n\\ndef reverse_mod_lib_data(X, y, y_pred):\\n    \\\"\\\"\\\"\\n    Reverse the library data back to original format\\n    \\\"\\\"\\\"\\n\\n    # Sklearn GaussianNB doesn't handle NaN-values in input.\\n    # We fill the NaN values with 0 and now change it back\\n    X = X.replace(0, np.nan)\\n\\n    # Change datatypes for features and labels back to\\n    X = X.astype(\\n        {\\n            \\\"record_id\\\": \\\"int\\\",\\n            \\\"084\\\": \\\"float\\\",\\n            \\\"092\\\": \\\"float\\\",\\n            \\\"093\\\": \\\"float\\\",\\n            \\\"094\\\": \\\"float\\\",\\n        }\\n    )\\n\\n\\n    # Convert labels from category back to int\\n    y = y.multiply(1/multiply_factor)\\n    y_pred = y_pred.multiply(1/multiply_factor)\\n    \\n    return X, y, y_pred\\n\\ndef get_train_test_data(X, y, seed, stratify=True, test_size=0.2, shuffle=True):\\n    \\\"\\\"\\\"\\n    Split the data into training and test sets\\n    \\\"\\\"\\\"\\n\\n    # Stratify won't work with all datasets, it requires at least 2 rows for each label value\\n    if stratify:\\n        return train_test_split(\\n            X, y, test_size=test_size, shuffle=shuffle, stratify=y, random_state=seed\\n        )\\n\\n    else:\\n        return train_test_split(\\n            X, y, test_size=test_size, shuffle=shuffle, random_state=seed\\n        )\\n\\n\\ndef fit(model, scaler, X_train, X_test, y_train, y_test):\\n    \\\"\\\"\\\"\\n    Fit the model\\n    \\\"\\\"\\\"\\n\\n    pipe = Pipeline([(\\\"scaler\\\", scaler), (\\\"model\\\", model)])\\n    pipe.fit(X_train, y_train)\\n    err_train = pipe.score(X_train, y_train)\\n    err_test = pipe.score(X_test, y_test)\\n\\n    return pipe\\n\\n\\ndef predict(pipe, X):\\n    \\\"\\\"\\\"\\n    Use the model (pipe object) to predict labels\\n    \\\"\\\"\\\"\\n\\n    y_pred = pipe.predict(X)\\n    pred_probabilities = pipe.predict_proba(X)\\n\\n    # Print probabilities for first data point only\\n    #print(\\n    #    f\\\"\\\\nPredicted probability of each label for first data point:\\\\n{pred_probabilities[0]}\\\"\\n    #)\\n\\n    return y_pred\\n\\n\\ndef get_train_loss(pipe, X_train, y_train):\\n    '''\\n    Return train loss of fitted model\\n    '''\\n    \\n    return pipe.score(X_train, y_train)\\n\\n\\ndef get_test_loss(pipe, X_test, y_test):\\n    '''\\n    Return test loss of fitted model\\n    '''\\n    return pipe.score(X_test, y_test)\\n\\n\\ndef loss(pipe, X, y):\\n    \\\"\\\"\\\"\\n    Return loss (model quality metric)\\n\\n    Note that this may be a different metric than the one that the model optimizer is using (scoring method).\\n    For example for LogisticRegression the scoring method is mean accuracy,\\n    but we might want to track for example f1-score for loss because it is better balanced.\\n    \\\"\\\"\\\"  \\n    \\n    #return mean_squared_error(predict(pipe, X), y)\\n    return f1_score(y, predict(pipe, X), average='macro')\\n\\n    \\ndef print_loss(pipe, X, y, X_train, y_train, X_test, y_test, model_name, dataset_name):\\n    '''\\n    Print training and validation errors\\n    '''\\n    print(\\\"\\\\n******************************************************************\\\")\\n    print(f\\\"  Results for {model_name} with {dataset_name}:\\\")\\n    print(\\\"******************************************************************\\\")\\n    \\n    print(f\\\"Training error: {get_train_loss(pipe, X_train, y_train)}\\\")\\n    print(f\\\"Validation error: {get_test_loss(pipe, X_test, y_test)}\\\")\\n    print(f\\\"Loss: {loss(pipe, X, y)}\\\")\\n    \\n    #train_test_df = X_train.iloc[:,1:].copy()\\n    #train_test_df[\\\"prediction_correct\\\"] = (predict(pipe, X_train) - y_train.values == 0)\\n    #display(train_test_df.head)\\n    #_ = plot_trellis(train_test_df, legend_title=\\\"prediction\\\", true_label=\\\"correct\\\")\\n\\n    \\ndef print_details(\\n    X,\\n    y,\\n    y_pred,\\n    label_name=\\\"label\\\",\\n    pred_column_name=\\\"pred\\\",\\n    n_rows=10\\n):\\n    \\\"\\\"\\\"\\n    Print the results for observation\\n    \\\"\\\"\\\"\\n    \\n    y_compare = pd.concat([y, y_pred], axis=1)\\n\\n    print( f\\\"\\\\nOriginal and predicted labels (first {n_rows} rows):\\\")\\n    display(y_compare.head(n_rows))\\n\\n    X_compare = pd.concat([X, y_compare], axis=1)\\n    false_preds = X_compare[X_compare[label_name] != X_compare[pred_column_name]]\\n    n_false_preds = len(false_preds)\\n    n_right_preds = len(X_compare) - n_false_preds\\n    print(f\\\"Number of false predictions: {n_false_preds}\\\")\\n    print(f\\\"Number of right predictions: {n_right_preds}\\\")\\n    print(\\\"\\\\n\\\\nAll false predictions in dataset:\\\")\\n    display(false_preds)\\n    \\n    print(\\\"\\\\nHow different classifications correlate with each other on true and false predictions:\\\")\\n    X_compare[\\\"prediction_correct\\\"] = (X_compare[label_name] - X_compare[pred_column_name] == 0)\\n    #display(X_compare.head())\\n    #FIX THIS IMPORT!!!\\n    #_ = plot_trellis(X_compare.iloc[:,1:], legend_title=\\\"prediction\\\", true_label=\\\"correct\\\")\\n    \\n    \\ndef test_model(model, scaler, df, model_name, dataset_name, test_size=0.2):\\n    '''\\n    Test the model with the help of functions above\\n    '''\\n              \\n    label_name = \\\"095\\\"\\n    pred_column_name=\\\"095_PRED\\\"\\n    \\n    \\n    # Create features and labels\\n    X, y = split_X_y(df)\\n    \\n    \\n    # Modify library data as needed\\n    X, y = modify_lib_data(X, y)\\n    \\n    # Split data into training and test sets\\n    X_train, X_test, y_train, y_test = get_train_test_data(\\n        X, y, seed, stratify=False, test_size=test_size\\n    )\\n    \\n    # Fit and predict\\n    pipe = fit(model, scaler, X_train, X_test, y_train, y_test)\\n    y_pred = predict(pipe, X)\\n    \\n    # convert predictions from numpy to dataframe and set an easy column name\\n    y_pred = pd.DataFrame(y_pred)\\n    y_pred = y_pred.rename(columns={y_pred.columns[0]: pred_column_name})\\n\\n    print_loss(pipe, X, y, X_train, y_train, X_test, y_test, model_name,dataset_name)\\n    \\n    # Modify the library data back to original format\\n    X, y, y_pred = reverse_mod_lib_data(X, y, y_pred)\\n    \\n    # Print the results with desired column names\\n    print_details(\\n        X,\\n        y,\\n        y_pred,\\n        label_name,\\n        pred_column_name,\\n        30\\n    )\\n\\n    \\n    return pipe\";\n",
       "                var nbb_formatted_code = \"# export\\n\\n# FIX THIS IMPORT!!!!\\n# These functions won't work if we don't define these also in this code block\\n# from lib_classification.plot import plot_trellis, plot_histogram\\n\\n\\nseed = 0\\n\\n\\n\\\"\\\"\\\"\\nLabels can't be of type float for classification. Thus we multiply floats\\nso that there are no decimals. When printing the results we do the opposite.operation\\n\\nmax_decimals tells number of possible decimals in library classification.\\nSet max_decimals to 0 if you want to omit decimals alltogether\\n\\\"\\\"\\\"\\nmax_decimals = 6\\nmultiply_factor = 10 ** max_decimals\\n\\n\\\"\\\"\\\"\\nSklearn models can\\u00e4t handle NaN values, replace them with suitable value, defaul = 0\\n\\\"\\\"\\\"\\nreplace_nan = 0\\n\\n\\ndef split_X_y(df):\\n    \\\"\\\"\\\"\\n    Split dataframe into features and labels\\n    \\\"\\\"\\\"\\n    # X = df.iloc[:, :-1]  # .to_numpy()\\n    # y = df.iloc[:, -1]  # .to_numpy()\\n\\n    # for col in df.columns:\\n    #    print(f\\\"*{col}*\\\")\\n\\n    X = df.copy().reset_index(drop=True)\\n    y = X.pop(\\\"095\\\").reset_index(drop=True)\\n\\n    return X, y\\n\\n\\ndef modify_lib_data(X, y):\\n    \\\"\\\"\\\"\\n    Do the needed modification for library data\\n    \\\"\\\"\\\"\\n\\n    # Sklearn GaussianNB doesn't handle NaN-values in input.\\n    # We fill the NaN values with 0.\\n    X = X.fillna(replace_nan)\\n\\n    # Change datatypes for features and labels\\n    X = X.astype(\\n        {\\n            \\\"record_id\\\": \\\"int\\\",\\n            \\\"084\\\": \\\"category\\\",\\n            \\\"092\\\": \\\"category\\\",\\n            \\\"093\\\": \\\"category\\\",\\n            \\\"094\\\": \\\"category\\\",\\n        }\\n    )\\n\\n    # for some reason y is of type Series\\n    # We need dataframe\\n    # y = y.to_frame()\\n\\n    # Convert labels from float to big integers,\\n    # Note: Type 'Category' won't work with categorization models (at least not with GaussianNB)\\n    y = y.multiply(multiply_factor)\\n    y = y.astype({\\\"095\\\": \\\"int\\\"})\\n\\n    return X, y\\n\\n\\ndef reverse_mod_lib_data(X, y, y_pred):\\n    \\\"\\\"\\\"\\n    Reverse the library data back to original format\\n    \\\"\\\"\\\"\\n\\n    # Sklearn GaussianNB doesn't handle NaN-values in input.\\n    # We fill the NaN values with 0 and now change it back\\n    X = X.replace(0, np.nan)\\n\\n    # Change datatypes for features and labels back to\\n    X = X.astype(\\n        {\\n            \\\"record_id\\\": \\\"int\\\",\\n            \\\"084\\\": \\\"float\\\",\\n            \\\"092\\\": \\\"float\\\",\\n            \\\"093\\\": \\\"float\\\",\\n            \\\"094\\\": \\\"float\\\",\\n        }\\n    )\\n\\n    # Convert labels from category back to int\\n    y = y.multiply(1 / multiply_factor)\\n    y_pred = y_pred.multiply(1 / multiply_factor)\\n\\n    return X, y, y_pred\\n\\n\\ndef get_train_test_data(X, y, seed, stratify=True, test_size=0.2, shuffle=True):\\n    \\\"\\\"\\\"\\n    Split the data into training and test sets\\n    \\\"\\\"\\\"\\n\\n    # Stratify won't work with all datasets, it requires at least 2 rows for each label value\\n    if stratify:\\n        return train_test_split(\\n            X, y, test_size=test_size, shuffle=shuffle, stratify=y, random_state=seed\\n        )\\n\\n    else:\\n        return train_test_split(\\n            X, y, test_size=test_size, shuffle=shuffle, random_state=seed\\n        )\\n\\n\\ndef fit(model, scaler, X_train, X_test, y_train, y_test):\\n    \\\"\\\"\\\"\\n    Fit the model\\n    \\\"\\\"\\\"\\n\\n    pipe = Pipeline([(\\\"scaler\\\", scaler), (\\\"model\\\", model)])\\n    pipe.fit(X_train, y_train)\\n    err_train = pipe.score(X_train, y_train)\\n    err_test = pipe.score(X_test, y_test)\\n\\n    return pipe\\n\\n\\ndef predict(pipe, X):\\n    \\\"\\\"\\\"\\n    Use the model (pipe object) to predict labels\\n    \\\"\\\"\\\"\\n\\n    y_pred = pipe.predict(X)\\n    pred_probabilities = pipe.predict_proba(X)\\n\\n    # Print probabilities for first data point only\\n    # print(\\n    #    f\\\"\\\\nPredicted probability of each label for first data point:\\\\n{pred_probabilities[0]}\\\"\\n    # )\\n\\n    return y_pred\\n\\n\\ndef get_train_loss(pipe, X_train, y_train):\\n    \\\"\\\"\\\"\\n    Return train loss of fitted model\\n    \\\"\\\"\\\"\\n\\n    return pipe.score(X_train, y_train)\\n\\n\\ndef get_test_loss(pipe, X_test, y_test):\\n    \\\"\\\"\\\"\\n    Return test loss of fitted model\\n    \\\"\\\"\\\"\\n    return pipe.score(X_test, y_test)\\n\\n\\ndef loss(pipe, X, y):\\n    \\\"\\\"\\\"\\n    Return loss (model quality metric)\\n\\n    Note that this may be a different metric than the one that the model optimizer is using (scoring method).\\n    For example for LogisticRegression the scoring method is mean accuracy,\\n    but we might want to track for example f1-score for loss because it is better balanced.\\n    \\\"\\\"\\\"\\n\\n    # return mean_squared_error(predict(pipe, X), y)\\n    return f1_score(y, predict(pipe, X), average=\\\"macro\\\")\\n\\n\\ndef print_loss(pipe, X, y, X_train, y_train, X_test, y_test, model_name, dataset_name):\\n    \\\"\\\"\\\"\\n    Print training and validation errors\\n    \\\"\\\"\\\"\\n    print(\\\"\\\\n******************************************************************\\\")\\n    print(f\\\"  Results for {model_name} with {dataset_name}:\\\")\\n    print(\\\"******************************************************************\\\")\\n\\n    print(f\\\"Training error: {get_train_loss(pipe, X_train, y_train)}\\\")\\n    print(f\\\"Validation error: {get_test_loss(pipe, X_test, y_test)}\\\")\\n    print(f\\\"Loss: {loss(pipe, X, y)}\\\")\\n\\n    # train_test_df = X_train.iloc[:,1:].copy()\\n    # train_test_df[\\\"prediction_correct\\\"] = (predict(pipe, X_train) - y_train.values == 0)\\n    # display(train_test_df.head)\\n    # _ = plot_trellis(train_test_df, legend_title=\\\"prediction\\\", true_label=\\\"correct\\\")\\n\\n\\ndef print_details(X, y, y_pred, label_name=\\\"label\\\", pred_column_name=\\\"pred\\\", n_rows=10):\\n    \\\"\\\"\\\"\\n    Print the results for observation\\n    \\\"\\\"\\\"\\n\\n    y_compare = pd.concat([y, y_pred], axis=1)\\n\\n    print(f\\\"\\\\nOriginal and predicted labels (first {n_rows} rows):\\\")\\n    display(y_compare.head(n_rows))\\n\\n    X_compare = pd.concat([X, y_compare], axis=1)\\n    false_preds = X_compare[X_compare[label_name] != X_compare[pred_column_name]]\\n    n_false_preds = len(false_preds)\\n    n_right_preds = len(X_compare) - n_false_preds\\n    print(f\\\"Number of false predictions: {n_false_preds}\\\")\\n    print(f\\\"Number of right predictions: {n_right_preds}\\\")\\n    print(\\\"\\\\n\\\\nAll false predictions in dataset:\\\")\\n    display(false_preds)\\n\\n    print(\\n        \\\"\\\\nHow different classifications correlate with each other on true and false predictions:\\\"\\n    )\\n    X_compare[\\\"prediction_correct\\\"] = (\\n        X_compare[label_name] - X_compare[pred_column_name] == 0\\n    )\\n    # display(X_compare.head())\\n    # FIX THIS IMPORT!!!\\n    # _ = plot_trellis(X_compare.iloc[:,1:], legend_title=\\\"prediction\\\", true_label=\\\"correct\\\")\\n\\n\\ndef test_model(model, scaler, df, model_name, dataset_name, test_size=0.2):\\n    \\\"\\\"\\\"\\n    Test the model with the help of functions above\\n    \\\"\\\"\\\"\\n\\n    label_name = \\\"095\\\"\\n    pred_column_name = \\\"095_PRED\\\"\\n\\n    # Create features and labels\\n    X, y = split_X_y(df)\\n\\n    # Modify library data as needed\\n    X, y = modify_lib_data(X, y)\\n\\n    # Split data into training and test sets\\n    X_train, X_test, y_train, y_test = get_train_test_data(\\n        X, y, seed, stratify=False, test_size=test_size\\n    )\\n\\n    # Fit and predict\\n    pipe = fit(model, scaler, X_train, X_test, y_train, y_test)\\n    y_pred = predict(pipe, X)\\n\\n    # convert predictions from numpy to dataframe and set an easy column name\\n    y_pred = pd.DataFrame(y_pred)\\n    y_pred = y_pred.rename(columns={y_pred.columns[0]: pred_column_name})\\n\\n    print_loss(pipe, X, y, X_train, y_train, X_test, y_test, model_name, dataset_name)\\n\\n    # Modify the library data back to original format\\n    X, y, y_pred = reverse_mod_lib_data(X, y, y_pred)\\n\\n    # Print the results with desired column names\\n    print_details(X, y, y_pred, label_name, pred_column_name, 30)\\n\\n    return pipe\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "# FIX THIS IMPORT!!!!\n",
    "# These functions won't work if we don't define these also in this code block\n",
    "#from lib_classification.plot import plot_trellis, plot_histogram\n",
    "\n",
    "\n",
    "seed = 0\n",
    "\n",
    "\n",
    "'''\n",
    "Labels can't be of type float for classification. Thus we multiply floats\n",
    "so that there are no decimals. When printing the results we do the opposite.operation\n",
    "\n",
    "max_decimals tells number of possible decimals in library classification.\n",
    "Set max_decimals to 0 if you want to omit decimals alltogether\n",
    "'''\n",
    "max_decimals = 6  \n",
    "multiply_factor = 10 ** max_decimals\n",
    "\n",
    "'''\n",
    "Sklearn models canät handle NaN values, replace them with suitable value, defaul = 0\n",
    "'''\n",
    "replace_nan = 0\n",
    "\n",
    "def split_X_y(df):\n",
    "    \"\"\"\n",
    "    Split dataframe into features and labels\n",
    "    \"\"\"\n",
    "    #X = df.iloc[:, :-1]  # .to_numpy()\n",
    "    #y = df.iloc[:, -1]  # .to_numpy()\n",
    "    \n",
    "    #for col in df.columns:\n",
    "    #    print(f\"*{col}*\")\n",
    "    \n",
    "    X = df.copy().reset_index(drop=True)\n",
    "    y = X.pop(\"095\").reset_index(drop=True)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def modify_lib_data(X, y):\n",
    "    \"\"\"\n",
    "    Do the needed modification for library data\n",
    "    \"\"\"\n",
    "\n",
    "    # Sklearn GaussianNB doesn't handle NaN-values in input.\n",
    "    # We fill the NaN values with 0.\n",
    "    X = X.fillna(replace_nan)\n",
    "\n",
    "    # Change datatypes for features and labels\n",
    "    X = X.astype(\n",
    "        {\n",
    "            \"record_id\": \"int\",\n",
    "            \"084\": \"category\",\n",
    "            \"092\": \"category\",\n",
    "            \"093\": \"category\",\n",
    "            \"094\": \"category\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # for some reason y is of type Series\n",
    "    # We need dataframe\n",
    "    # y = y.to_frame()\n",
    "\n",
    "    # Convert labels from float to big integers, \n",
    "    # Note: Type 'Category' won't work with categorization models (at least not with GaussianNB)\n",
    "    y = y.multiply(multiply_factor)\n",
    "    y = y.astype({\"095\": \"int\"})\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def reverse_mod_lib_data(X, y, y_pred):\n",
    "    \"\"\"\n",
    "    Reverse the library data back to original format\n",
    "    \"\"\"\n",
    "\n",
    "    # Sklearn GaussianNB doesn't handle NaN-values in input.\n",
    "    # We fill the NaN values with 0 and now change it back\n",
    "    X = X.replace(0, np.nan)\n",
    "\n",
    "    # Change datatypes for features and labels back to\n",
    "    X = X.astype(\n",
    "        {\n",
    "            \"record_id\": \"int\",\n",
    "            \"084\": \"float\",\n",
    "            \"092\": \"float\",\n",
    "            \"093\": \"float\",\n",
    "            \"094\": \"float\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    # Convert labels from category back to int\n",
    "    y = y.multiply(1/multiply_factor)\n",
    "    y_pred = y_pred.multiply(1/multiply_factor)\n",
    "    \n",
    "    return X, y, y_pred\n",
    "\n",
    "def get_train_test_data(X, y, seed, stratify=True, test_size=0.2, shuffle=True):\n",
    "    \"\"\"\n",
    "    Split the data into training and test sets\n",
    "    \"\"\"\n",
    "\n",
    "    # Stratify won't work with all datasets, it requires at least 2 rows for each label value\n",
    "    if stratify:\n",
    "        return train_test_split(\n",
    "            X, y, test_size=test_size, shuffle=shuffle, stratify=y, random_state=seed\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        return train_test_split(\n",
    "            X, y, test_size=test_size, shuffle=shuffle, random_state=seed\n",
    "        )\n",
    "\n",
    "\n",
    "def fit(model, scaler, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Fit the model\n",
    "    \"\"\"\n",
    "\n",
    "    pipe = Pipeline([(\"scaler\", scaler), (\"model\", model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    err_train = pipe.score(X_train, y_train)\n",
    "    err_test = pipe.score(X_test, y_test)\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def predict(pipe, X):\n",
    "    \"\"\"\n",
    "    Use the model (pipe object) to predict labels\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = pipe.predict(X)\n",
    "    pred_probabilities = pipe.predict_proba(X)\n",
    "\n",
    "    # Print probabilities for first data point only\n",
    "    #print(\n",
    "    #    f\"\\nPredicted probability of each label for first data point:\\n{pred_probabilities[0]}\"\n",
    "    #)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def get_train_loss(pipe, X_train, y_train):\n",
    "    '''\n",
    "    Return train loss of fitted model\n",
    "    '''\n",
    "    \n",
    "    return pipe.score(X_train, y_train)\n",
    "\n",
    "\n",
    "def get_test_loss(pipe, X_test, y_test):\n",
    "    '''\n",
    "    Return test loss of fitted model\n",
    "    '''\n",
    "    return pipe.score(X_test, y_test)\n",
    "\n",
    "\n",
    "def loss(pipe, X, y):\n",
    "    \"\"\"\n",
    "    Return loss (model quality metric)\n",
    "\n",
    "    Note that this may be a different metric than the one that the model optimizer is using (scoring method).\n",
    "    For example for LogisticRegression the scoring method is mean accuracy,\n",
    "    but we might want to track for example f1-score for loss because it is better balanced.\n",
    "    \"\"\"  \n",
    "    \n",
    "    #return mean_squared_error(predict(pipe, X), y)\n",
    "    return f1_score(y, predict(pipe, X), average='macro')\n",
    "\n",
    "    \n",
    "def print_loss(pipe, X, y, X_train, y_train, X_test, y_test, model_name, dataset_name):\n",
    "    '''\n",
    "    Print training and validation errors\n",
    "    '''\n",
    "    print(\"\\n******************************************************************\")\n",
    "    print(f\"  Results for {model_name} with {dataset_name}:\")\n",
    "    print(\"******************************************************************\")\n",
    "    \n",
    "    print(f\"Training error: {get_train_loss(pipe, X_train, y_train)}\")\n",
    "    print(f\"Validation error: {get_test_loss(pipe, X_test, y_test)}\")\n",
    "    print(f\"Loss: {loss(pipe, X, y)}\")\n",
    "    \n",
    "    #train_test_df = X_train.iloc[:,1:].copy()\n",
    "    #train_test_df[\"prediction_correct\"] = (predict(pipe, X_train) - y_train.values == 0)\n",
    "    #display(train_test_df.head)\n",
    "    #_ = plot_trellis(train_test_df, legend_title=\"prediction\", true_label=\"correct\")\n",
    "\n",
    "    \n",
    "def print_details(\n",
    "    X,\n",
    "    y,\n",
    "    y_pred,\n",
    "    label_name=\"label\",\n",
    "    pred_column_name=\"pred\",\n",
    "    n_rows=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Print the results for observation\n",
    "    \"\"\"\n",
    "    \n",
    "    y_compare = pd.concat([y, y_pred], axis=1)\n",
    "\n",
    "    print( f\"\\nOriginal and predicted labels (first {n_rows} rows):\")\n",
    "    display(y_compare.head(n_rows))\n",
    "\n",
    "    X_compare = pd.concat([X, y_compare], axis=1)\n",
    "    false_preds = X_compare[X_compare[label_name] != X_compare[pred_column_name]]\n",
    "    n_false_preds = len(false_preds)\n",
    "    n_right_preds = len(X_compare) - n_false_preds\n",
    "    print(f\"Number of false predictions: {n_false_preds}\")\n",
    "    print(f\"Number of right predictions: {n_right_preds}\")\n",
    "    print(\"\\n\\nAll false predictions in dataset:\")\n",
    "    display(false_preds)\n",
    "    \n",
    "    print(\"\\nHow different classifications correlate with each other on true and false predictions:\")\n",
    "    X_compare[\"prediction_correct\"] = (X_compare[label_name] - X_compare[pred_column_name] == 0)\n",
    "    #display(X_compare.head())\n",
    "    #FIX THIS IMPORT!!!\n",
    "    #_ = plot_trellis(X_compare.iloc[:,1:], legend_title=\"prediction\", true_label=\"correct\")\n",
    "    \n",
    "    \n",
    "def test_model(model, scaler, df, model_name, dataset_name, test_size=0.2):\n",
    "    '''\n",
    "    Test the model with the help of functions above\n",
    "    '''\n",
    "              \n",
    "    label_name = \"095\"\n",
    "    pred_column_name=\"095_PRED\"\n",
    "    \n",
    "    \n",
    "    # Create features and labels\n",
    "    X, y = split_X_y(df)\n",
    "    \n",
    "    \n",
    "    # Modify library data as needed\n",
    "    X, y = modify_lib_data(X, y)\n",
    "    \n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = get_train_test_data(\n",
    "        X, y, seed, stratify=False, test_size=test_size\n",
    "    )\n",
    "    \n",
    "    # Fit and predict\n",
    "    pipe = fit(model, scaler, X_train, X_test, y_train, y_test)\n",
    "    y_pred = predict(pipe, X)\n",
    "    \n",
    "    # convert predictions from numpy to dataframe and set an easy column name\n",
    "    y_pred = pd.DataFrame(y_pred)\n",
    "    y_pred = y_pred.rename(columns={y_pred.columns[0]: pred_column_name})\n",
    "\n",
    "    print_loss(pipe, X, y, X_train, y_train, X_test, y_test, model_name,dataset_name)\n",
    "    \n",
    "    # Modify the library data back to original format\n",
    "    X, y, y_pred = reverse_mod_lib_data(X, y, y_pred)\n",
    "    \n",
    "    # Print the results with desired column names\n",
    "    print_details(\n",
    "        X,\n",
    "        y,\n",
    "        y_pred,\n",
    "        label_name,\n",
    "        pred_column_name,\n",
    "        30\n",
    "    )\n",
    "\n",
    "    \n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by splitting our data to train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by selecting and tuning our ML model with the help of general functions we created above.\n",
    "[Choosing the right estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
    "\n",
    "We probably want to try at least these models:\n",
    " - [linear SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
    " - [SGD classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html?highlight=sgd%20classifier#sklearn.linear_model.SGDClassifier)\n",
    " - [KNeighbors classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=kneighbors#sklearn.neighbors.KNeighborsClassifier)\n",
    " - [Decision tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontree#sklearn.tree.DecisionTreeClassifier)\n",
    " - [Random forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforest#sklearn.ensemble.RandomForestClassifier)\n",
    " - [Naive Bayes](https://scikit-learn.org/stable/modules/classes.html?highlight=naive%20bayes#module-sklearn.naive_bayes) (Sami)\n",
    "\n",
    "Some useful links for choosing the estimator\n",
    " - [Classifier comparison](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    " - [Choosing the right estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************\n",
      "  Results for GAUSSIAN NAIVE BAYES with TOY DATASET:\n",
      "******************************************************************\n",
      "Training error: 1.0\n",
      "Validation error: 0.8333333333333334\n",
      "Loss: 0.6608187134502924\n",
      "\n",
      "Original and predicted labels (first 30 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>095</th>\n",
       "      <th>095_PRED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>796.3</td>\n",
       "      <td>796.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>796.2</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>796.1</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      095  095_PRED\n",
       "0   796.1     796.1\n",
       "1   796.1     796.1\n",
       "2   796.1     796.1\n",
       "3   796.1     796.1\n",
       "4   796.1     796.1\n",
       "5   796.1     796.1\n",
       "6   796.1     796.1\n",
       "7   796.1     796.1\n",
       "8   796.1     796.1\n",
       "9   796.1     796.1\n",
       "10  796.1     796.1\n",
       "11  796.1     796.1\n",
       "12  796.1     796.1\n",
       "13  796.1     796.1\n",
       "14  796.1     796.1\n",
       "15  796.1     796.1\n",
       "16  796.1     796.1\n",
       "17  796.1     796.1\n",
       "18  796.1     796.1\n",
       "19  796.1     796.1\n",
       "20  796.3     796.3\n",
       "21  796.1     796.1\n",
       "22  796.1     796.1\n",
       "23  796.1     796.1\n",
       "24  796.1     796.1\n",
       "25  796.1     796.1\n",
       "26  796.1     796.1\n",
       "27  796.1     796.1\n",
       "28  796.2     796.1\n",
       "29  796.1     796.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of false predictions: 1\n",
      "Number of right predictions: 29\n",
      "\n",
      "\n",
      "All false predictions in dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>084</th>\n",
       "      <th>092</th>\n",
       "      <th>093</th>\n",
       "      <th>094</th>\n",
       "      <th>jalkapallo</th>\n",
       "      <th>historia</th>\n",
       "      <th>urheilijat</th>\n",
       "      <th>urheiluseurat</th>\n",
       "      <th>urheiluliigat</th>\n",
       "      <th>...</th>\n",
       "      <th>skandaalit</th>\n",
       "      <th>epärehellisyys</th>\n",
       "      <th>vertailu</th>\n",
       "      <th>uppslagsverk</th>\n",
       "      <th>yhteiskunta</th>\n",
       "      <th>onnistuminen</th>\n",
       "      <th>statistik</th>\n",
       "      <th>homoseksuaalisuus</th>\n",
       "      <th>095</th>\n",
       "      <th>095_PRED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>420908975453</td>\n",
       "      <td>79.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>796.2</td>\n",
       "      <td>796.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 298 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       record_id    084  092  093  094  jalkapallo  historia  urheilijat  \\\n",
       "28  420908975453  79.31  NaN  NaN  NaN         NaN       NaN         NaN   \n",
       "\n",
       "    urheiluseurat  urheiluliigat  ...  skandaalit  epärehellisyys  vertailu  \\\n",
       "28            NaN            NaN  ...         NaN             NaN       NaN   \n",
       "\n",
       "    uppslagsverk  yhteiskunta  onnistuminen  statistik  homoseksuaalisuus  \\\n",
       "28           NaN          NaN           NaN        NaN                NaN   \n",
       "\n",
       "      095  095_PRED  \n",
       "28  796.2     796.1  \n",
       "\n",
       "[1 rows x 298 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How different classifications correlate with each other on true and false predictions:\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 135;\n",
       "                var nbb_unformatted_code = \"# Define some initial params\\nk = 5\\ntest_size = 0.2\\n\\n#######################################\\n# Test Naive Bayes model\\n#######################################\\n\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.naive_bayes import ComplementNB\\n\\n#for col in toy_df.columns:\\n#    print(f\\\"*{col}*\\\")\\n          \\n_ = test_model(\\n    GaussianNB(),\\n    StandardScaler(),\\n    toy_df,\\n    \\\"GAUSSIAN NAIVE BAYES\\\",\\n    \\\"TOY DATASET\\\",\\n    test_size,\\n)\\n\\n#\\n# Much worse results with complementNB\\n#\\n# test_model(\\n#\\n# ComplementNB(),\\n#    MinMaxScaler(),\\n#    toy_df,\\n#    \\\"COMPLEMENT NAIVE BAYES\\\",\\n#    \\\"TOY DATASET\\\",\\n#    test_size,\\n# )\\n#\\n\\n#\\n# Skip hyperparameter tuning for now, maybe implement this later\\n#\\n\\n#cv = StratifiedKFold(n_splits=k)\\n#print(cross_val_score(pipe, X_train, y_train, cv=cv))\\n\\n## optimize\\n#param_grid = {\\n#    \\\"estimator__C\\\": np.logspace(-4, 4, 10),\\n#}\\n\\n# make_pipeline(Imputer(),StandardScaler(),PCA(n_components=2),SVC(random_state=1))\\n\\n# cv = StratifiedKFold(n_splits=5)\\n#gs = GridSearchCV(\\n#    estimator=pipe,\\n#    param_grid=param_grid,\\n#    scoring=\\\"accuracy\\\",\\n#    cv=cv,\\n#    return_train_score=True,\\n#)\\n#gs.fit(X_train, y_train)\\n#\\n#print(\\\"Best Estimator: \\\\n{}\\\\n\\\".format(gs.best_estimator_))\\n#print(\\\"Best Parameters: \\\\n{}\\\\n\\\".format(gs.best_params_))\\n#print(\\\"Best Test Score: \\\\n{}\\\\n\\\".format(gs.best_score_))\\n#print(\\n#    \\\"Best Training Score: \\\\n{}\\\\n\\\".format(\\n#        gs.cv_results_[\\\"mean_train_score\\\"][gs.best_index_]\\n#    )\\n#)\\n#print(\\\"All Training Scores: \\\\n{}\\\\n\\\".format(gs.cv_results_[\\\"mean_train_score\\\"]))\\n#print(\\\"All Test Scores: \\\\n{}\\\\n\\\".format(gs.cv_results_[\\\"mean_test_score\\\"]))\\n# # This prints out all results during Cross-Validation in details\\n# print(\\\"All Meta Results During CV Search: \\\\n{}\\\\n\\\".format(gs.cv_results_))\\n\\n# Reset pipeline with best params\\n#pipe.set_params(estimator__C=gs.best_params_[\\\"estimator__C\\\"])\\n#pipe.fit(X_train, y_train)\\n#print(\\\"Test score with best params (should equal to Best Test Score above)\\\")\\n#print(pipe.score(X_test, y_test))\";\n",
       "                var nbb_formatted_code = \"# Define some initial params\\nk = 5\\ntest_size = 0.2\\n\\n#######################################\\n# Test Naive Bayes model\\n#######################################\\n\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.naive_bayes import ComplementNB\\n\\n# for col in toy_df.columns:\\n#    print(f\\\"*{col}*\\\")\\n\\n_ = test_model(\\n    GaussianNB(),\\n    StandardScaler(),\\n    toy_df,\\n    \\\"GAUSSIAN NAIVE BAYES\\\",\\n    \\\"TOY DATASET\\\",\\n    test_size,\\n)\\n\\n#\\n# Much worse results with complementNB\\n#\\n# test_model(\\n#\\n# ComplementNB(),\\n#    MinMaxScaler(),\\n#    toy_df,\\n#    \\\"COMPLEMENT NAIVE BAYES\\\",\\n#    \\\"TOY DATASET\\\",\\n#    test_size,\\n# )\\n#\\n\\n#\\n# Skip hyperparameter tuning for now, maybe implement this later\\n#\\n\\n# cv = StratifiedKFold(n_splits=k)\\n# print(cross_val_score(pipe, X_train, y_train, cv=cv))\\n\\n## optimize\\n# param_grid = {\\n#    \\\"estimator__C\\\": np.logspace(-4, 4, 10),\\n# }\\n\\n# make_pipeline(Imputer(),StandardScaler(),PCA(n_components=2),SVC(random_state=1))\\n\\n# cv = StratifiedKFold(n_splits=5)\\n# gs = GridSearchCV(\\n#    estimator=pipe,\\n#    param_grid=param_grid,\\n#    scoring=\\\"accuracy\\\",\\n#    cv=cv,\\n#    return_train_score=True,\\n# )\\n# gs.fit(X_train, y_train)\\n#\\n# print(\\\"Best Estimator: \\\\n{}\\\\n\\\".format(gs.best_estimator_))\\n# print(\\\"Best Parameters: \\\\n{}\\\\n\\\".format(gs.best_params_))\\n# print(\\\"Best Test Score: \\\\n{}\\\\n\\\".format(gs.best_score_))\\n# print(\\n#    \\\"Best Training Score: \\\\n{}\\\\n\\\".format(\\n#        gs.cv_results_[\\\"mean_train_score\\\"][gs.best_index_]\\n#    )\\n# )\\n# print(\\\"All Training Scores: \\\\n{}\\\\n\\\".format(gs.cv_results_[\\\"mean_train_score\\\"]))\\n# print(\\\"All Test Scores: \\\\n{}\\\\n\\\".format(gs.cv_results_[\\\"mean_test_score\\\"]))\\n# # This prints out all results during Cross-Validation in details\\n# print(\\\"All Meta Results During CV Search: \\\\n{}\\\\n\\\".format(gs.cv_results_))\\n\\n# Reset pipeline with best params\\n# pipe.set_params(estimator__C=gs.best_params_[\\\"estimator__C\\\"])\\n# pipe.fit(X_train, y_train)\\n# print(\\\"Test score with best params (should equal to Best Test Score above)\\\")\\n# print(pipe.score(X_test, y_test))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define some initial params\n",
    "k = 5\n",
    "test_size = 0.2\n",
    "\n",
    "#######################################\n",
    "# Test Naive Bayes model\n",
    "#######################################\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "#for col in toy_df.columns:\n",
    "#    print(f\"*{col}*\")\n",
    "          \n",
    "_ = test_model(\n",
    "    GaussianNB(),\n",
    "    StandardScaler(),\n",
    "    toy_df,\n",
    "    \"GAUSSIAN NAIVE BAYES\",\n",
    "    \"TOY DATASET\",\n",
    "    test_size,\n",
    ")\n",
    "\n",
    "#\n",
    "# Much worse results with complementNB\n",
    "#\n",
    "# test_model(\n",
    "#\n",
    "# ComplementNB(),\n",
    "#    MinMaxScaler(),\n",
    "#    toy_df,\n",
    "#    \"COMPLEMENT NAIVE BAYES\",\n",
    "#    \"TOY DATASET\",\n",
    "#    test_size,\n",
    "# )\n",
    "#\n",
    "\n",
    "#\n",
    "# Skip hyperparameter tuning for now, maybe implement this later\n",
    "#\n",
    "\n",
    "#cv = StratifiedKFold(n_splits=k)\n",
    "#print(cross_val_score(pipe, X_train, y_train, cv=cv))\n",
    "\n",
    "## optimize\n",
    "#param_grid = {\n",
    "#    \"estimator__C\": np.logspace(-4, 4, 10),\n",
    "#}\n",
    "\n",
    "# make_pipeline(Imputer(),StandardScaler(),PCA(n_components=2),SVC(random_state=1))\n",
    "\n",
    "# cv = StratifiedKFold(n_splits=5)\n",
    "#gs = GridSearchCV(\n",
    "#    estimator=pipe,\n",
    "#    param_grid=param_grid,\n",
    "#    scoring=\"accuracy\",\n",
    "#    cv=cv,\n",
    "#    return_train_score=True,\n",
    "#)\n",
    "#gs.fit(X_train, y_train)\n",
    "#\n",
    "#print(\"Best Estimator: \\n{}\\n\".format(gs.best_estimator_))\n",
    "#print(\"Best Parameters: \\n{}\\n\".format(gs.best_params_))\n",
    "#print(\"Best Test Score: \\n{}\\n\".format(gs.best_score_))\n",
    "#print(\n",
    "#    \"Best Training Score: \\n{}\\n\".format(\n",
    "#        gs.cv_results_[\"mean_train_score\"][gs.best_index_]\n",
    "#    )\n",
    "#)\n",
    "#print(\"All Training Scores: \\n{}\\n\".format(gs.cv_results_[\"mean_train_score\"]))\n",
    "#print(\"All Test Scores: \\n{}\\n\".format(gs.cv_results_[\"mean_test_score\"]))\n",
    "# # This prints out all results during Cross-Validation in details\n",
    "# print(\"All Meta Results During CV Search: \\n{}\\n\".format(gs.cv_results_))\n",
    "\n",
    "# Reset pipeline with best params\n",
    "#pipe.set_params(estimator__C=gs.best_params_[\"estimator__C\"])\n",
    "#pipe.fit(X_train, y_train)\n",
    "#print(\"Test score with best params (should equal to Best Test Score above)\")\n",
    "#print(pipe.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 136;\n",
       "                var nbb_unformatted_code = \"#\\n# Samuli: you can implement your toy dataset model testing here (use the functions above if they are suitable for you)\\n#\";\n",
       "                var nbb_formatted_code = \"#\\n# Samuli: you can implement your toy dataset model testing here (use the functions above if they are suitable for you)\\n#\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Samuli: you can implement your toy dataset model testing here (use the functions above if they are suitable for you)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 137;\n",
       "                var nbb_unformatted_code = \"#\\n# Jarkko: you can implement your toy dataset model testing here (use the functions above if they are suitable for you)\\n#\";\n",
       "                var nbb_formatted_code = \"#\\n# Jarkko: you can implement your toy dataset model testing here (use the functions above if they are suitable for you)\\n#\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Jarkko: you can implement your toy dataset model testing here (use the functions above if they are suitable for you)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 138;\n",
       "                var nbb_unformatted_code = \"#\\n# Timo: you can implement your toy dataset model testing here (use the functions above if they are suitable for you)\\n#\";\n",
       "                var nbb_formatted_code = \"#\\n# Timo: you can implement your toy dataset model testing here (use the functions above if they are suitable for you)\\n#\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Timo: you can implement your toy dataset model testing here (use the functions above if they are suitable for you)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 139;\n",
       "                var nbb_unformatted_code = \"#\\n# Eila: you can implement your toy dataset model testing here (use the functions above if they are suitable for you)\\n#\";\n",
       "                var nbb_formatted_code = \"#\\n# Eila: you can implement your toy dataset model testing here (use the functions above if they are suitable for you)\\n#\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Eila: you can implement your toy dataset model testing here (use the functions above if they are suitable for you)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 140;\n",
       "                var nbb_unformatted_code = \"#\\n# Mari: you can implement your toy dataset model testing here (use the functions above if they are suitable for you)\\n#\";\n",
       "                var nbb_formatted_code = \"#\\n# Mari: you can implement your toy dataset model testing here (use the functions above if they are suitable for you)\\n#\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Mari: you can implement your toy dataset model testing here (use the functions above if they are suitable for you)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace or remove this!!!\n",
    "Ok, so we can fit a model to the data and it appears to do something (with the small test data we can not necessarily say if it's meaningful).\n",
    "\n",
    "However, how would you take this script into production?\n",
    "How would you scale it, or use it with completely different setup of data and parameters?\n",
    "(well, this is a tiny example, so we could actually easily parameterize a script, but that's rarely the case in real world applications)\n",
    "\n",
    "This is why we need to construct a model class, to hold all of the steps required in separate, tidy functions.\n",
    "Then we can recreate the model and the steps with different data, without copy-pasting or manually editing all the tiny details.\n",
    "\n",
    "Follow along the example - you'll see, that half the work was done in the scripting cells above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "#### Gaussian Naive Bayes\n",
    " - We can't use float as a type in label (even the casting to type 'Category' didn't help)\n",
    "    - Workaround: multiply the HKLJ-CLASS with a big number and cast it to int\n",
    " - Sklearn libraries do not accept NaN values\n",
    "    - Workaround: replace NaN-values with 0\n",
    " - There are lots of rows where there is no info at all about other library classification\n",
    "    - Use keywords as additional info\n",
    "    - Omit the the rows where there is no class-information at all from other classification systems\n",
    "\n",
    "#### Complement Naive Bayes\n",
    " - This is an enhancement of Multinomial Naive Bayes\n",
    " - We can't use StandardScaler\n",
    "     - For unknown reason algorithm returns error \"negative values in input\"\n",
    "     - Workaround: use MinMaxScaler\n",
    " - Quick testing show much worse results than Gaussian Naive Bayes\n",
    " \n",
    " \n",
    "## NOTE: Sami's modifications end here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define base class for your ML model\n",
    "\n",
    "You will probably do just fine creating a one simple class that does not inherit anything.\n",
    "You can then use this example base class as a template for your machine learning class. \n",
    "However, this is not always the case, and class inheritance is one of the most useful features of Python (and object oriented programming).\n",
    "This is why we wanted to demonstrate a base class - subclass division.\n",
    "\n",
    "Here we define the base class `MachineLearningModel` that holds some simple functions for handling data, that would be common for all subclasses.\n",
    "If a function only contains a `pass`-statement, it will be defined in the subclass.\n",
    "\n",
    "> **Note**: in this example the model instance contains the data. \n",
    "This is rarely applicable in practice if the data is large.\n",
    "Instead, in most applications the model should be routed to query the data when needed, in a similar way that it would appear as if the model instance contained the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# Define base class for our classifier\n",
    "class MachineLearningModel:\n",
    "    \"\"\"\n",
    "    Overly simplified example for a base class:\n",
    "\n",
    "    data handling operations\n",
    "\n",
    "    handle definitions of other functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, n_splits=5, seed=0):\n",
    "        self.n_splits = n_splits  # k-fold n_splits\n",
    "        self.seed = seed  # random state\n",
    "\n",
    "        self.set_data(X, y)  # init model data (see below)\n",
    "\n",
    "    def set_data(self, X, y):\n",
    "        \"\"\"\n",
    "        Set traing and evaluation data\n",
    "        \"\"\"\n",
    "        self.X = X.copy()\n",
    "        self.y = y.copy()\n",
    "\n",
    "        # in addition we separate train and test data:\n",
    "        self.__create_train_test_data()  # see below\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_data(self) -> (np.ndarray, np.ndarray):\n",
    "        \"\"\"\n",
    "        Get training and evaluation data\n",
    "        \"\"\"\n",
    "        return self.X.copy(), self.y.copy()\n",
    "\n",
    "    def __create_train_test_data(self, n_splits=None, seed=None):\n",
    "        \"\"\"\n",
    "        Create training and testing data\n",
    "        \"\"\"\n",
    "        # you might want to control the seed:\n",
    "        if seed is None:\n",
    "            seed = self.seed\n",
    "\n",
    "        # you might want to control the number of splits\n",
    "        if n_splits is None:\n",
    "            n_splits = self.n_splits\n",
    "\n",
    "        # split train and test data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.X, self.y, test_size=(1 / n_splits), random_state=seed, stratify=self.y\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_train_test_data(self):\n",
    "        \"\"\"\n",
    "        Return X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "\n",
    "    def fit(self, X=None, y=None, **fit_params):\n",
    "        \"\"\"\n",
    "        To be defined in the subclass\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        To be defined in the subclass\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        \"\"\"\n",
    "        To be defined in the subclass\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_train_loss(self):\n",
    "        \"\"\"\n",
    "        To be defined in the subclass\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_test_loss(self):\n",
    "        \"\"\"\n",
    "        To be defined in the subclass\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        To be defined in the subclass\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        To be defined in the subclass\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit test base class\n",
    "\n",
    "Purpose of unit testing is to cover all possible excecution paths in code.\n",
    "Unit testing helps developers to avoid and identify bugs in code.\n",
    "\n",
    "To unit test the class, we want to try and call every function of it and execute every different possible execution path.\n",
    "The proportion of possible paths covered is called the test coverage.\n",
    "100% coverage is rarely possible,\n",
    "but it would be good to try and test the functions with good and bad input,\n",
    "and with possible limit values (min, max, zero, None, np.nan, empty list etc.).\n",
    "\n",
    "However, a few simple tests are easier to maintain (and to actually get done in the first place) than exhaustive unit testing.\n",
    "Don't worry about coverage too much!\n",
    "A few well considered tests are better than having none.\n",
    "\n",
    "At the moment, nbdev considers all cells that do not have `# export` tag as tests.\n",
    "Unit tests can be defined with `assert` command - the nbdev git hooks run these commands when you push commits.\n",
    "Cells with `# slow` -tag will be omitted for time savings.\n",
    "\n",
    "The line after `assert` should have a `True` or non-zero value.\n",
    "`False`, 0 or None object will raise an `AssertionError`. Note that `np.nan` does not raise the error.\n",
    "\n",
    "This is handy because you can now keep all your tests in the same file (notebook) with the code and documentation.\n",
    "The downside is, however, that at the moment there is no good solution for monitoring test coverage of notebook developed code. \n",
    "If test coverage measuring is required, one option would be to implement tests with  `pytest` or `unittest` and export the tests to separate test.py file.\n",
    "\n",
    "Let's begin by introducing a couple of super simple unit test examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a simple unit test\n",
    "this_statement_is_true = True\n",
    "assert this_statement_is_true\n",
    "\n",
    "# Another example of simple unit test of a function\n",
    "def return_three():\n",
    "    return 3\n",
    "\n",
    "\n",
    "# unit test return_tree\n",
    "assert return_three() == 3\n",
    "\n",
    "# Third example of simple unit testing of a simple class\n",
    "class SimpleClass:\n",
    "    \"\"\"\n",
    "    Simple class that stores an attribute and has a function to return it\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parameter):\n",
    "        self.attribute = parameter\n",
    "\n",
    "    def get_attribute(self):\n",
    "        return self.attribute\n",
    "\n",
    "\n",
    "# unit thest init (should return class instance)\n",
    "assert SimpleClass(\"hello world!\")\n",
    "# unit test get_attribute (should return 'Hello world!')\n",
    "assert SimpleClass(\"Hello world!\").get_attribute() == \"Hello world!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the assert commands run without an error, the tests pass.\n",
    "\n",
    "Now, let's include some tests with our example ML base class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test init\n",
    "assert MachineLearningModel(X, y)\n",
    "\n",
    "mlmodel = MachineLearningModel(X, y)\n",
    "\n",
    "# test get_data\n",
    "assert mlmodel.get_data()[0].shape == X.shape\n",
    "assert mlmodel.get_data()[1].iloc[1] == y.iloc[1]\n",
    "\n",
    "# test __create_test_train_data and get_train_test_data\n",
    "assert np.ceil(\n",
    "    10 * mlmodel.get_train_test_data()[-1].shape[0] / mlmodel.get_data()[1].shape[0]\n",
    ") == np.ceil(10 / mlmodel.n_splits)\n",
    "\n",
    "# test set_data (you should be able to change the model data completely)\n",
    "assert (\n",
    "    MachineLearningModel(X, y)  # create model as usual\n",
    "    .set_data(X.iloc[range(X.shape[0] - 1, -1, -1)], y)  # reset data in reverse order\n",
    "    .get_data()[0]  # get data\n",
    "    .iloc[0, 0]\n",
    ") == X.iloc[\n",
    "    -1, 0\n",
    "]  # first element is now reversed to last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define subclasses & functions\n",
    "\n",
    "A subclass or child class inherits all attributes and functions of a parent class, but may also have additional functions defined.\n",
    "\n",
    "Here we define an example of a subclass of `MachineLearningModel`, the `LogisticRegressionModel` which performs logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# Create subclass\n",
    "# now you see, that the subclass inherits data handling functions from the base class,\n",
    "# and we do not need to redefine them (although we could if we wanted to!\n",
    "\n",
    "\n",
    "class LogisticRegressionClassifier(MachineLearningModel):\n",
    "    \"\"\"\n",
    "    Logistic regression classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, n_splits=5, seed=0):\n",
    "\n",
    "        # we need to initialize the parent class with super.init:\n",
    "        super(LogisticRegressionClassifier, self).__init__(\n",
    "            X, y, n_splits=n_splits, seed=seed\n",
    "        )\n",
    "\n",
    "        # define preprocessing, algorithm and pipe\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = LogisticRegression()\n",
    "        self.pipe = Pipeline([(\"scaler\", self.scaler), (\"estimator\", self.model)])\n",
    "\n",
    "        # cross validation for optimization\n",
    "        self.cv = StratifiedKFold(n_splits=self.n_splits)\n",
    "\n",
    "        # param grid for optimization\n",
    "        self.param_grid = {\n",
    "            \"estimator__C\": np.linspace(0.3, 1.7, 10)  # logspace(-4, 4, 10),\n",
    "        }\n",
    "\n",
    "        # define optimization method for optimizing the model\n",
    "        self.optimization_pipe = GridSearchCV(\n",
    "            estimator=self.pipe,\n",
    "            param_grid=self.param_grid,\n",
    "            scoring=\"accuracy\",\n",
    "            cv=self.cv,\n",
    "            return_train_score=True,\n",
    "        )\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        \"\"\"\n",
    "        Train and evaluate model\n",
    "        \"\"\"\n",
    "        if X is None or y is None:\n",
    "            self.pipe.fit(self.X_train, self.y_train)\n",
    "        else:  # reset data, recreate training and testing data and recursively call fit\n",
    "            self.set_data(X, y).fit()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Get predicted value at X\n",
    "        \"\"\"\n",
    "        return self.pipe.predict(X)\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        \"\"\"\n",
    "        Return loss (model quality metric) [f1 score]\n",
    "\n",
    "        Note that this may be a different metric than the one that the model optimizer is using (scoring method).\n",
    "        For example for LogisticRegression the scoring method is mean accuracy,\n",
    "        but we want to track f1-score for loss because it is better balanced.\n",
    "        \"\"\"\n",
    "\n",
    "        return f1_score(y, self.predict(X))\n",
    "\n",
    "    def get_train_loss(self):\n",
    "        \"\"\"\n",
    "        Return loss for training data\n",
    "        \"\"\"\n",
    "        return self.loss(self.X_train, self.y_train)\n",
    "\n",
    "    def get_test_loss(self):\n",
    "        \"\"\"\n",
    "        Return loss for testing data\n",
    "        \"\"\"\n",
    "        return self.loss(self.X_test, self.y_test)\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Optimize model hyperparameters and fit the model with optimized parameters.\n",
    "\n",
    "        This example is with GridSearchCV, but more efficient algorithms can be implemented in practice.\n",
    "        \"\"\"\n",
    "        self.optimization_pipe.fit(self.X_train, self.y_train)\n",
    "        self.pipe.set_params(\n",
    "            estimator__C=self.optimization_pipe.best_params_[\"estimator__C\"]\n",
    "        )\n",
    "        self.fit()\n",
    "        return self\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Return parameters\n",
    "        \"\"\"\n",
    "        return self.pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit test subclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test init\n",
    "assert LogisticRegressionClassifier(X, y)\n",
    "lgr_model = LogisticRegressionClassifier(X, y)\n",
    "\n",
    "# test fit\n",
    "try:  # fit should be called before predict or score\n",
    "    lgr_model.predict([1, 1, 1, 1])  # so this will cause an error\n",
    "except:  # but the except statement catches the error\n",
    "    pass  # yes, you can also test what should not work!\n",
    "\n",
    "# there are two ways we can call the fit function: with and without data\n",
    "assert lgr_model.fit()\n",
    "assert lgr_model.fit(X, y)\n",
    "\n",
    "# test predict\n",
    "assert lgr_model.predict(X[::1]).any()\n",
    "# test loss\n",
    "assert lgr_model.get_train_loss()\n",
    "assert lgr_model.get_test_loss()\n",
    "\n",
    "# test get_params\n",
    "assert lgr_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we might observe that our model is very slow (this example is not).\n",
    "\n",
    "Then, we could try to evaluate how much time each of the functions,\n",
    "and even the contents of the functions take to identify the bottlenecks.\n",
    "The theory of [order of functions](https://en.wikipedia.org/wiki/Big_O_notation) may also be useful.\n",
    "\n",
    "However, this is something you should only do in the late stages of your project.\n",
    "Remember, thinking time is what matters in data science!\n",
    "Begin with overoptimizing things, and you'll never have results.\n",
    "\n",
    "Anyway, you can easily time functions in notebooks with `%%timeit` [magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.65 ms ± 141 µs per loop (mean ± std. dev. of 4 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "# slow\n",
    "%timeit -n 3 -r 4 LogisticRegressionClassifier(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize model behaviour with toy data\n",
    "\n",
    "Now, with the unit tests we can assume that our model does something right.\n",
    "At least any of the functions do not crash with expected input.\n",
    "\n",
    "As with the data preparation, the last step is to visualize the model performance.\n",
    "With small sample data, we may not see anything interestin,\n",
    "but sometimes already small number of datapoints can reveal interesting properties of the model when visualized.\n",
    "\n",
    "You can also define functions for visualizing the model performance, and export them to the model module,\n",
    "or include them directly as part of your machine learning model class if you see benefits from it.\n",
    "Either way, it's better to test them too with the toy data before the real deal!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite often we would like to see how a model performs when the number of data is increased.\n",
    "The two common questions are:\n",
    "\n",
    "1. How much data is needed that the model is accurate?\n",
    "\n",
    "2. How much data we can put in the model and still be able to run it with our resources?\n",
    "\n",
    "Then, we might have to balance between these two.\n",
    "\n",
    "\n",
    "\n",
    "So, for our example, let's loop through a range of data points, fit and time the model at each round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/templateenv/lib/python3.8/site-packages/sklearn/model_selection/_split.py:666: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_obs</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>optimized_C</th>\n",
       "      <th>optimization_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>round</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.517263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.490386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.486138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>0.478212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.480910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_obs  train_loss  test_loss  optimized_C  optimization_time\n",
       "round                                                              \n",
       "5         15    0.400000   0.000000     0.300000           0.517263\n",
       "6         16    0.750000   0.000000     0.300000           0.490386\n",
       "7         17    0.888889   0.000000     0.300000           0.486138\n",
       "8         18    0.800000   0.666667     0.455556           0.478212\n",
       "9         19    0.727273   0.666667     0.300000           0.480910"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time  # library for checking time\n",
    "\n",
    "lgr = LogisticRegressionClassifier(X, y)\n",
    "\n",
    "# dataframe to save results\n",
    "eval_df = pd.DataFrame()\n",
    "\n",
    "for i in range(15, X.shape[0]):\n",
    "    lgr.fit(X.iloc[:i], y.iloc[:i])\n",
    "\n",
    "    begin = time.time()  # measure time before model is optimized\n",
    "    lgr.optimize()\n",
    "    end = time.time()  # measure time after optimization\n",
    "\n",
    "    ret = pd.DataFrame(\n",
    "        {\n",
    "            \"round\": [i - 10],\n",
    "            \"n_obs\": [lgr.get_data()[0].shape[0]],\n",
    "            \"train_loss\": [lgr.get_train_loss()],\n",
    "            \"test_loss\": [lgr.get_test_loss()],\n",
    "            \"optimized_C\": [lgr.get_params()[\"estimator__C\"]],\n",
    "            \"optimization_time\": end - begin,  # time spent in optimization\n",
    "        }\n",
    "    )\n",
    "    eval_df = pd.concat([eval_df, ret], axis=0, ignore_index=True)\n",
    "eval_df.set_index(\"round\", inplace=True)\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the stratification gives some warnings with such a small test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can also visualize the results. Super simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "_ = eval_df.drop(\"n_obs\", axis=1).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, it appears that with very little data the model is overfitting a lot (observe test loss of 0, meaning that every single prediction went wrong at that round).\n",
    "Remember that in you application you might be interested in completely different measures!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output of this notebook\n",
    "\n",
    "The result of this notebook is a collection methods ready for evaluation with the real data.\n",
    "\n",
    "You should export classes and functions to `model.py` with `# nbdev_build_lib` (workflows will do this automatically)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can move on to loss notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (ml-sami-libclas)",
   "language": "python",
   "name": "ml-sami-libclas"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
