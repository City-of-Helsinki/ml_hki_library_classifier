{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# default_exp model\n",
        "# %load_ext lab_black\n",
        "\n",
        "# nb_black if running in jupyter\n",
        "%load_ext nb_black\n",
        "\n",
        "%load_ext autoreload\n",
        "# automatically reload python modules if there are changes in the\n",
        "%autoreload 2"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "The nb_black extension is already loaded. To reload it, use:\n  %reload_ext nb_black\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 45;\n                var nbb_unformatted_code = \"# default_exp model\\n# %load_ext lab_black\\n\\n# nb_black if running in jupyter\\n%load_ext nb_black\\n\\n%load_ext autoreload\\n# automatically reload python modules if there are changes in the\\n%autoreload 2\";\n                var nbb_formatted_code = \"# default_exp model\\n# %load_ext lab_black\\n\\n# nb_black if running in jupyter\\n%load_ext nb_black\\n\\n%load_ext autoreload\\n# automatically reload python modules if there are changes in the\\n%autoreload 2\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": "<IPython.core.display.Javascript object>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# hide\n",
        "from nbdev.showdoc import *"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 46;\n                var nbb_unformatted_code = \"# hide\\nfrom nbdev.showdoc import *\";\n                var nbb_formatted_code = \"# hide\\nfrom nbdev.showdoc import *\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": "<IPython.core.display.Javascript object>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "> In this notebook you create and test a Python class to hold your machine learning model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "***input***: toy dataset from data-notebook\n",
        "\n",
        "***output***: python module containing ML model class \n",
        "\n",
        "***description:***\n",
        "\n",
        "In this notebook you hypothetize, explain and explore machine learning models to solve your problem.\n",
        "\n",
        "Then, you should encapsulate the model inside a Python class to be exported into `your_repository/your_module/model.py`,\n",
        "so that it can be evaluated in the loss notebook, and intergrated with your target application.\n",
        "Repository name and module name are the same by default.\n",
        "You should also unit test the classes created in this notebook with the toy data created in data notebook.\n",
        "\n",
        "You should probably have a person in your team familiar with object oriented programming with python and unit testing, but if not, don't worry.\n",
        "If you can explore by scripting in the cells and create a draft of the properties and functions you want to have, and that's a great start.\n",
        "Then, any Python developer can easily build the model class for you.\n",
        "However, we encourage you to learn more on [object oriented programming with Python](https://realpython.com/python3-object-oriented-programming/)\n",
        "and [getting started with unit testing in Python](https://realpython.com/python-testing/).\n",
        "You can also follow the example here to create your own simple machine learning Python class.\n",
        "\n",
        "This notebook contains an example ML model for classifying the heart disease dataset with logistic regression.\n",
        "\n",
        "The example is split into a base class and a subclass for demonstrating class inheritance of Python.\n",
        "You can probably just write one class that contains all the attributes and functions you need, without inheriting anything.\n",
        "However, alternative implementations of a model might be implemented in separate subclasses, for example.\n",
        "You can also define multiple classes, for example one for ML model and another for optimization.\n",
        "If the methods are complicated or you are comparing multiple methods that don't share common functions, \n",
        "you can also separate models or subclasses to different notebooks similar to this.\n",
        "Adjust the running number, name, header and top cell `#default_exp module_name` of the notebooks accordingly.\n",
        "\n",
        "Remember to add `# export` to top of all cells containing functions or classes that you have defined and want to use outside this notebook.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import relevant modules"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# export\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "from sklearn.model_selection import (\n",
        "    GridSearchCV,\n",
        "    cross_val_score,\n",
        "    train_test_split,\n",
        "    StratifiedKFold,\n",
        ")\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 47;\n                var nbb_unformatted_code = \"# export\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.linear_model import LogisticRegression\\nfrom pandas.api.types import CategoricalDtype\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.metrics import f1_score\\nfrom sklearn.metrics import log_loss\\n\\nfrom sklearn.model_selection import (\\n    GridSearchCV,\\n    cross_val_score,\\n    train_test_split,\\n    StratifiedKFold,\\n)\\nfrom sklearn.pipeline import Pipeline, make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import mean_squared_error\";\n                var nbb_formatted_code = \"# export\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.linear_model import LogisticRegression\\nfrom pandas.api.types import CategoricalDtype\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.metrics import f1_score\\nfrom sklearn.metrics import log_loss\\n\\nfrom sklearn.model_selection import (\\n    GridSearchCV,\\n    cross_val_score,\\n    train_test_split,\\n    StratifiedKFold,\\n)\\nfrom sklearn.pipeline import Pipeline, make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import mean_squared_error\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": "<IPython.core.display.Javascript object>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# FIX THIS!!!!\n",
        "\n",
        "# Use 'nbdev_build_lib' shell command to update library\n",
        "# from ml_project_template.plot import plot_trellis, plot_histogram"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 48;\n                var nbb_unformatted_code = \"# FIX THIS!!!!\\n\\n# Use 'nbdev_build_lib' shell command to update library\\n# from ml_project_template.plot import plot_trellis, plot_histogram\";\n                var nbb_formatted_code = \"# FIX THIS!!!!\\n\\n# Use 'nbdev_build_lib' shell command to update library\\n# from ml_project_template.plot import plot_trellis, plot_histogram\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": "<IPython.core.display.Javascript object>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define notebook parameters\n",
        "\n",
        "Remember, only simple assignments here!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "\n",
        "# this cell is tagged with 'parameters'\n",
        "toy_data_file = \"data/preprocessed_data/dataset_toy_all_classes.csv\"\n",
        "all_classes_data_file = \"data/preprocessed_data/dataset_clean_all_classes.csv\"\n",
        "input_data_file = \"data/preprocessed_data/input_file.csv\"\n",
        "seed = 0"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 49;\n                var nbb_unformatted_code = \"# Parameters\\n\\n# this cell is tagged with 'parameters'\\ntoy_data_file = \\\"data/preprocessed_data/dataset_toy_all_classes.csv\\\"\\nall_classes_data_file = \\\"data/preprocessed_data/dataset_clean_all_classes.csv\\\"\\ninput_data_file = \\\"data/preprocessed_data/input_file.csv\\\"\\nseed = 0\";\n                var nbb_formatted_code = \"# Parameters\\n\\n# this cell is tagged with 'parameters'\\ntoy_data_file = \\\"data/preprocessed_data/dataset_toy_all_classes.csv\\\"\\nall_classes_data_file = \\\"data/preprocessed_data/dataset_clean_all_classes.csv\\\"\\ninput_data_file = \\\"data/preprocessed_data/input_file.csv\\\"\\nseed = 0\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": "<IPython.core.display.Javascript object>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make immediate derivations from the parameters:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(seed)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 50;\n                var nbb_unformatted_code = \"np.random.seed(seed)\";\n                var nbb_formatted_code = \"np.random.seed(seed)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": "<IPython.core.display.Javascript object>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import toy data for testing"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "toy_df = pd.read_csv(toy_data_file, index_col=0)\n",
        "toy_df.head(30)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": null,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>record_id</th>\n      <th>084</th>\n      <th>092</th>\n      <th>093</th>\n      <th>094</th>\n      <th>095</th>\n      <th>650</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>40826</th>\n      <td>420909226072</td>\n      <td>78.89700</td>\n      <td>78.89170</td>\n      <td>78.8917</td>\n      <td>78.89170</td>\n      <td>788.830</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>50227</th>\n      <td>420909204870</td>\n      <td>78.89113</td>\n      <td>78.89110</td>\n      <td>78.8911</td>\n      <td>NaN</td>\n      <td>788.330</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>70429</th>\n      <td>420908537805</td>\n      <td>99.10000</td>\n      <td>77.19200</td>\n      <td>NaN</td>\n      <td>77.19200</td>\n      <td>770.921</td>\n      <td>näyttelijät,elokuvanäyttelijät,laulajat,iskelm...</td>\n    </tr>\n    <tr>\n      <th>3171</th>\n      <td>420907800313</td>\n      <td>30.12000</td>\n      <td>30.12000</td>\n      <td>30.1200</td>\n      <td>30.12000</td>\n      <td>357.000</td>\n      <td>maailmankuva,arvot,sukupuoli,tanssi,ympäristö,...</td>\n    </tr>\n    <tr>\n      <th>99360</th>\n      <td>420908893012</td>\n      <td>78.89112</td>\n      <td>78.89112</td>\n      <td>NaN</td>\n      <td>78.89112</td>\n      <td>788.332</td>\n      <td>heavy rock,death metal</td>\n    </tr>\n    <tr>\n      <th>31897</th>\n      <td>420908507234</td>\n      <td>78.89320</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>78.89320</td>\n      <td>788.410</td>\n      <td>viihdeohjelmat,musiikki,televisio-ohjelmat,huu...</td>\n    </tr>\n    <tr>\n      <th>26220</th>\n      <td>420909108380</td>\n      <td>99.23000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>92.73000</td>\n      <td>928.100</td>\n      <td>virolaiset,vapaaehtoiset,sotilaat,jatkosota,to...</td>\n    </tr>\n    <tr>\n      <th>102930</th>\n      <td>420909221339</td>\n      <td>78.89110</td>\n      <td>78.89110</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.330</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>98161</th>\n      <td>420908060431</td>\n      <td>22.20000</td>\n      <td>22.20000</td>\n      <td>NaN</td>\n      <td>22.20000</td>\n      <td>226.000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7428</th>\n      <td>420908985491</td>\n      <td>48.18400</td>\n      <td>48.18400</td>\n      <td>NaN</td>\n      <td>48.18400</td>\n      <td>427.000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>87870</th>\n      <td>420908840876</td>\n      <td>78.89300</td>\n      <td>78.89300</td>\n      <td>78.8930</td>\n      <td>78.89300</td>\n      <td>788.400</td>\n      <td>viihdemusiikki,ikivihreät</td>\n    </tr>\n    <tr>\n      <th>87339</th>\n      <td>420908163172</td>\n      <td>33.36000</td>\n      <td>33.36000</td>\n      <td>NaN</td>\n      <td>33.36000</td>\n      <td>333.400</td>\n      <td>kilpailuoikeus,kilpailupolitiikka,kilpailu,lai...</td>\n    </tr>\n    <tr>\n      <th>44694</th>\n      <td>420908426586</td>\n      <td>78.54000</td>\n      <td>78.55500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>785.620</td>\n      <td>konsertot,orkesterit,orkesterimusiikki</td>\n    </tr>\n    <tr>\n      <th>54566</th>\n      <td>420909110168</td>\n      <td>16.10000</td>\n      <td>16.10000</td>\n      <td>NaN</td>\n      <td>16.10000</td>\n      <td>140.000</td>\n      <td>kriittinen ajattelu,kriittisyys,lukutaito,vies...</td>\n    </tr>\n    <tr>\n      <th>84271</th>\n      <td>420908989548</td>\n      <td>48.29000</td>\n      <td>48.29000</td>\n      <td>NaN</td>\n      <td>48.29000</td>\n      <td>439.000</td>\n      <td>saaret</td>\n    </tr>\n    <tr>\n      <th>9114</th>\n      <td>420908232180</td>\n      <td>52.00000</td>\n      <td>52.00000</td>\n      <td>52.0000</td>\n      <td>52.00000</td>\n      <td>520.000</td>\n      <td>tähtitiede</td>\n    </tr>\n    <tr>\n      <th>64029</th>\n      <td>420908423592</td>\n      <td>78.35100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>78.35100</td>\n      <td>783.610</td>\n      <td>oopperat</td>\n    </tr>\n    <tr>\n      <th>54250</th>\n      <td>420908878815</td>\n      <td>78.89110</td>\n      <td>78.89110</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.330</td>\n      <td>rock,konsertit</td>\n    </tr>\n    <tr>\n      <th>30866</th>\n      <td>420908977974</td>\n      <td>67.52000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>657.000</td>\n      <td>uitto,historia,puutavara,vesikuljetus,vesiväyl...</td>\n    </tr>\n    <tr>\n      <th>78245</th>\n      <td>420908772469</td>\n      <td>59.34000</td>\n      <td>59.34000</td>\n      <td>NaN</td>\n      <td>59.34000</td>\n      <td>613.100</td>\n      <td>vähähiilihydraattinen ruokavalio,painonhallint...</td>\n    </tr>\n    <tr>\n      <th>23748</th>\n      <td>420908124744</td>\n      <td>38.29600</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>38.54000</td>\n      <td>182.600</td>\n      <td>tekninen työ,opetus,peruskoulu,lukio,työkasvat...</td>\n    </tr>\n    <tr>\n      <th>613</th>\n      <td>420909220559</td>\n      <td>59.49000</td>\n      <td>59.49000</td>\n      <td>59.4900</td>\n      <td>59.49000</td>\n      <td>618.240</td>\n      <td>kärsimys,elämän tarkoitus,elämä,kuolema,ihmisk...</td>\n    </tr>\n    <tr>\n      <th>67905</th>\n      <td>420908695896</td>\n      <td>78.89111</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>78.89111</td>\n      <td>788.330</td>\n      <td>rock and roll,rock</td>\n    </tr>\n    <tr>\n      <th>17701</th>\n      <td>420908851722</td>\n      <td>78.89420</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>78.89320</td>\n      <td>788.410</td>\n      <td>laulelmat,viihdemusiikki,sekakuorot,rock,sovit...</td>\n    </tr>\n    <tr>\n      <th>31636</th>\n      <td>420908875333</td>\n      <td>86.83200</td>\n      <td>86.83200</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>846.100</td>\n      <td>kirjallisuudentutkimus,lyriikka,kurdin kieli</td>\n    </tr>\n    <tr>\n      <th>15429</th>\n      <td>420908920378</td>\n      <td>30.16000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>99.10000</td>\n      <td>990.000</td>\n      <td>reportage,journalister,fotografier,fångar,fång...</td>\n    </tr>\n    <tr>\n      <th>51749</th>\n      <td>420908761605</td>\n      <td>68.24000</td>\n      <td>68.24000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>691.110</td>\n      <td>matrecept,bakverk,jul,kakor,bakelser,tårtor,sm...</td>\n    </tr>\n    <tr>\n      <th>41661</th>\n      <td>420908014879</td>\n      <td>55.30000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>55.30000</td>\n      <td>493.000</td>\n      <td>sten,bergarter,mineraler,geologi,mineralogi</td>\n    </tr>\n    <tr>\n      <th>62483</th>\n      <td>420908955905</td>\n      <td>92.83300</td>\n      <td>NaN</td>\n      <td>92.8330</td>\n      <td>NaN</td>\n      <td>929.110</td>\n      <td>kaupunginosat,historia</td>\n    </tr>\n    <tr>\n      <th>69056</th>\n      <td>420908250607</td>\n      <td>7.01000</td>\n      <td>7.00000</td>\n      <td>NaN</td>\n      <td>7.00000</td>\n      <td>384.000</td>\n      <td>joukkoviestintä,journalistiikka,tutkimus,tiedo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "           record_id       084       092      093       094      095  \\\n40826   420909226072  78.89700  78.89170  78.8917  78.89170  788.830   \n50227   420909204870  78.89113  78.89110  78.8911       NaN  788.330   \n70429   420908537805  99.10000  77.19200      NaN  77.19200  770.921   \n3171    420907800313  30.12000  30.12000  30.1200  30.12000  357.000   \n99360   420908893012  78.89112  78.89112      NaN  78.89112  788.332   \n31897   420908507234  78.89320       NaN      NaN  78.89320  788.410   \n26220   420909108380  99.23000       NaN      NaN  92.73000  928.100   \n102930  420909221339  78.89110  78.89110      NaN       NaN  788.330   \n98161   420908060431  22.20000  22.20000      NaN  22.20000  226.000   \n7428    420908985491  48.18400  48.18400      NaN  48.18400  427.000   \n87870   420908840876  78.89300  78.89300  78.8930  78.89300  788.400   \n87339   420908163172  33.36000  33.36000      NaN  33.36000  333.400   \n44694   420908426586  78.54000  78.55500      NaN       NaN  785.620   \n54566   420909110168  16.10000  16.10000      NaN  16.10000  140.000   \n84271   420908989548  48.29000  48.29000      NaN  48.29000  439.000   \n9114    420908232180  52.00000  52.00000  52.0000  52.00000  520.000   \n64029   420908423592  78.35100       NaN      NaN  78.35100  783.610   \n54250   420908878815  78.89110  78.89110      NaN       NaN  788.330   \n30866   420908977974  67.52000       NaN      NaN       NaN  657.000   \n78245   420908772469  59.34000  59.34000      NaN  59.34000  613.100   \n23748   420908124744  38.29600       NaN      NaN  38.54000  182.600   \n613     420909220559  59.49000  59.49000  59.4900  59.49000  618.240   \n67905   420908695896  78.89111       NaN      NaN  78.89111  788.330   \n17701   420908851722  78.89420       NaN      NaN  78.89320  788.410   \n31636   420908875333  86.83200  86.83200      NaN       NaN  846.100   \n15429   420908920378  30.16000       NaN      NaN  99.10000  990.000   \n51749   420908761605  68.24000  68.24000      NaN       NaN  691.110   \n41661   420908014879  55.30000       NaN      NaN  55.30000  493.000   \n62483   420908955905  92.83300       NaN  92.8330       NaN  929.110   \n69056   420908250607   7.01000   7.00000      NaN   7.00000  384.000   \n\n                                                      650  \n40826                                                 NaN  \n50227                                                 NaN  \n70429   näyttelijät,elokuvanäyttelijät,laulajat,iskelm...  \n3171    maailmankuva,arvot,sukupuoli,tanssi,ympäristö,...  \n99360                              heavy rock,death metal  \n31897   viihdeohjelmat,musiikki,televisio-ohjelmat,huu...  \n26220   virolaiset,vapaaehtoiset,sotilaat,jatkosota,to...  \n102930                                                NaN  \n98161                                                 NaN  \n7428                                                  NaN  \n87870                           viihdemusiikki,ikivihreät  \n87339   kilpailuoikeus,kilpailupolitiikka,kilpailu,lai...  \n44694              konsertot,orkesterit,orkesterimusiikki  \n54566   kriittinen ajattelu,kriittisyys,lukutaito,vies...  \n84271                                              saaret  \n9114                                           tähtitiede  \n64029                                            oopperat  \n54250                                      rock,konsertit  \n30866   uitto,historia,puutavara,vesikuljetus,vesiväyl...  \n78245   vähähiilihydraattinen ruokavalio,painonhallint...  \n23748   tekninen työ,opetus,peruskoulu,lukio,työkasvat...  \n613     kärsimys,elämän tarkoitus,elämä,kuolema,ihmisk...  \n67905                                  rock and roll,rock  \n17701   laulelmat,viihdemusiikki,sekakuorot,rock,sovit...  \n31636        kirjallisuudentutkimus,lyriikka,kurdin kieli  \n15429   reportage,journalister,fotografier,fångar,fång...  \n51749   matrecept,bakverk,jul,kakor,bakelser,tårtor,sm...  \n41661         sten,bergarter,mineraler,geologi,mineralogi  \n62483                              kaupunginosat,historia  \n69056   joukkoviestintä,journalistiikka,tutkimus,tiedo...  "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 51;\n                var nbb_unformatted_code = \"toy_df = pd.read_csv(toy_data_file, index_col=0)\\ntoy_df.head(30)\";\n                var nbb_formatted_code = \"toy_df = pd.read_csv(toy_data_file, index_col=0)\\ntoy_df.head(30)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": "<IPython.core.display.Javascript object>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def keywords_to_features(df_to_parse, keywords=[]):\n",
        "    \"\"\"\n",
        "    Parse dataframe keywords and create features of them\n",
        "    \"\"\"\n",
        "    #print(f\"KEYWORDS: {len(keywords)}\")\n",
        "    \n",
        "    # Create keyword list\n",
        "    if len(keywords) == 0:\n",
        "        for i in range(len(df_to_parse)):\n",
        "            item_keywords_str = (str)(df_to_parse.iloc[(i), 6])\n",
        "            item_keywords_lst = item_keywords_str.split(\",\")\n",
        "            \n",
        "            for word in item_keywords_lst:\n",
        "                word = word.strip().lower()\n",
        "                if word not in keywords:\n",
        "                    keywords.append(word)\n",
        "            \n",
        "    # Add keyword columns with keyword as a title (value will be 0 or 1 depending if the keyword belongs to the volume or not)\n",
        "    # NOTE: \"A Pandas Series is like a column in a table\" (https://www.w3schools.com/python/pandas/pandas_series.asp)\n",
        "    for i in range(len(keywords)):\n",
        "        df_to_parse[keywords[i]] = pd.Series([], dtype=\"int64\")\n",
        "        df_to_parse = df_to_parse.reset_index(drop=True)\n",
        "        \n",
        "    # Fill features with keywords attached to item with value \"1\"\n",
        "    for i in range(len(df_to_parse)):\n",
        "        item_keywords_str = (str)(df_to_parse.iloc[(i), 6])\n",
        "        item_keywords_lst = item_keywords_str.split(\",\")\n",
        "        \n",
        "        for word in item_keywords_lst:\n",
        "            word = word.strip().lower()\n",
        "            if word in keywords:\n",
        "                df_to_parse.at[i, word] = 1\n",
        "            \n",
        "    # Drop column with comma-separated keywords and fill NaN with 0\n",
        "    df_to_parse = df_to_parse.drop([\"650\"], axis=1)\n",
        "    df_to_parse = df_to_parse.fillna(0)\n",
        "    \n",
        "    #print(f\"DF TO PARSE SHAPE: {df_to_parse.shape}\")\n",
        "    #print(f\"KEYWORDS: {len(keywords)}\")\n",
        "    return df_to_parse, keywords\n",
        "    \n",
        "\n",
        "toy_df, keywords = keywords_to_features(toy_df)\n",
        "\n",
        "# printing final dataset for training and validating the model\n",
        "print(f\"\\n*****\")\n",
        "print(\n",
        "    f\"Input data: {toy_df.shape[0]} rows.\"\n",
        ")\n",
        "print(f\"Number of keywords in input dataset is: {toy_df.shape[1] - 6}={len(keywords)}\")\n",
        "print(f\"*****\\n\")\n",
        "toy_df.head()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n*****\nInput data: 100 rows.\nNumber of keywords in input dataset is: 481=481\n*****\n\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": null,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>record_id</th>\n      <th>084</th>\n      <th>092</th>\n      <th>093</th>\n      <th>094</th>\n      <th>095</th>\n      <th>nan</th>\n      <th>näyttelijät</th>\n      <th>elokuvanäyttelijät</th>\n      <th>laulajat</th>\n      <th>...</th>\n      <th>turvallisuuspolitiikka</th>\n      <th>joustavuus</th>\n      <th>resilienssi</th>\n      <th>selviytyminen</th>\n      <th>vaikeudet</th>\n      <th>itsekasvatus</th>\n      <th>elämänhallinta</th>\n      <th>itseluottamus</th>\n      <th>tunnetaidot</th>\n      <th>itsensä toteuttaminen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>420909226072</td>\n      <td>78.89700</td>\n      <td>78.89170</td>\n      <td>78.8917</td>\n      <td>78.89170</td>\n      <td>788.830</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>420909204870</td>\n      <td>78.89113</td>\n      <td>78.89110</td>\n      <td>78.8911</td>\n      <td>0.00000</td>\n      <td>788.330</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>420908537805</td>\n      <td>99.10000</td>\n      <td>77.19200</td>\n      <td>0.0000</td>\n      <td>77.19200</td>\n      <td>770.921</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>420907800313</td>\n      <td>30.12000</td>\n      <td>30.12000</td>\n      <td>30.1200</td>\n      <td>30.12000</td>\n      <td>357.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>420908893012</td>\n      <td>78.89112</td>\n      <td>78.89112</td>\n      <td>0.0000</td>\n      <td>78.89112</td>\n      <td>788.332</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 487 columns</p>\n</div>",
            "text/plain": "      record_id       084       092      093       094      095  nan  \\\n0  420909226072  78.89700  78.89170  78.8917  78.89170  788.830  1.0   \n1  420909204870  78.89113  78.89110  78.8911   0.00000  788.330  1.0   \n2  420908537805  99.10000  77.19200   0.0000  77.19200  770.921  0.0   \n3  420907800313  30.12000  30.12000  30.1200  30.12000  357.000  0.0   \n4  420908893012  78.89112  78.89112   0.0000  78.89112  788.332  0.0   \n\n   näyttelijät  elokuvanäyttelijät  laulajat  ...  turvallisuuspolitiikka  \\\n0          0.0                 0.0       0.0  ...                     0.0   \n1          0.0                 0.0       0.0  ...                     0.0   \n2          1.0                 1.0       1.0  ...                     0.0   \n3          0.0                 0.0       0.0  ...                     0.0   \n4          0.0                 0.0       0.0  ...                     0.0   \n\n   joustavuus  resilienssi  selviytyminen  vaikeudet  itsekasvatus  \\\n0         0.0          0.0            0.0        0.0           0.0   \n1         0.0          0.0            0.0        0.0           0.0   \n2         0.0          0.0            0.0        0.0           0.0   \n3         0.0          0.0            0.0        0.0           0.0   \n4         0.0          0.0            0.0        0.0           0.0   \n\n   elämänhallinta  itseluottamus  tunnetaidot  itsensä toteuttaminen  \n0             0.0            0.0          0.0                    0.0  \n1             0.0            0.0          0.0                    0.0  \n2             0.0            0.0          0.0                    0.0  \n3             0.0            0.0          0.0                    0.0  \n4             0.0            0.0          0.0                    0.0  \n\n[5 rows x 487 columns]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 52;\n                var nbb_unformatted_code = \"def keywords_to_features(df_to_parse, keywords=[]):\\n    \\\"\\\"\\\"\\n    Parse dataframe keywords and create features of them\\n    \\\"\\\"\\\"\\n    #print(f\\\"KEYWORDS: {len(keywords)}\\\")\\n    \\n    # Create keyword list\\n    if len(keywords) == 0:\\n        for i in range(len(df_to_parse)):\\n            item_keywords_str = (str)(df_to_parse.iloc[(i), 6])\\n            item_keywords_lst = item_keywords_str.split(\\\",\\\")\\n            \\n            for word in item_keywords_lst:\\n                word = word.strip().lower()\\n                if word not in keywords:\\n                    keywords.append(word)\\n            \\n    # Add keyword columns with keyword as a title (value will be 0 or 1 depending if the keyword belongs to the volume or not)\\n    # NOTE: \\\"A Pandas Series is like a column in a table\\\" (https://www.w3schools.com/python/pandas/pandas_series.asp)\\n    for i in range(len(keywords)):\\n        df_to_parse[keywords[i]] = pd.Series([], dtype=\\\"int64\\\")\\n        df_to_parse = df_to_parse.reset_index(drop=True)\\n        \\n    # Fill features with keywords attached to item with value \\\"1\\\"\\n    for i in range(len(df_to_parse)):\\n        item_keywords_str = (str)(df_to_parse.iloc[(i), 6])\\n        item_keywords_lst = item_keywords_str.split(\\\",\\\")\\n        \\n        for word in item_keywords_lst:\\n            word = word.strip().lower()\\n            if word in keywords:\\n                df_to_parse.at[i, word] = 1\\n            \\n    # Drop column with comma-separated keywords and fill NaN with 0\\n    df_to_parse = df_to_parse.drop([\\\"650\\\"], axis=1)\\n    df_to_parse = df_to_parse.fillna(0)\\n    \\n    #print(f\\\"DF TO PARSE SHAPE: {df_to_parse.shape}\\\")\\n    #print(f\\\"KEYWORDS: {len(keywords)}\\\")\\n    return df_to_parse, keywords\\n    \\n\\ntoy_df, keywords = keywords_to_features(toy_df)\\n\\n# printing final dataset for training and validating the model\\nprint(f\\\"\\\\n*****\\\")\\nprint(\\n    f\\\"Input data: {toy_df.shape[0]} rows.\\\"\\n)\\nprint(f\\\"Number of keywords in input dataset is: {toy_df.shape[1] - 6}={len(keywords)}\\\")\\nprint(f\\\"*****\\\\n\\\")\\ntoy_df.head()\";\n                var nbb_formatted_code = \"def keywords_to_features(df_to_parse, keywords=[]):\\n    \\\"\\\"\\\"\\n    Parse dataframe keywords and create features of them\\n    \\\"\\\"\\\"\\n    # print(f\\\"KEYWORDS: {len(keywords)}\\\")\\n\\n    # Create keyword list\\n    if len(keywords) == 0:\\n        for i in range(len(df_to_parse)):\\n            item_keywords_str = (str)(df_to_parse.iloc[(i), 6])\\n            item_keywords_lst = item_keywords_str.split(\\\",\\\")\\n\\n            for word in item_keywords_lst:\\n                word = word.strip().lower()\\n                if word not in keywords:\\n                    keywords.append(word)\\n\\n    # Add keyword columns with keyword as a title (value will be 0 or 1 depending if the keyword belongs to the volume or not)\\n    # NOTE: \\\"A Pandas Series is like a column in a table\\\" (https://www.w3schools.com/python/pandas/pandas_series.asp)\\n    for i in range(len(keywords)):\\n        df_to_parse[keywords[i]] = pd.Series([], dtype=\\\"int64\\\")\\n        df_to_parse = df_to_parse.reset_index(drop=True)\\n\\n    # Fill features with keywords attached to item with value \\\"1\\\"\\n    for i in range(len(df_to_parse)):\\n        item_keywords_str = (str)(df_to_parse.iloc[(i), 6])\\n        item_keywords_lst = item_keywords_str.split(\\\",\\\")\\n\\n        for word in item_keywords_lst:\\n            word = word.strip().lower()\\n            if word in keywords:\\n                df_to_parse.at[i, word] = 1\\n\\n    # Drop column with comma-separated keywords and fill NaN with 0\\n    df_to_parse = df_to_parse.drop([\\\"650\\\"], axis=1)\\n    df_to_parse = df_to_parse.fillna(0)\\n\\n    # print(f\\\"DF TO PARSE SHAPE: {df_to_parse.shape}\\\")\\n    # print(f\\\"KEYWORDS: {len(keywords)}\\\")\\n    return df_to_parse, keywords\\n\\n\\ntoy_df, keywords = keywords_to_features(toy_df)\\n\\n# printing final dataset for training and validating the model\\nprint(f\\\"\\\\n*****\\\")\\nprint(f\\\"Input data: {toy_df.shape[0]} rows.\\\")\\nprint(f\\\"Number of keywords in input dataset is: {toy_df.shape[1] - 6}={len(keywords)}\\\")\\nprint(f\\\"*****\\\\n\\\")\\ntoy_df.head()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": "<IPython.core.display.Javascript object>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note, that depending on your choice of file format and your variables, you might have to redefine data types once you load data! "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO!\n",
        "\n",
        "## Explain the math behind\n",
        "\n",
        "Give a short explanation of how the algorithms work that you are planning to use.\n",
        "\n",
        "For industrial use, you can keep this light and simple: you can provide links to external sources for further reading.\n",
        "\n",
        "For research, you might want to dig deeper - this is your core documentation, after all!\n",
        "\n",
        "You can use $\\LaTeX$ notation to write math symbols and equations:\n",
        "\n",
        "$$\n",
        "Pr(Y_i=1|X_i) = {\\frac{exp(\\beta_0 + \\beta_1X_i + \\dots + \\beta_nX_n)}{1 + exp (\\beta_0 + \\beta_1X_i + \\dots + \\beta_nX_n)}}\n",
        "$$\n",
        "\n",
        "You can also draft algorithms:\n",
        "\n",
        "    ALGORITHM\n",
        "    input: X\n",
        "    output: y\n",
        "\n",
        "    while: condition\n",
        "        do thing\n",
        "\n",
        "#### Naive Bayes\n",
        "\n",
        "Naive Bayes is classifier assumes that features are independent of eachother. Naive Bayes is used to calculate posterior probability P(c|x) from class prior probability P(c), predictor prior probability P(x) and likelihood P(x|c)\n",
        "\n",
        "$$\n",
        "P(c|x)= {\\frac{P(x|c)P(c)}{P(x)}}\n",
        "$$\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Begin with simple scripts before constructing the model class\n",
        "\n",
        "This is the place where you can explore and play around with different machine learning operations.\n",
        "Your goal is to plan and demonstrate the features and functions you want your machine learning class to have.\n",
        "The good thing is, that you don't need to think about object oriented programming here.\n",
        "Just assign variables and call functions. \n",
        "\n",
        "It's good to define at least the following steps:\n",
        "\n",
        "1. Splitting data into training and testing data\n",
        "2. Preprocess the data (scale, dimension reduction, convolutions etc.)\n",
        "3. Define your model algorithm and fit it with toy data\n",
        "4. Define your loss function - how do you evaluate your model?\n",
        "5. Consider hyperparameter optimization\n",
        "6. Try to pipe the previous steps\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# export\n",
        "\n",
        "# FIX THIS IMPORT!!!!\n",
        "# These functions won't work if we don't define these also in this code block\n",
        "# from lib_classification.plot import plot_trellis, plot_histogram\n",
        "\n",
        "\n",
        "seed = 0\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Labels can't be of type float for classification. Thus we multiply floats\n",
        "so that there are no decimals. When printing the results we do the opposite.operation\n",
        "\n",
        "max_decimals tells number of possible decimals in library classification.\n",
        "Set max_decimals to 0 if you want to omit decimals alltogether\n",
        "\"\"\"\n",
        "max_decimals = 6\n",
        "multiply_factor = 10 ** max_decimals\n",
        "\n",
        "\"\"\"\n",
        "Sklearn models canät handle NaN values, replace them with suitable value, defaul = 0\n",
        "\"\"\"\n",
        "replace_nan = 0\n",
        "\n",
        "\n",
        "def split_X_y(df):\n",
        "    \"\"\"\n",
        "    Split dataframe into features and labels\n",
        "    \"\"\"\n",
        "    # X = df.iloc[:, :-1]  # .to_numpy()\n",
        "    # y = df.iloc[:, -1]  # .to_numpy()\n",
        "\n",
        "    # for col in df.columns:\n",
        "    #    print(f\"*{col}*\")\n",
        "\n",
        "    X = df.copy().reset_index(drop=True)\n",
        "    y = X.pop(\"095\").reset_index(drop=True)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def modify_lib_data(X, y=None):\n",
        "    \"\"\"\n",
        "    Do the needed modification for library data\n",
        "    \"\"\"\n",
        "\n",
        "    # Sklearn GaussianNB doesn't handle NaN-values in input.\n",
        "    # We fill the NaN values with 0.\n",
        "    X = X.fillna(replace_nan)\n",
        "\n",
        "    # Change datatypes for features and labels\n",
        "    X = X.astype(\n",
        "        {\n",
        "            \"record_id\": \"int\",\n",
        "            \"084\": \"category\",\n",
        "            \"092\": \"category\",\n",
        "            \"093\": \"category\",\n",
        "            \"094\": \"category\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # for some reason y is of type Series\n",
        "    # We need dataframe\n",
        "    # y = y.to_frame()\n",
        "\n",
        "    # Convert labels from float to big integers,\n",
        "    # Note: Type 'Category' won't work with categorization models (at least not with GaussianNB)\n",
        "    if y is not None:\n",
        "        y = y.multiply(multiply_factor)\n",
        "        y = y.astype({\"095\": \"int\"})\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def reverse_mod_lib_data(X, y_pred, y=None):\n",
        "    \"\"\"\n",
        "    Reverse the library data back to original format\n",
        "    \"\"\"\n",
        "\n",
        "    # Sklearn GaussianNB doesn't handle NaN-values in input.\n",
        "    # We fill the NaN values with 0 and now change it back\n",
        "    X = X.replace(0, np.nan)\n",
        "\n",
        "    # Change datatypes for features and labels back to\n",
        "    X = X.astype(\n",
        "        {\n",
        "            \"record_id\": \"int\",\n",
        "            \"084\": \"float\",\n",
        "            \"092\": \"float\",\n",
        "            \"093\": \"float\",\n",
        "            \"094\": \"float\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Convert labels from category back to int\n",
        "    if y is not None:\n",
        "        y = y.multiply(1 / multiply_factor)\n",
        "    y_pred = y_pred.multiply(1 / multiply_factor)\n",
        "\n",
        "    return X, y, y_pred\n",
        "\n",
        "\n",
        "def get_train_test_data(X, y, seed, stratify=True, test_size=0.2, shuffle=True):\n",
        "    \"\"\"\n",
        "    Split the data into training and test sets\n",
        "    \"\"\"\n",
        "\n",
        "    # Stratify won't work with all datasets, it requires at least 2 rows for each label value\n",
        "    if stratify:\n",
        "        return train_test_split(\n",
        "            X, y, test_size=test_size, shuffle=shuffle, stratify=y, random_state=seed\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        return train_test_split(\n",
        "            X, y, test_size=test_size, shuffle=shuffle, random_state=seed\n",
        "        )\n",
        "\n",
        "\n",
        "def fit(model, scaler, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Fit the model\n",
        "    \"\"\"\n",
        "\n",
        "    pipe = Pipeline([(\"scaler\", scaler), (\"model\", model)])\n",
        "    pipe.fit(X_train, y_train)\n",
        "    err_train = pipe.score(X_train, y_train)\n",
        "    err_test = pipe.score(X_test, y_test)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "\n",
        "def predict(pipe, X):\n",
        "    \"\"\"\n",
        "    Use the model (pipe object) to predict labels\n",
        "    \"\"\"\n",
        "\n",
        "    y_pred = pipe.predict(X)\n",
        "    pred_probabilities = pipe.predict_proba(X)\n",
        "\n",
        "    # Print probabilities for first data point only\n",
        "    # print(\n",
        "    #    f\"\\nPredicted probability of each label for first data point:\\n{pred_probabilities[0]}\"\n",
        "    # )\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "def get_train_loss(pipe, X_train, y_train):\n",
        "    \"\"\"\n",
        "    Return train loss of fitted model\n",
        "    \"\"\"\n",
        "\n",
        "    return pipe.score(X_train, y_train)\n",
        "\n",
        "\n",
        "def get_test_loss(pipe, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Return test loss of fitted model\n",
        "    \"\"\"\n",
        "    return pipe.score(X_test, y_test)\n",
        "\n",
        "\n",
        "def loss(pipe, X, y):\n",
        "    \"\"\"\n",
        "    Return loss (model quality metric)\n",
        "\n",
        "    Note that this may be a different metric than the one that the model optimizer is using (scoring method).\n",
        "    For example for LogisticRegression the scoring method is mean accuracy,\n",
        "    but we might want to track for example f1-score for loss because it is better balanced.\n",
        "    \"\"\"\n",
        "\n",
        "    # return mean_squared_error(predict(pipe, X), y)\n",
        "    return f1_score(y, predict(pipe, X), average=\"macro\")\n",
        "\n",
        "\n",
        "def print_loss(pipe, X, y, X_train, y_train, X_test, y_test, model_name, dataset_name):\n",
        "    \"\"\"\n",
        "    Print training and validation errors\n",
        "    \"\"\"\n",
        "    print(\"\\n******************************************************************\")\n",
        "    print(f\"  Results for {model_name} with {dataset_name}:\")\n",
        "    print(\"******************************************************************\")\n",
        "\n",
        "    print(f\"Training error: {get_train_loss(pipe, X_train, y_train)}\")\n",
        "    print(f\"Validation error: {get_test_loss(pipe, X_test, y_test)}\")\n",
        "    print(f\"Loss: {loss(pipe, X, y)}\")\n",
        "\n",
        "    # train_test_df = X_train.iloc[:,1:].copy()\n",
        "    # train_test_df[\"prediction_correct\"] = (predict(pipe, X_train) - y_train.values == 0)\n",
        "    # display(train_test_df.head)\n",
        "    # _ = plot_trellis(train_test_df, legend_title=\"prediction\", true_label=\"correct\")\n",
        "\n",
        "\n",
        "def print_details(X, y, y_pred, label_name=\"label\", pred_column_name=\"pred\", n_rows=10):\n",
        "    \"\"\"\n",
        "    Print the results for observation\n",
        "    \"\"\"\n",
        "\n",
        "    y_compare = pd.concat([y, y_pred], axis=1)\n",
        "\n",
        "    print(f\"\\nOriginal and predicted labels (first {n_rows} rows):\")\n",
        "    display(y_compare.head(n_rows))\n",
        "\n",
        "    X_compare = pd.concat([X, y_compare], axis=1)\n",
        "    false_preds = X_compare[X_compare[label_name] != X_compare[pred_column_name]]\n",
        "    n_false_preds = len(false_preds)\n",
        "    n_right_preds = len(X_compare) - n_false_preds\n",
        "    print(f\"Number of false predictions: {n_false_preds}\")\n",
        "    print(f\"Number of right predictions: {n_right_preds}\")\n",
        "    print(\"\\n\\nAll false predictions in dataset:\")\n",
        "    display(false_preds)\n",
        "\n",
        "    print(\n",
        "        \"\\nHow different classifications correlate with each other on true and false predictions:\"\n",
        "    )\n",
        "    X_compare[\"prediction_correct\"] = (\n",
        "        X_compare[label_name] - X_compare[pred_column_name] == 0\n",
        "    )\n",
        "    # display(X_compare.head())\n",
        "    # FIX THIS IMPORT!!!\n",
        "    # _ = plot_trellis(X_compare.iloc[:,1:], legend_title=\"prediction\", true_label=\"correct\")\n",
        "\n",
        "\n",
        "def test_model(\n",
        "    model, scaler, df, model_name, dataset_name, test_size=0.2, verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Test the model with the help of functions above\n",
        "    \"\"\"\n",
        "\n",
        "    label_name = \"095\"\n",
        "    pred_column_name = \"095_PRED\"\n",
        "\n",
        "    # Create features and labels\n",
        "    X, y = split_X_y(df)\n",
        "\n",
        "    # Modify library data as needed\n",
        "    X, y = modify_lib_data(X, y)\n",
        "\n",
        "    # Split data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = get_train_test_data(\n",
        "        X, y, seed, stratify=False, test_size=test_size\n",
        "    )\n",
        "\n",
        "    # Fit and predict\n",
        "    pipe = fit(model, scaler, X_train, X_test, y_train, y_test)\n",
        "    y_pred = predict(pipe, X)\n",
        "\n",
        "    # convert predictions from numpy to dataframe and set an easy column name\n",
        "    y_pred = pd.DataFrame(y_pred)\n",
        "    y_pred = y_pred.rename(columns={y_pred.columns[0]: pred_column_name})\n",
        "\n",
        "    if verbose:\n",
        "        print_loss(\n",
        "            pipe, X, y, X_train, y_train, X_test, y_test, model_name, dataset_name\n",
        "        )\n",
        "\n",
        "    # Modify the library data back to original format\n",
        "    X, y, y_pred = reverse_mod_lib_data(X, y_pred, y)\n",
        "\n",
        "    # Print the results with desired column names\n",
        "    if verbose:\n",
        "        print_details(X, y, y_pred, label_name, pred_column_name, 30)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "\n",
        "def predict_hkl_class(pipe, items_to_classify, y=None):\n",
        "    \"\"\"\n",
        "    Predict the actual HKL class\n",
        "    \"\"\"\n",
        "    pred_column_name = \"095_PRED\"\n",
        "\n",
        "    # Modify library data as needed\n",
        "    X, y = modify_lib_data(items_to_classify, y)\n",
        "\n",
        "    y_pred = predict(pipe, X)\n",
        "\n",
        "    # convert predictions from numpy to dataframe and set an easy column name\n",
        "    y_pred = pd.DataFrame(y_pred)\n",
        "    y_pred = y_pred.rename(columns={y_pred.columns[0]: pred_column_name})\n",
        "\n",
        "    # Modify the library data back to original format\n",
        "    X, y, y_pred = reverse_mod_lib_data(X, y_pred, y)\n",
        "\n",
        "    result = pd.concat([X, y_pred], axis=1)\n",
        "\n",
        "    return result"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Javascript object>",
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 10;\n                var nbb_unformatted_code = \"# export\\n\\n# FIX THIS IMPORT!!!!\\n# These functions won't work if we don't define these also in this code block\\n# from lib_classification.plot import plot_trellis, plot_histogram\\n\\n\\nseed = 0\\n\\n\\n\\\"\\\"\\\"\\nLabels can't be of type float for classification. Thus we multiply floats\\nso that there are no decimals. When printing the results we do the opposite.operation\\n\\nmax_decimals tells number of possible decimals in library classification.\\nSet max_decimals to 0 if you want to omit decimals alltogether\\n\\\"\\\"\\\"\\nmax_decimals = 6\\nmultiply_factor = 10 ** max_decimals\\n\\n\\\"\\\"\\\"\\nSklearn models can\\u00e4t handle NaN values, replace them with suitable value, defaul = 0\\n\\\"\\\"\\\"\\nreplace_nan = 0\\n\\n\\ndef split_X_y(df):\\n    \\\"\\\"\\\"\\n    Split dataframe into features and labels\\n    \\\"\\\"\\\"\\n    # X = df.iloc[:, :-1]  # .to_numpy()\\n    # y = df.iloc[:, -1]  # .to_numpy()\\n\\n    # for col in df.columns:\\n    #    print(f\\\"*{col}*\\\")\\n\\n    X = df.copy().reset_index(drop=True)\\n    y = X.pop(\\\"095\\\").reset_index(drop=True)\\n\\n    return X, y\\n\\n\\ndef modify_lib_data(X, y=None):\\n    \\\"\\\"\\\"\\n    Do the needed modification for library data\\n    \\\"\\\"\\\"\\n\\n    # Sklearn GaussianNB doesn't handle NaN-values in input.\\n    # We fill the NaN values with 0.\\n    X = X.fillna(replace_nan)\\n\\n    # Change datatypes for features and labels\\n    X = X.astype(\\n        {\\n            \\\"record_id\\\": \\\"int\\\",\\n            \\\"084\\\": \\\"category\\\",\\n            \\\"092\\\": \\\"category\\\",\\n            \\\"093\\\": \\\"category\\\",\\n            \\\"094\\\": \\\"category\\\",\\n        }\\n    )\\n\\n    # for some reason y is of type Series\\n    # We need dataframe\\n    # y = y.to_frame()\\n\\n    # Convert labels from float to big integers,\\n    # Note: Type 'Category' won't work with categorization models (at least not with GaussianNB)\\n    if y is not None:\\n        y = y.multiply(multiply_factor)\\n        y = y.astype({\\\"095\\\": \\\"int\\\"})\\n\\n    return X, y\\n\\n\\ndef reverse_mod_lib_data(X, y_pred, y=None):\\n    \\\"\\\"\\\"\\n    Reverse the library data back to original format\\n    \\\"\\\"\\\"\\n\\n    # Sklearn GaussianNB doesn't handle NaN-values in input.\\n    # We fill the NaN values with 0 and now change it back\\n    X = X.replace(0, np.nan)\\n\\n    # Change datatypes for features and labels back to\\n    X = X.astype(\\n        {\\n            \\\"record_id\\\": \\\"int\\\",\\n            \\\"084\\\": \\\"float\\\",\\n            \\\"092\\\": \\\"float\\\",\\n            \\\"093\\\": \\\"float\\\",\\n            \\\"094\\\": \\\"float\\\",\\n        }\\n    )\\n\\n    # Convert labels from category back to int\\n    if y is not None:\\n        y = y.multiply(1 / multiply_factor)\\n    y_pred = y_pred.multiply(1 / multiply_factor)\\n\\n    return X, y, y_pred\\n\\n\\ndef get_train_test_data(X, y, seed, stratify=True, test_size=0.2, shuffle=True):\\n    \\\"\\\"\\\"\\n    Split the data into training and test sets\\n    \\\"\\\"\\\"\\n\\n    # Stratify won't work with all datasets, it requires at least 2 rows for each label value\\n    if stratify:\\n        return train_test_split(\\n            X, y, test_size=test_size, shuffle=shuffle, stratify=y, random_state=seed\\n        )\\n\\n    else:\\n        return train_test_split(\\n            X, y, test_size=test_size, shuffle=shuffle, random_state=seed\\n        )\\n\\n\\ndef fit(model, scaler, X_train, X_test, y_train, y_test):\\n    \\\"\\\"\\\"\\n    Fit the model\\n    \\\"\\\"\\\"\\n\\n    pipe = Pipeline([(\\\"scaler\\\", scaler), (\\\"model\\\", model)])\\n    pipe.fit(X_train, y_train)\\n    err_train = pipe.score(X_train, y_train)\\n    err_test = pipe.score(X_test, y_test)\\n\\n    return pipe\\n\\n\\ndef predict(pipe, X):\\n    \\\"\\\"\\\"\\n    Use the model (pipe object) to predict labels\\n    \\\"\\\"\\\"\\n\\n    y_pred = pipe.predict(X)\\n    pred_probabilities = pipe.predict_proba(X)\\n\\n    # Print probabilities for first data point only\\n    # print(\\n    #    f\\\"\\\\nPredicted probability of each label for first data point:\\\\n{pred_probabilities[0]}\\\"\\n    # )\\n\\n    return y_pred\\n\\n\\ndef get_train_loss(pipe, X_train, y_train):\\n    \\\"\\\"\\\"\\n    Return train loss of fitted model\\n    \\\"\\\"\\\"\\n\\n    return pipe.score(X_train, y_train)\\n\\n\\ndef get_test_loss(pipe, X_test, y_test):\\n    \\\"\\\"\\\"\\n    Return test loss of fitted model\\n    \\\"\\\"\\\"\\n    return pipe.score(X_test, y_test)\\n\\n\\ndef loss(pipe, X, y):\\n    \\\"\\\"\\\"\\n    Return loss (model quality metric)\\n\\n    Note that this may be a different metric than the one that the model optimizer is using (scoring method).\\n    For example for LogisticRegression the scoring method is mean accuracy,\\n    but we might want to track for example f1-score for loss because it is better balanced.\\n    \\\"\\\"\\\"\\n\\n    # return mean_squared_error(predict(pipe, X), y)\\n    return f1_score(y, predict(pipe, X), average=\\\"macro\\\")\\n\\n\\ndef print_loss(pipe, X, y, X_train, y_train, X_test, y_test, model_name, dataset_name):\\n    \\\"\\\"\\\"\\n    Print training and validation errors\\n    \\\"\\\"\\\"\\n    print(\\\"\\\\n******************************************************************\\\")\\n    print(f\\\"  Results for {model_name} with {dataset_name}:\\\")\\n    print(\\\"******************************************************************\\\")\\n\\n    print(f\\\"Training error: {get_train_loss(pipe, X_train, y_train)}\\\")\\n    print(f\\\"Validation error: {get_test_loss(pipe, X_test, y_test)}\\\")\\n    print(f\\\"Loss: {loss(pipe, X, y)}\\\")\\n\\n    # train_test_df = X_train.iloc[:,1:].copy()\\n    # train_test_df[\\\"prediction_correct\\\"] = (predict(pipe, X_train) - y_train.values == 0)\\n    # display(train_test_df.head)\\n    # _ = plot_trellis(train_test_df, legend_title=\\\"prediction\\\", true_label=\\\"correct\\\")\\n\\n\\ndef print_details(X, y, y_pred, label_name=\\\"label\\\", pred_column_name=\\\"pred\\\", n_rows=10):\\n    \\\"\\\"\\\"\\n    Print the results for observation\\n    \\\"\\\"\\\"\\n\\n    y_compare = pd.concat([y, y_pred], axis=1)\\n\\n    print(f\\\"\\\\nOriginal and predicted labels (first {n_rows} rows):\\\")\\n    display(y_compare.head(n_rows))\\n\\n    X_compare = pd.concat([X, y_compare], axis=1)\\n    false_preds = X_compare[X_compare[label_name] != X_compare[pred_column_name]]\\n    n_false_preds = len(false_preds)\\n    n_right_preds = len(X_compare) - n_false_preds\\n    print(f\\\"Number of false predictions: {n_false_preds}\\\")\\n    print(f\\\"Number of right predictions: {n_right_preds}\\\")\\n    print(\\\"\\\\n\\\\nAll false predictions in dataset:\\\")\\n    display(false_preds)\\n\\n    print(\\n        \\\"\\\\nHow different classifications correlate with each other on true and false predictions:\\\"\\n    )\\n    X_compare[\\\"prediction_correct\\\"] = (\\n        X_compare[label_name] - X_compare[pred_column_name] == 0\\n    )\\n    # display(X_compare.head())\\n    # FIX THIS IMPORT!!!\\n    # _ = plot_trellis(X_compare.iloc[:,1:], legend_title=\\\"prediction\\\", true_label=\\\"correct\\\")\\n\\n\\ndef test_model(\\n    model, scaler, df, model_name, dataset_name, test_size=0.2, verbose=True\\n):\\n    \\\"\\\"\\\"\\n    Test the model with the help of functions above\\n    \\\"\\\"\\\"\\n\\n    label_name = \\\"095\\\"\\n    pred_column_name = \\\"095_PRED\\\"\\n\\n    # Create features and labels\\n    X, y = split_X_y(df)\\n\\n    # Modify library data as needed\\n    X, y = modify_lib_data(X, y)\\n\\n    # Split data into training and test sets\\n    X_train, X_test, y_train, y_test = get_train_test_data(\\n        X, y, seed, stratify=False, test_size=test_size\\n    )\\n\\n    # Fit and predict\\n    pipe = fit(model, scaler, X_train, X_test, y_train, y_test)\\n    y_pred = predict(pipe, X)\\n\\n    # convert predictions from numpy to dataframe and set an easy column name\\n    y_pred = pd.DataFrame(y_pred)\\n    y_pred = y_pred.rename(columns={y_pred.columns[0]: pred_column_name})\\n\\n    if verbose:\\n        print_loss(\\n            pipe, X, y, X_train, y_train, X_test, y_test, model_name, dataset_name\\n        )\\n\\n    # Modify the library data back to original format\\n    X, y, y_pred = reverse_mod_lib_data(X, y_pred, y)\\n\\n    # Print the results with desired column names\\n    if verbose:\\n        print_details(X, y, y_pred, label_name, pred_column_name, 30)\\n\\n    return pipe\\n\\n\\ndef predict_hkl_class(pipe, items_to_classify, y=None):\\n    \\\"\\\"\\\"\\n    Predict the actual HKL class\\n    \\\"\\\"\\\"\\n    pred_column_name = \\\"095_PRED\\\"\\n\\n    # Modify library data as needed\\n    X, y = modify_lib_data(items_to_classify, y)\\n\\n    y_pred = predict(pipe, X)\\n\\n    # convert predictions from numpy to dataframe and set an easy column name\\n    y_pred = pd.DataFrame(y_pred)\\n    y_pred = y_pred.rename(columns={y_pred.columns[0]: pred_column_name})\\n\\n    # Modify the library data back to original format\\n    X, y, y_pred = reverse_mod_lib_data(X, y_pred, y)\\n\\n    result = pd.concat([X, y_pred], axis=1)\\n\\n    return result\";\n                var nbb_formatted_code = \"# export\\n\\n# FIX THIS IMPORT!!!!\\n# These functions won't work if we don't define these also in this code block\\n# from lib_classification.plot import plot_trellis, plot_histogram\\n\\n\\nseed = 0\\n\\n\\n\\\"\\\"\\\"\\nLabels can't be of type float for classification. Thus we multiply floats\\nso that there are no decimals. When printing the results we do the opposite.operation\\n\\nmax_decimals tells number of possible decimals in library classification.\\nSet max_decimals to 0 if you want to omit decimals alltogether\\n\\\"\\\"\\\"\\nmax_decimals = 6\\nmultiply_factor = 10 ** max_decimals\\n\\n\\\"\\\"\\\"\\nSklearn models can\\u00e4t handle NaN values, replace them with suitable value, defaul = 0\\n\\\"\\\"\\\"\\nreplace_nan = 0\\n\\n\\ndef split_X_y(df):\\n    \\\"\\\"\\\"\\n    Split dataframe into features and labels\\n    \\\"\\\"\\\"\\n    # X = df.iloc[:, :-1]  # .to_numpy()\\n    # y = df.iloc[:, -1]  # .to_numpy()\\n\\n    # for col in df.columns:\\n    #    print(f\\\"*{col}*\\\")\\n\\n    X = df.copy().reset_index(drop=True)\\n    y = X.pop(\\\"095\\\").reset_index(drop=True)\\n\\n    return X, y\\n\\n\\ndef modify_lib_data(X, y=None):\\n    \\\"\\\"\\\"\\n    Do the needed modification for library data\\n    \\\"\\\"\\\"\\n\\n    # Sklearn GaussianNB doesn't handle NaN-values in input.\\n    # We fill the NaN values with 0.\\n    X = X.fillna(replace_nan)\\n\\n    # Change datatypes for features and labels\\n    X = X.astype(\\n        {\\n            \\\"record_id\\\": \\\"int\\\",\\n            \\\"084\\\": \\\"category\\\",\\n            \\\"092\\\": \\\"category\\\",\\n            \\\"093\\\": \\\"category\\\",\\n            \\\"094\\\": \\\"category\\\",\\n        }\\n    )\\n\\n    # for some reason y is of type Series\\n    # We need dataframe\\n    # y = y.to_frame()\\n\\n    # Convert labels from float to big integers,\\n    # Note: Type 'Category' won't work with categorization models (at least not with GaussianNB)\\n    if y is not None:\\n        y = y.multiply(multiply_factor)\\n        y = y.astype({\\\"095\\\": \\\"int\\\"})\\n\\n    return X, y\\n\\n\\ndef reverse_mod_lib_data(X, y_pred, y=None):\\n    \\\"\\\"\\\"\\n    Reverse the library data back to original format\\n    \\\"\\\"\\\"\\n\\n    # Sklearn GaussianNB doesn't handle NaN-values in input.\\n    # We fill the NaN values with 0 and now change it back\\n    X = X.replace(0, np.nan)\\n\\n    # Change datatypes for features and labels back to\\n    X = X.astype(\\n        {\\n            \\\"record_id\\\": \\\"int\\\",\\n            \\\"084\\\": \\\"float\\\",\\n            \\\"092\\\": \\\"float\\\",\\n            \\\"093\\\": \\\"float\\\",\\n            \\\"094\\\": \\\"float\\\",\\n        }\\n    )\\n\\n    # Convert labels from category back to int\\n    if y is not None:\\n        y = y.multiply(1 / multiply_factor)\\n    y_pred = y_pred.multiply(1 / multiply_factor)\\n\\n    return X, y, y_pred\\n\\n\\ndef get_train_test_data(X, y, seed, stratify=True, test_size=0.2, shuffle=True):\\n    \\\"\\\"\\\"\\n    Split the data into training and test sets\\n    \\\"\\\"\\\"\\n\\n    # Stratify won't work with all datasets, it requires at least 2 rows for each label value\\n    if stratify:\\n        return train_test_split(\\n            X, y, test_size=test_size, shuffle=shuffle, stratify=y, random_state=seed\\n        )\\n\\n    else:\\n        return train_test_split(\\n            X, y, test_size=test_size, shuffle=shuffle, random_state=seed\\n        )\\n\\n\\ndef fit(model, scaler, X_train, X_test, y_train, y_test):\\n    \\\"\\\"\\\"\\n    Fit the model\\n    \\\"\\\"\\\"\\n\\n    pipe = Pipeline([(\\\"scaler\\\", scaler), (\\\"model\\\", model)])\\n    pipe.fit(X_train, y_train)\\n    err_train = pipe.score(X_train, y_train)\\n    err_test = pipe.score(X_test, y_test)\\n\\n    return pipe\\n\\n\\ndef predict(pipe, X):\\n    \\\"\\\"\\\"\\n    Use the model (pipe object) to predict labels\\n    \\\"\\\"\\\"\\n\\n    y_pred = pipe.predict(X)\\n    pred_probabilities = pipe.predict_proba(X)\\n\\n    # Print probabilities for first data point only\\n    # print(\\n    #    f\\\"\\\\nPredicted probability of each label for first data point:\\\\n{pred_probabilities[0]}\\\"\\n    # )\\n\\n    return y_pred\\n\\n\\ndef get_train_loss(pipe, X_train, y_train):\\n    \\\"\\\"\\\"\\n    Return train loss of fitted model\\n    \\\"\\\"\\\"\\n\\n    return pipe.score(X_train, y_train)\\n\\n\\ndef get_test_loss(pipe, X_test, y_test):\\n    \\\"\\\"\\\"\\n    Return test loss of fitted model\\n    \\\"\\\"\\\"\\n    return pipe.score(X_test, y_test)\\n\\n\\ndef loss(pipe, X, y):\\n    \\\"\\\"\\\"\\n    Return loss (model quality metric)\\n\\n    Note that this may be a different metric than the one that the model optimizer is using (scoring method).\\n    For example for LogisticRegression the scoring method is mean accuracy,\\n    but we might want to track for example f1-score for loss because it is better balanced.\\n    \\\"\\\"\\\"\\n\\n    # return mean_squared_error(predict(pipe, X), y)\\n    return f1_score(y, predict(pipe, X), average=\\\"macro\\\")\\n\\n\\ndef print_loss(pipe, X, y, X_train, y_train, X_test, y_test, model_name, dataset_name):\\n    \\\"\\\"\\\"\\n    Print training and validation errors\\n    \\\"\\\"\\\"\\n    print(\\\"\\\\n******************************************************************\\\")\\n    print(f\\\"  Results for {model_name} with {dataset_name}:\\\")\\n    print(\\\"******************************************************************\\\")\\n\\n    print(f\\\"Training error: {get_train_loss(pipe, X_train, y_train)}\\\")\\n    print(f\\\"Validation error: {get_test_loss(pipe, X_test, y_test)}\\\")\\n    print(f\\\"Loss: {loss(pipe, X, y)}\\\")\\n\\n    # train_test_df = X_train.iloc[:,1:].copy()\\n    # train_test_df[\\\"prediction_correct\\\"] = (predict(pipe, X_train) - y_train.values == 0)\\n    # display(train_test_df.head)\\n    # _ = plot_trellis(train_test_df, legend_title=\\\"prediction\\\", true_label=\\\"correct\\\")\\n\\n\\ndef print_details(X, y, y_pred, label_name=\\\"label\\\", pred_column_name=\\\"pred\\\", n_rows=10):\\n    \\\"\\\"\\\"\\n    Print the results for observation\\n    \\\"\\\"\\\"\\n\\n    y_compare = pd.concat([y, y_pred], axis=1)\\n\\n    print(f\\\"\\\\nOriginal and predicted labels (first {n_rows} rows):\\\")\\n    display(y_compare.head(n_rows))\\n\\n    X_compare = pd.concat([X, y_compare], axis=1)\\n    false_preds = X_compare[X_compare[label_name] != X_compare[pred_column_name]]\\n    n_false_preds = len(false_preds)\\n    n_right_preds = len(X_compare) - n_false_preds\\n    print(f\\\"Number of false predictions: {n_false_preds}\\\")\\n    print(f\\\"Number of right predictions: {n_right_preds}\\\")\\n    print(\\\"\\\\n\\\\nAll false predictions in dataset:\\\")\\n    display(false_preds)\\n\\n    print(\\n        \\\"\\\\nHow different classifications correlate with each other on true and false predictions:\\\"\\n    )\\n    X_compare[\\\"prediction_correct\\\"] = (\\n        X_compare[label_name] - X_compare[pred_column_name] == 0\\n    )\\n    # display(X_compare.head())\\n    # FIX THIS IMPORT!!!\\n    # _ = plot_trellis(X_compare.iloc[:,1:], legend_title=\\\"prediction\\\", true_label=\\\"correct\\\")\\n\\n\\ndef test_model(\\n    model, scaler, df, model_name, dataset_name, test_size=0.2, verbose=True\\n):\\n    \\\"\\\"\\\"\\n    Test the model with the help of functions above\\n    \\\"\\\"\\\"\\n\\n    label_name = \\\"095\\\"\\n    pred_column_name = \\\"095_PRED\\\"\\n\\n    # Create features and labels\\n    X, y = split_X_y(df)\\n\\n    # Modify library data as needed\\n    X, y = modify_lib_data(X, y)\\n\\n    # Split data into training and test sets\\n    X_train, X_test, y_train, y_test = get_train_test_data(\\n        X, y, seed, stratify=False, test_size=test_size\\n    )\\n\\n    # Fit and predict\\n    pipe = fit(model, scaler, X_train, X_test, y_train, y_test)\\n    y_pred = predict(pipe, X)\\n\\n    # convert predictions from numpy to dataframe and set an easy column name\\n    y_pred = pd.DataFrame(y_pred)\\n    y_pred = y_pred.rename(columns={y_pred.columns[0]: pred_column_name})\\n\\n    if verbose:\\n        print_loss(\\n            pipe, X, y, X_train, y_train, X_test, y_test, model_name, dataset_name\\n        )\\n\\n    # Modify the library data back to original format\\n    X, y, y_pred = reverse_mod_lib_data(X, y_pred, y)\\n\\n    # Print the results with desired column names\\n    if verbose:\\n        print_details(X, y, y_pred, label_name, pred_column_name, 30)\\n\\n    return pipe\\n\\n\\ndef predict_hkl_class(pipe, items_to_classify, y=None):\\n    \\\"\\\"\\\"\\n    Predict the actual HKL class\\n    \\\"\\\"\\\"\\n    pred_column_name = \\\"095_PRED\\\"\\n\\n    # Modify library data as needed\\n    X, y = modify_lib_data(items_to_classify, y)\\n\\n    y_pred = predict(pipe, X)\\n\\n    # convert predictions from numpy to dataframe and set an easy column name\\n    y_pred = pd.DataFrame(y_pred)\\n    y_pred = y_pred.rename(columns={y_pred.columns[0]: pred_column_name})\\n\\n    # Modify the library data back to original format\\n    X, y, y_pred = reverse_mod_lib_data(X, y_pred, y)\\n\\n    result = pd.concat([X, y_pred], axis=1)\\n\\n    return result\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1635766019951
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin by splitting our data to train and test data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by selecting and tuning our ML model with the help of general functions we created above.\n",
        "[Choosing the right estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
        "\n",
        "We probably want to try at least these models:\n",
        " - [linear SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
        " - [SGD classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html?highlight=sgd%20classifier#sklearn.linear_model.SGDClassifier)\n",
        " - [KNeighbors classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=kneighbors#sklearn.neighbors.KNeighborsClassifier)\n",
        " - [Decision tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontree#sklearn.tree.DecisionTreeClassifier)\n",
        " - [Random forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforest#sklearn.ensemble.RandomForestClassifier)\n",
        " - [Naive Bayes](https://scikit-learn.org/stable/modules/classes.html?highlight=naive%20bayes#module-sklearn.naive_bayes) (Sami)\n",
        "\n",
        "Some useful links for choosing the estimator\n",
        " - [Classifier comparison](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
        " - [Choosing the right estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some initial params\n",
        "k = 5\n",
        "test_size = 0.2\n",
        "\n",
        "#######################################\n",
        "# Test Naive Bayes model\n",
        "#######################################\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "\n",
        "#for col in toy_df.columns:\n",
        "#    print(f\"*{col}*\")\n",
        "          \n",
        "_ = test_model(\n",
        "    GaussianNB(),\n",
        "    StandardScaler(),\n",
        "    toy_df,\n",
        "    \"GAUSSIAN NAIVE BAYES\",\n",
        "    \"TOY DATASET\",\n",
        "    test_size,\n",
        ")\n",
        "\n",
        "#\n",
        "# Much worse results with complementNB\n",
        "#\n",
        "# test_model(\n",
        "#\n",
        "# ComplementNB(),\n",
        "#    MinMaxScaler(),\n",
        "#    toy_df,\n",
        "#    \"COMPLEMENT NAIVE BAYES\",\n",
        "#    \"TOY DATASET\",\n",
        "#    test_size,\n",
        "# )\n",
        "#\n",
        "\n",
        "#\n",
        "# Skip hyperparameter tuning for now, maybe implement this later\n",
        "#\n",
        "\n",
        "#cv = StratifiedKFold(n_splits=k)\n",
        "#print(cross_val_score(pipe, X_train, y_train, cv=cv))\n",
        "\n",
        "## optimize\n",
        "#param_grid = {\n",
        "#    \"estimator__C\": np.logspace(-4, 4, 10),\n",
        "#}\n",
        "\n",
        "# make_pipeline(Imputer(),StandardScaler(),PCA(n_components=2),SVC(random_state=1))\n",
        "\n",
        "# cv = StratifiedKFold(n_splits=5)\n",
        "#gs = GridSearchCV(\n",
        "#    estimator=pipe,\n",
        "#    param_grid=param_grid,\n",
        "#    scoring=\"accuracy\",\n",
        "#    cv=cv,\n",
        "#    return_train_score=True,\n",
        "#)\n",
        "#gs.fit(X_train, y_train)\n",
        "#\n",
        "#print(\"Best Estimator: \\n{}\\n\".format(gs.best_estimator_))\n",
        "#print(\"Best Parameters: \\n{}\\n\".format(gs.best_params_))\n",
        "#print(\"Best Test Score: \\n{}\\n\".format(gs.best_score_))\n",
        "#print(\n",
        "#    \"Best Training Score: \\n{}\\n\".format(\n",
        "#        gs.cv_results_[\"mean_train_score\"][gs.best_index_]\n",
        "#    )\n",
        "#)\n",
        "#print(\"All Training Scores: \\n{}\\n\".format(gs.cv_results_[\"mean_train_score\"]))\n",
        "#print(\"All Test Scores: \\n{}\\n\".format(gs.cv_results_[\"mean_test_score\"]))\n",
        "# # This prints out all results during Cross-Validation in details\n",
        "# print(\"All Meta Results During CV Search: \\n{}\\n\".format(gs.cv_results_))\n",
        "\n",
        "# Reset pipeline with best params\n",
        "#pipe.set_params(estimator__C=gs.best_params_[\"estimator__C\"])\n",
        "#pipe.fit(X_train, y_train)\n",
        "#print(\"Test score with best params (should equal to Best Test Score above)\")\n",
        "#print(pipe.score(X_test, y_test))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n******************************************************************\n  Results for GAUSSIAN NAIVE BAYES with TOY DATASET:\n******************************************************************\nTraining error: 1.0\nValidation error: 0.1\nLoss: 0.8016064257028113\n\nOriginal and predicted labels (first 30 rows):\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>095</th>\n      <th>095_PRED</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>788.830</td>\n      <td>788.830</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>788.330</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>770.921</td>\n      <td>788.410</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>357.000</td>\n      <td>357.000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>788.332</td>\n      <td>788.332</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>788.410</td>\n      <td>788.410</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>928.100</td>\n      <td>928.100</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>788.330</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>226.000</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>427.000</td>\n      <td>427.000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>788.400</td>\n      <td>788.400</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>333.400</td>\n      <td>333.400</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>785.620</td>\n      <td>785.620</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>140.000</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>439.000</td>\n      <td>439.000</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>520.000</td>\n      <td>520.000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>783.610</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>788.330</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>657.000</td>\n      <td>657.000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>613.100</td>\n      <td>613.100</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>182.600</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>618.240</td>\n      <td>618.240</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>788.330</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>788.410</td>\n      <td>788.410</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>846.100</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>990.000</td>\n      <td>990.000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>691.110</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>493.000</td>\n      <td>493.000</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>929.110</td>\n      <td>929.110</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>384.000</td>\n      <td>384.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "        095  095_PRED\n0   788.830   788.830\n1   788.330   788.330\n2   770.921   788.410\n3   357.000   357.000\n4   788.332   788.332\n5   788.410   788.410\n6   928.100   928.100\n7   788.330   788.330\n8   226.000   788.330\n9   427.000   427.000\n10  788.400   788.400\n11  333.400   333.400\n12  785.620   785.620\n13  140.000   788.330\n14  439.000   439.000\n15  520.000   520.000\n16  783.610   788.330\n17  788.330   788.330\n18  657.000   657.000\n19  613.100   613.100\n20  182.600   182.600\n21  618.240   618.240\n22  788.330   788.330\n23  788.410   788.410\n24  846.100   788.330\n25  990.000   990.000\n26  691.110   788.330\n27  493.000   493.000\n28  929.110   929.110\n29  384.000   384.000"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of false predictions: 18\nNumber of right predictions: 82\n\n\nAll false predictions in dataset:\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>record_id</th>\n      <th>084</th>\n      <th>092</th>\n      <th>093</th>\n      <th>094</th>\n      <th>nan</th>\n      <th>näyttelijät</th>\n      <th>elokuvanäyttelijät</th>\n      <th>laulajat</th>\n      <th>iskelmät</th>\n      <th>...</th>\n      <th>resilienssi</th>\n      <th>selviytyminen</th>\n      <th>vaikeudet</th>\n      <th>itsekasvatus</th>\n      <th>elämänhallinta</th>\n      <th>itseluottamus</th>\n      <th>tunnetaidot</th>\n      <th>itsensä toteuttaminen</th>\n      <th>095</th>\n      <th>095_PRED</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>420908537805</td>\n      <td>99.10000</td>\n      <td>77.1920</td>\n      <td>NaN</td>\n      <td>77.19200</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>770.921</td>\n      <td>788.410</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>420908060431</td>\n      <td>22.20000</td>\n      <td>22.2000</td>\n      <td>NaN</td>\n      <td>22.20000</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>226.000</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>420909110168</td>\n      <td>16.10000</td>\n      <td>16.1000</td>\n      <td>NaN</td>\n      <td>16.10000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>140.000</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>420908423592</td>\n      <td>78.35100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>78.35100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>783.610</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>420908875333</td>\n      <td>86.83200</td>\n      <td>86.8320</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>846.100</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>420908761605</td>\n      <td>68.24000</td>\n      <td>68.2400</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>691.110</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>420909025020</td>\n      <td>42.33000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>412.110</td>\n      <td>357.000</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>420908069830</td>\n      <td>32.21000</td>\n      <td>32.2100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>323.300</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>420908655558</td>\n      <td>78.34300</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>78.34300</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>783.421</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>420909085349</td>\n      <td>59.24100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>613.300</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>420908822169</td>\n      <td>79.12000</td>\n      <td>79.1200</td>\n      <td>NaN</td>\n      <td>79.12000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>791.500</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>420908642255</td>\n      <td>59.30000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>59.50000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>616.200</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>420907909925</td>\n      <td>69.10000</td>\n      <td>69.1000</td>\n      <td>NaN</td>\n      <td>69.10000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>674.000</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>420909177908</td>\n      <td>78.89112</td>\n      <td>78.8911</td>\n      <td>NaN</td>\n      <td>78.89112</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.330</td>\n      <td>788.332</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>420908241951</td>\n      <td>76.00000</td>\n      <td>76.0000</td>\n      <td>NaN</td>\n      <td>76.00000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>760.000</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>420907935317</td>\n      <td>78.61000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>786.111</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>420908733366</td>\n      <td>78.89000</td>\n      <td>78.8910</td>\n      <td>78.891</td>\n      <td>78.89100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.330</td>\n      <td>788.410</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>420909137083</td>\n      <td>1.40000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10.600</td>\n      <td>788.330</td>\n    </tr>\n  </tbody>\n</table>\n<p>18 rows × 488 columns</p>\n</div>",
            "text/plain": "       record_id       084      092     093       094  nan  näyttelijät  \\\n2   420908537805  99.10000  77.1920     NaN  77.19200  NaN          1.0   \n8   420908060431  22.20000  22.2000     NaN  22.20000  1.0          NaN   \n13  420909110168  16.10000  16.1000     NaN  16.10000  NaN          NaN   \n16  420908423592  78.35100      NaN     NaN  78.35100  NaN          NaN   \n24  420908875333  86.83200  86.8320     NaN       NaN  NaN          NaN   \n26  420908761605  68.24000  68.2400     NaN       NaN  NaN          NaN   \n30  420909025020  42.33000      NaN     NaN       NaN  NaN          NaN   \n33  420908069830  32.21000  32.2100     NaN       NaN  NaN          NaN   \n53  420908655558  78.34300      NaN     NaN  78.34300  NaN          NaN   \n54  420909085349  59.24100      NaN     NaN       NaN  NaN          NaN   \n55  420908822169  79.12000  79.1200     NaN  79.12000  NaN          NaN   \n73  420908642255  59.30000      NaN     NaN  59.50000  NaN          NaN   \n75  420907909925  69.10000  69.1000     NaN  69.10000  NaN          NaN   \n78  420909177908  78.89112  78.8911     NaN  78.89112  1.0          NaN   \n86  420908241951  76.00000  76.0000     NaN  76.00000  NaN          NaN   \n92  420907935317  78.61000      NaN     NaN       NaN  NaN          NaN   \n93  420908733366  78.89000  78.8910  78.891  78.89100  NaN          NaN   \n95  420909137083   1.40000      NaN     NaN       NaN  NaN          NaN   \n\n    elokuvanäyttelijät  laulajat  iskelmät  ...  resilienssi  selviytyminen  \\\n2                  1.0       1.0       1.0  ...          NaN            NaN   \n8                  NaN       NaN       NaN  ...          NaN            NaN   \n13                 NaN       NaN       NaN  ...          NaN            NaN   \n16                 NaN       NaN       NaN  ...          NaN            NaN   \n24                 NaN       NaN       NaN  ...          NaN            NaN   \n26                 NaN       NaN       NaN  ...          NaN            NaN   \n30                 NaN       NaN       NaN  ...          NaN            NaN   \n33                 NaN       NaN       NaN  ...          NaN            NaN   \n53                 NaN       NaN       NaN  ...          NaN            NaN   \n54                 NaN       NaN       NaN  ...          NaN            NaN   \n55                 NaN       NaN       NaN  ...          NaN            NaN   \n73                 NaN       NaN       NaN  ...          NaN            NaN   \n75                 NaN       NaN       NaN  ...          NaN            NaN   \n78                 NaN       NaN       NaN  ...          NaN            NaN   \n86                 NaN       NaN       NaN  ...          NaN            NaN   \n92                 NaN       NaN       NaN  ...          NaN            NaN   \n93                 NaN       NaN       1.0  ...          NaN            NaN   \n95                 NaN       NaN       NaN  ...          NaN            NaN   \n\n    vaikeudet  itsekasvatus  elämänhallinta  itseluottamus  tunnetaidot  \\\n2         NaN           NaN             NaN            NaN          NaN   \n8         NaN           NaN             NaN            NaN          NaN   \n13        NaN           NaN             NaN            NaN          NaN   \n16        NaN           NaN             NaN            NaN          NaN   \n24        NaN           NaN             NaN            NaN          NaN   \n26        NaN           NaN             NaN            NaN          NaN   \n30        NaN           NaN             NaN            NaN          NaN   \n33        NaN           NaN             NaN            NaN          NaN   \n53        NaN           NaN             NaN            NaN          NaN   \n54        NaN           NaN             NaN            NaN          NaN   \n55        NaN           NaN             NaN            NaN          NaN   \n73        NaN           NaN             NaN            NaN          NaN   \n75        NaN           NaN             NaN            NaN          NaN   \n78        NaN           NaN             NaN            NaN          NaN   \n86        NaN           NaN             NaN            NaN          NaN   \n92        NaN           NaN             NaN            NaN          NaN   \n93        NaN           NaN             NaN            NaN          NaN   \n95        NaN           NaN             NaN            NaN          NaN   \n\n    itsensä toteuttaminen      095  095_PRED  \n2                     NaN  770.921   788.410  \n8                     NaN  226.000   788.330  \n13                    NaN  140.000   788.330  \n16                    NaN  783.610   788.330  \n24                    NaN  846.100   788.330  \n26                    NaN  691.110   788.330  \n30                    NaN  412.110   357.000  \n33                    NaN  323.300   788.330  \n53                    NaN  783.421   788.330  \n54                    NaN  613.300   788.330  \n55                    NaN  791.500   788.330  \n73                    NaN  616.200   788.330  \n75                    NaN  674.000   788.330  \n78                    NaN  788.330   788.332  \n86                    NaN  760.000   788.330  \n92                    NaN  786.111   788.330  \n93                    NaN  788.330   788.410  \n95                    NaN   10.600   788.330  \n\n[18 rows x 488 columns]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nHow different classifications correlate with each other on true and false predictions:\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 54;\n                var nbb_unformatted_code = \"# Define some initial params\\nk = 5\\ntest_size = 0.2\\n\\n#######################################\\n# Test Naive Bayes model\\n#######################################\\n\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.naive_bayes import ComplementNB\\n\\n#for col in toy_df.columns:\\n#    print(f\\\"*{col}*\\\")\\n          \\n_ = test_model(\\n    GaussianNB(),\\n    StandardScaler(),\\n    toy_df,\\n    \\\"GAUSSIAN NAIVE BAYES\\\",\\n    \\\"TOY DATASET\\\",\\n    test_size,\\n)\\n\\n#\\n# Much worse results with complementNB\\n#\\n# test_model(\\n#\\n# ComplementNB(),\\n#    MinMaxScaler(),\\n#    toy_df,\\n#    \\\"COMPLEMENT NAIVE BAYES\\\",\\n#    \\\"TOY DATASET\\\",\\n#    test_size,\\n# )\\n#\\n\\n#\\n# Skip hyperparameter tuning for now, maybe implement this later\\n#\\n\\n#cv = StratifiedKFold(n_splits=k)\\n#print(cross_val_score(pipe, X_train, y_train, cv=cv))\\n\\n## optimize\\n#param_grid = {\\n#    \\\"estimator__C\\\": np.logspace(-4, 4, 10),\\n#}\\n\\n# make_pipeline(Imputer(),StandardScaler(),PCA(n_components=2),SVC(random_state=1))\\n\\n# cv = StratifiedKFold(n_splits=5)\\n#gs = GridSearchCV(\\n#    estimator=pipe,\\n#    param_grid=param_grid,\\n#    scoring=\\\"accuracy\\\",\\n#    cv=cv,\\n#    return_train_score=True,\\n#)\\n#gs.fit(X_train, y_train)\\n#\\n#print(\\\"Best Estimator: \\\\n{}\\\\n\\\".format(gs.best_estimator_))\\n#print(\\\"Best Parameters: \\\\n{}\\\\n\\\".format(gs.best_params_))\\n#print(\\\"Best Test Score: \\\\n{}\\\\n\\\".format(gs.best_score_))\\n#print(\\n#    \\\"Best Training Score: \\\\n{}\\\\n\\\".format(\\n#        gs.cv_results_[\\\"mean_train_score\\\"][gs.best_index_]\\n#    )\\n#)\\n#print(\\\"All Training Scores: \\\\n{}\\\\n\\\".format(gs.cv_results_[\\\"mean_train_score\\\"]))\\n#print(\\\"All Test Scores: \\\\n{}\\\\n\\\".format(gs.cv_results_[\\\"mean_test_score\\\"]))\\n# # This prints out all results during Cross-Validation in details\\n# print(\\\"All Meta Results During CV Search: \\\\n{}\\\\n\\\".format(gs.cv_results_))\\n\\n# Reset pipeline with best params\\n#pipe.set_params(estimator__C=gs.best_params_[\\\"estimator__C\\\"])\\n#pipe.fit(X_train, y_train)\\n#print(\\\"Test score with best params (should equal to Best Test Score above)\\\")\\n#print(pipe.score(X_test, y_test))\";\n                var nbb_formatted_code = \"# Define some initial params\\nk = 5\\ntest_size = 0.2\\n\\n#######################################\\n# Test Naive Bayes model\\n#######################################\\n\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.naive_bayes import ComplementNB\\n\\n# for col in toy_df.columns:\\n#    print(f\\\"*{col}*\\\")\\n\\n_ = test_model(\\n    GaussianNB(),\\n    StandardScaler(),\\n    toy_df,\\n    \\\"GAUSSIAN NAIVE BAYES\\\",\\n    \\\"TOY DATASET\\\",\\n    test_size,\\n)\\n\\n#\\n# Much worse results with complementNB\\n#\\n# test_model(\\n#\\n# ComplementNB(),\\n#    MinMaxScaler(),\\n#    toy_df,\\n#    \\\"COMPLEMENT NAIVE BAYES\\\",\\n#    \\\"TOY DATASET\\\",\\n#    test_size,\\n# )\\n#\\n\\n#\\n# Skip hyperparameter tuning for now, maybe implement this later\\n#\\n\\n# cv = StratifiedKFold(n_splits=k)\\n# print(cross_val_score(pipe, X_train, y_train, cv=cv))\\n\\n## optimize\\n# param_grid = {\\n#    \\\"estimator__C\\\": np.logspace(-4, 4, 10),\\n# }\\n\\n# make_pipeline(Imputer(),StandardScaler(),PCA(n_components=2),SVC(random_state=1))\\n\\n# cv = StratifiedKFold(n_splits=5)\\n# gs = GridSearchCV(\\n#    estimator=pipe,\\n#    param_grid=param_grid,\\n#    scoring=\\\"accuracy\\\",\\n#    cv=cv,\\n#    return_train_score=True,\\n# )\\n# gs.fit(X_train, y_train)\\n#\\n# print(\\\"Best Estimator: \\\\n{}\\\\n\\\".format(gs.best_estimator_))\\n# print(\\\"Best Parameters: \\\\n{}\\\\n\\\".format(gs.best_params_))\\n# print(\\\"Best Test Score: \\\\n{}\\\\n\\\".format(gs.best_score_))\\n# print(\\n#    \\\"Best Training Score: \\\\n{}\\\\n\\\".format(\\n#        gs.cv_results_[\\\"mean_train_score\\\"][gs.best_index_]\\n#    )\\n# )\\n# print(\\\"All Training Scores: \\\\n{}\\\\n\\\".format(gs.cv_results_[\\\"mean_train_score\\\"]))\\n# print(\\\"All Test Scores: \\\\n{}\\\\n\\\".format(gs.cv_results_[\\\"mean_test_score\\\"]))\\n# # This prints out all results during Cross-Validation in details\\n# print(\\\"All Meta Results During CV Search: \\\\n{}\\\\n\\\".format(gs.cv_results_))\\n\\n# Reset pipeline with best params\\n# pipe.set_params(estimator__C=gs.best_params_[\\\"estimator__C\\\"])\\n# pipe.fit(X_train, y_train)\\n# print(\\\"Test score with best params (should equal to Best Test Score above)\\\")\\n# print(pipe.score(X_test, y_test))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": "<IPython.core.display.Javascript object>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some initial params\n",
        "k = 5\n",
        "test_size = 0.2\n",
        "\n",
        "#######################################\n",
        "# Test SVC Classifier\n",
        "#######################################\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "_ = test_model(\n",
        "    SVC(probability=True),\n",
        "    StandardScaler(),\n",
        "    toy_df,\n",
        "    \"SVC CLASSIFIER\",\n",
        "    \"TOY DATASET\",\n",
        "    test_size,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n******************************************************************\n  Results for SVC CLASSIFIER with TOY DATASET:\n******************************************************************\nTraining error: 0.5625\nValidation error: 0.2\nLoss: 0.41106039063084643\n\nOriginal and predicted labels (first 30 rows):\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>095</th>\n      <th>095_PRED</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>788.830</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>788.330</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>770.921</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>357.000</td>\n      <td>357.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>788.332</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>788.410</td>\n      <td>788.41</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>928.100</td>\n      <td>928.10</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>788.330</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>226.000</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>427.000</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>788.400</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>333.400</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>785.620</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>140.000</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>439.000</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>520.000</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>783.610</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>788.330</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>657.000</td>\n      <td>657.00</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>613.100</td>\n      <td>613.10</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>182.600</td>\n      <td>182.60</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>618.240</td>\n      <td>618.24</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>788.330</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>788.410</td>\n      <td>788.41</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>846.100</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>990.000</td>\n      <td>990.00</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>691.110</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>493.000</td>\n      <td>493.00</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>929.110</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>384.000</td>\n      <td>384.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "        095  095_PRED\n0   788.830    788.33\n1   788.330    788.33\n2   770.921    788.33\n3   357.000    357.00\n4   788.332    788.33\n5   788.410    788.41\n6   928.100    928.10\n7   788.330    788.33\n8   226.000    788.33\n9   427.000    788.33\n10  788.400    788.33\n11  333.400    788.33\n12  785.620    788.33\n13  140.000    788.33\n14  439.000    788.33\n15  520.000    788.33\n16  783.610    788.33\n17  788.330    788.33\n18  657.000    657.00\n19  613.100    613.10\n20  182.600    182.60\n21  618.240    618.24\n22  788.330    788.33\n23  788.410    788.41\n24  846.100    788.33\n25  990.000    990.00\n26  691.110    788.33\n27  493.000    493.00\n28  929.110    788.33\n29  384.000    384.00"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of false predictions: 51\nNumber of right predictions: 49\n\n\nAll false predictions in dataset:\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>record_id</th>\n      <th>084</th>\n      <th>092</th>\n      <th>093</th>\n      <th>094</th>\n      <th>nan</th>\n      <th>näyttelijät</th>\n      <th>elokuvanäyttelijät</th>\n      <th>laulajat</th>\n      <th>iskelmät</th>\n      <th>...</th>\n      <th>resilienssi</th>\n      <th>selviytyminen</th>\n      <th>vaikeudet</th>\n      <th>itsekasvatus</th>\n      <th>elämänhallinta</th>\n      <th>itseluottamus</th>\n      <th>tunnetaidot</th>\n      <th>itsensä toteuttaminen</th>\n      <th>095</th>\n      <th>095_PRED</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>420909226072</td>\n      <td>78.89700</td>\n      <td>78.89170</td>\n      <td>78.8917</td>\n      <td>78.89170</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.830</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>420908537805</td>\n      <td>99.10000</td>\n      <td>77.19200</td>\n      <td>NaN</td>\n      <td>77.19200</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>770.921</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>420908893012</td>\n      <td>78.89112</td>\n      <td>78.89112</td>\n      <td>NaN</td>\n      <td>78.89112</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.332</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>420908060431</td>\n      <td>22.20000</td>\n      <td>22.20000</td>\n      <td>NaN</td>\n      <td>22.20000</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>226.000</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>420908985491</td>\n      <td>48.18400</td>\n      <td>48.18400</td>\n      <td>NaN</td>\n      <td>48.18400</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>427.000</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>420908840876</td>\n      <td>78.89300</td>\n      <td>78.89300</td>\n      <td>78.8930</td>\n      <td>78.89300</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.400</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>420908163172</td>\n      <td>33.36000</td>\n      <td>33.36000</td>\n      <td>NaN</td>\n      <td>33.36000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>333.400</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>420908426586</td>\n      <td>78.54000</td>\n      <td>78.55500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>785.620</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>420909110168</td>\n      <td>16.10000</td>\n      <td>16.10000</td>\n      <td>NaN</td>\n      <td>16.10000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>140.000</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>420908989548</td>\n      <td>48.29000</td>\n      <td>48.29000</td>\n      <td>NaN</td>\n      <td>48.29000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>439.000</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>420908232180</td>\n      <td>52.00000</td>\n      <td>52.00000</td>\n      <td>52.0000</td>\n      <td>52.00000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>520.000</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>420908423592</td>\n      <td>78.35100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>78.35100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>783.610</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>420908875333</td>\n      <td>86.83200</td>\n      <td>86.83200</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>846.100</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>420908761605</td>\n      <td>68.24000</td>\n      <td>68.24000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>691.110</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>420908955905</td>\n      <td>92.83300</td>\n      <td>NaN</td>\n      <td>92.8330</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>929.110</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>420909025020</td>\n      <td>42.33000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>412.110</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>420908217278</td>\n      <td>86.31000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>86.31000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>821.000</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>420907856984</td>\n      <td>2.32100</td>\n      <td>2.32100</td>\n      <td>2.3210</td>\n      <td>2.32100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>22.500</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>420908069830</td>\n      <td>32.21000</td>\n      <td>32.21000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>323.300</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>420909174735</td>\n      <td>59.57000</td>\n      <td>59.57000</td>\n      <td>59.5700</td>\n      <td>59.57000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>617.800</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>420908573024</td>\n      <td>77.49000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>777.981</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>420908686187</td>\n      <td>78.89220</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>78.89220</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.200</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>420908787488</td>\n      <td>67.45100</td>\n      <td>67.45100</td>\n      <td>67.4510</td>\n      <td>67.45100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>654.910</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>420908079988</td>\n      <td>23.00000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>230.000</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>420908475002</td>\n      <td>78.52100</td>\n      <td>NaN</td>\n      <td>78.5210</td>\n      <td>78.52100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>785.200</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>420908646967</td>\n      <td>78.89220</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.210</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>420907837759</td>\n      <td>78.89360</td>\n      <td>NaN</td>\n      <td>78.8946</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.480</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>420909064572</td>\n      <td>46.30000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>419.110</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>420908744475</td>\n      <td>68.25000</td>\n      <td>NaN</td>\n      <td>68.2500</td>\n      <td>68.25000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>691.140</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>420908655558</td>\n      <td>78.34300</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>78.34300</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>783.421</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>420909085349</td>\n      <td>59.24100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>613.300</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>420908822169</td>\n      <td>79.12000</td>\n      <td>79.12000</td>\n      <td>NaN</td>\n      <td>79.12000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>791.500</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>420909159579</td>\n      <td>32.30000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>375.000</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>420908922010</td>\n      <td>78.89141</td>\n      <td>78.89141</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.120</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>420908540522</td>\n      <td>78.89320</td>\n      <td>78.89320</td>\n      <td>78.8932</td>\n      <td>78.89320</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.410</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>420908830411</td>\n      <td>40.71000</td>\n      <td>40.71000</td>\n      <td>40.7100</td>\n      <td>40.71000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>461.000</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>420908642255</td>\n      <td>59.30000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>59.50000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>616.200</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>420908815520</td>\n      <td>78.89220</td>\n      <td>78.89223</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.211</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>420907909925</td>\n      <td>69.10000</td>\n      <td>69.10000</td>\n      <td>NaN</td>\n      <td>69.10000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>674.000</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>420908820707</td>\n      <td>78.34400</td>\n      <td>78.34400</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>783.421</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>420908792675</td>\n      <td>78.89120</td>\n      <td>78.89120</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.321</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>420908727542</td>\n      <td>78.61000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>78.61000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>786.110</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>420909175732</td>\n      <td>78.89112</td>\n      <td>78.89112</td>\n      <td>NaN</td>\n      <td>78.89112</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.332</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>420908241951</td>\n      <td>76.00000</td>\n      <td>76.00000</td>\n      <td>NaN</td>\n      <td>76.00000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>760.000</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>420909221824</td>\n      <td>89.94300</td>\n      <td>89.94300</td>\n      <td>NaN</td>\n      <td>89.94300</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>896.000</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>420908659765</td>\n      <td>78.89350</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.440</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>420908532696</td>\n      <td>55.40000</td>\n      <td>55.40000</td>\n      <td>55.4000</td>\n      <td>55.40000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>499.000</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>420907935317</td>\n      <td>78.61000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>786.111</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>420909137083</td>\n      <td>1.40000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10.600</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>420908988678</td>\n      <td>58.21000</td>\n      <td>58.21000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>597.610</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>420908797221</td>\n      <td>78.89110</td>\n      <td>78.89113</td>\n      <td>78.8911</td>\n      <td>78.89113</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.334</td>\n      <td>788.33</td>\n    </tr>\n  </tbody>\n</table>\n<p>51 rows × 488 columns</p>\n</div>",
            "text/plain": "       record_id       084       092      093       094  nan  näyttelijät  \\\n0   420909226072  78.89700  78.89170  78.8917  78.89170  1.0          NaN   \n2   420908537805  99.10000  77.19200      NaN  77.19200  NaN          1.0   \n4   420908893012  78.89112  78.89112      NaN  78.89112  NaN          NaN   \n8   420908060431  22.20000  22.20000      NaN  22.20000  1.0          NaN   \n9   420908985491  48.18400  48.18400      NaN  48.18400  1.0          NaN   \n10  420908840876  78.89300  78.89300  78.8930  78.89300  NaN          NaN   \n11  420908163172  33.36000  33.36000      NaN  33.36000  NaN          NaN   \n12  420908426586  78.54000  78.55500      NaN       NaN  NaN          NaN   \n13  420909110168  16.10000  16.10000      NaN  16.10000  NaN          NaN   \n14  420908989548  48.29000  48.29000      NaN  48.29000  NaN          NaN   \n15  420908232180  52.00000  52.00000  52.0000  52.00000  NaN          NaN   \n16  420908423592  78.35100       NaN      NaN  78.35100  NaN          NaN   \n24  420908875333  86.83200  86.83200      NaN       NaN  NaN          NaN   \n26  420908761605  68.24000  68.24000      NaN       NaN  NaN          NaN   \n28  420908955905  92.83300       NaN  92.8330       NaN  NaN          NaN   \n30  420909025020  42.33000       NaN      NaN       NaN  NaN          NaN   \n31  420908217278  86.31000       NaN      NaN  86.31000  NaN          NaN   \n32  420907856984   2.32100   2.32100   2.3210   2.32100  NaN          NaN   \n33  420908069830  32.21000  32.21000      NaN       NaN  NaN          NaN   \n35  420909174735  59.57000  59.57000  59.5700  59.57000  NaN          NaN   \n40  420908573024  77.49000       NaN      NaN       NaN  NaN          NaN   \n41  420908686187  78.89220       NaN      NaN  78.89220  NaN          NaN   \n43  420908787488  67.45100  67.45100  67.4510  67.45100  NaN          NaN   \n45  420908079988  23.00000       NaN      NaN       NaN  1.0          NaN   \n46  420908475002  78.52100       NaN  78.5210  78.52100  NaN          NaN   \n48  420908646967  78.89220       NaN      NaN       NaN  NaN          NaN   \n49  420907837759  78.89360       NaN  78.8946       NaN  NaN          NaN   \n51  420909064572  46.30000       NaN      NaN       NaN  1.0          NaN   \n52  420908744475  68.25000       NaN  68.2500  68.25000  NaN          NaN   \n53  420908655558  78.34300       NaN      NaN  78.34300  NaN          NaN   \n54  420909085349  59.24100       NaN      NaN       NaN  NaN          NaN   \n55  420908822169  79.12000  79.12000      NaN  79.12000  NaN          NaN   \n60  420909159579  32.30000       NaN      NaN       NaN  NaN          NaN   \n63  420908922010  78.89141  78.89141      NaN       NaN  NaN          NaN   \n66  420908540522  78.89320  78.89320  78.8932  78.89320  NaN          NaN   \n72  420908830411  40.71000  40.71000  40.7100  40.71000  NaN          NaN   \n73  420908642255  59.30000       NaN      NaN  59.50000  NaN          NaN   \n74  420908815520  78.89220  78.89223      NaN       NaN  NaN          NaN   \n75  420907909925  69.10000  69.10000      NaN  69.10000  NaN          NaN   \n77  420908820707  78.34400  78.34400      NaN       NaN  NaN          NaN   \n79  420908792675  78.89120  78.89120      NaN       NaN  NaN          NaN   \n82  420908727542  78.61000       NaN      NaN  78.61000  NaN          NaN   \n84  420909175732  78.89112  78.89112      NaN  78.89112  1.0          NaN   \n86  420908241951  76.00000  76.00000      NaN  76.00000  NaN          NaN   \n88  420909221824  89.94300  89.94300      NaN  89.94300  NaN          NaN   \n89  420908659765  78.89350       NaN      NaN       NaN  NaN          NaN   \n91  420908532696  55.40000  55.40000  55.4000  55.40000  NaN          NaN   \n92  420907935317  78.61000       NaN      NaN       NaN  NaN          NaN   \n95  420909137083   1.40000       NaN      NaN       NaN  NaN          NaN   \n96  420908988678  58.21000  58.21000      NaN       NaN  NaN          NaN   \n97  420908797221  78.89110  78.89113  78.8911  78.89113  NaN          NaN   \n\n    elokuvanäyttelijät  laulajat  iskelmät  ...  resilienssi  selviytyminen  \\\n0                  NaN       NaN       NaN  ...          NaN            NaN   \n2                  1.0       1.0       1.0  ...          NaN            NaN   \n4                  NaN       NaN       NaN  ...          NaN            NaN   \n8                  NaN       NaN       NaN  ...          NaN            NaN   \n9                  NaN       NaN       NaN  ...          NaN            NaN   \n10                 NaN       NaN       NaN  ...          NaN            NaN   \n11                 NaN       NaN       NaN  ...          NaN            NaN   \n12                 NaN       NaN       NaN  ...          NaN            NaN   \n13                 NaN       NaN       NaN  ...          NaN            NaN   \n14                 NaN       NaN       NaN  ...          NaN            NaN   \n15                 NaN       NaN       NaN  ...          NaN            NaN   \n16                 NaN       NaN       NaN  ...          NaN            NaN   \n24                 NaN       NaN       NaN  ...          NaN            NaN   \n26                 NaN       NaN       NaN  ...          NaN            NaN   \n28                 NaN       NaN       NaN  ...          NaN            NaN   \n30                 NaN       NaN       NaN  ...          NaN            NaN   \n31                 NaN       NaN       NaN  ...          NaN            NaN   \n32                 NaN       NaN       NaN  ...          NaN            NaN   \n33                 NaN       NaN       NaN  ...          NaN            NaN   \n35                 NaN       NaN       NaN  ...          NaN            NaN   \n40                 NaN       NaN       NaN  ...          NaN            NaN   \n41                 NaN       NaN       NaN  ...          NaN            NaN   \n43                 NaN       NaN       NaN  ...          NaN            NaN   \n45                 NaN       NaN       NaN  ...          NaN            NaN   \n46                 NaN       NaN       NaN  ...          NaN            NaN   \n48                 NaN       NaN       NaN  ...          NaN            NaN   \n49                 NaN       NaN       NaN  ...          NaN            NaN   \n51                 NaN       NaN       NaN  ...          NaN            NaN   \n52                 NaN       NaN       NaN  ...          NaN            NaN   \n53                 NaN       NaN       NaN  ...          NaN            NaN   \n54                 NaN       NaN       NaN  ...          NaN            NaN   \n55                 NaN       NaN       NaN  ...          NaN            NaN   \n60                 NaN       NaN       NaN  ...          NaN            NaN   \n63                 NaN       NaN       NaN  ...          NaN            NaN   \n66                 NaN       NaN       1.0  ...          NaN            NaN   \n72                 NaN       NaN       NaN  ...          NaN            NaN   \n73                 NaN       NaN       NaN  ...          NaN            NaN   \n74                 NaN       1.0       NaN  ...          NaN            NaN   \n75                 NaN       NaN       NaN  ...          NaN            NaN   \n77                 NaN       NaN       NaN  ...          NaN            NaN   \n79                 NaN       NaN       NaN  ...          NaN            NaN   \n82                 NaN       NaN       NaN  ...          NaN            NaN   \n84                 NaN       NaN       NaN  ...          NaN            NaN   \n86                 NaN       NaN       NaN  ...          NaN            NaN   \n88                 NaN       NaN       NaN  ...          NaN            NaN   \n89                 NaN       NaN       NaN  ...          NaN            NaN   \n91                 NaN       NaN       NaN  ...          NaN            NaN   \n92                 NaN       NaN       NaN  ...          NaN            NaN   \n95                 NaN       NaN       NaN  ...          NaN            NaN   \n96                 NaN       NaN       NaN  ...          NaN            NaN   \n97                 NaN       NaN       NaN  ...          NaN            NaN   \n\n    vaikeudet  itsekasvatus  elämänhallinta  itseluottamus  tunnetaidot  \\\n0         NaN           NaN             NaN            NaN          NaN   \n2         NaN           NaN             NaN            NaN          NaN   \n4         NaN           NaN             NaN            NaN          NaN   \n8         NaN           NaN             NaN            NaN          NaN   \n9         NaN           NaN             NaN            NaN          NaN   \n10        NaN           NaN             NaN            NaN          NaN   \n11        NaN           NaN             NaN            NaN          NaN   \n12        NaN           NaN             NaN            NaN          NaN   \n13        NaN           NaN             NaN            NaN          NaN   \n14        NaN           NaN             NaN            NaN          NaN   \n15        NaN           NaN             NaN            NaN          NaN   \n16        NaN           NaN             NaN            NaN          NaN   \n24        NaN           NaN             NaN            NaN          NaN   \n26        NaN           NaN             NaN            NaN          NaN   \n28        NaN           NaN             NaN            NaN          NaN   \n30        NaN           NaN             NaN            NaN          NaN   \n31        NaN           NaN             NaN            NaN          NaN   \n32        NaN           NaN             NaN            NaN          NaN   \n33        NaN           NaN             NaN            NaN          NaN   \n35        NaN           NaN             NaN            NaN          NaN   \n40        NaN           NaN             NaN            NaN          NaN   \n41        NaN           NaN             NaN            NaN          NaN   \n43        NaN           NaN             NaN            NaN          NaN   \n45        NaN           NaN             NaN            NaN          NaN   \n46        NaN           NaN             NaN            NaN          NaN   \n48        NaN           NaN             NaN            NaN          NaN   \n49        NaN           NaN             NaN            NaN          NaN   \n51        NaN           NaN             NaN            NaN          NaN   \n52        NaN           NaN             NaN            NaN          NaN   \n53        NaN           NaN             NaN            NaN          NaN   \n54        NaN           NaN             NaN            NaN          NaN   \n55        NaN           NaN             NaN            NaN          NaN   \n60        NaN           NaN             NaN            NaN          NaN   \n63        NaN           NaN             NaN            NaN          NaN   \n66        NaN           NaN             NaN            NaN          NaN   \n72        NaN           NaN             NaN            NaN          NaN   \n73        NaN           NaN             NaN            NaN          NaN   \n74        NaN           NaN             NaN            NaN          NaN   \n75        NaN           NaN             NaN            NaN          NaN   \n77        NaN           NaN             NaN            NaN          NaN   \n79        NaN           NaN             NaN            NaN          NaN   \n82        NaN           NaN             NaN            NaN          NaN   \n84        NaN           NaN             NaN            NaN          NaN   \n86        NaN           NaN             NaN            NaN          NaN   \n88        NaN           NaN             NaN            NaN          NaN   \n89        NaN           NaN             NaN            NaN          NaN   \n91        NaN           NaN             NaN            NaN          NaN   \n92        NaN           NaN             NaN            NaN          NaN   \n95        NaN           NaN             NaN            NaN          NaN   \n96        NaN           NaN             NaN            NaN          NaN   \n97        NaN           NaN             NaN            NaN          NaN   \n\n    itsensä toteuttaminen      095  095_PRED  \n0                     NaN  788.830    788.33  \n2                     NaN  770.921    788.33  \n4                     NaN  788.332    788.33  \n8                     NaN  226.000    788.33  \n9                     NaN  427.000    788.33  \n10                    NaN  788.400    788.33  \n11                    NaN  333.400    788.33  \n12                    NaN  785.620    788.33  \n13                    NaN  140.000    788.33  \n14                    NaN  439.000    788.33  \n15                    NaN  520.000    788.33  \n16                    NaN  783.610    788.33  \n24                    NaN  846.100    788.33  \n26                    NaN  691.110    788.33  \n28                    NaN  929.110    788.33  \n30                    NaN  412.110    788.33  \n31                    NaN  821.000    788.33  \n32                    NaN   22.500    788.33  \n33                    NaN  323.300    788.33  \n35                    NaN  617.800    788.33  \n40                    NaN  777.981    788.33  \n41                    NaN  788.200    788.33  \n43                    NaN  654.910    788.33  \n45                    NaN  230.000    788.33  \n46                    NaN  785.200    788.33  \n48                    NaN  788.210    788.33  \n49                    NaN  788.480    788.33  \n51                    NaN  419.110    788.33  \n52                    NaN  691.140    788.33  \n53                    NaN  783.421    788.33  \n54                    NaN  613.300    788.33  \n55                    NaN  791.500    788.33  \n60                    NaN  375.000    788.33  \n63                    NaN  788.120    788.33  \n66                    NaN  788.410    788.33  \n72                    NaN  461.000    788.33  \n73                    NaN  616.200    788.33  \n74                    NaN  788.211    788.33  \n75                    NaN  674.000    788.33  \n77                    NaN  783.421    788.33  \n79                    NaN  788.321    788.33  \n82                    NaN  786.110    788.33  \n84                    NaN  788.332    788.33  \n86                    NaN  760.000    788.33  \n88                    NaN  896.000    788.33  \n89                    NaN  788.440    788.33  \n91                    NaN  499.000    788.33  \n92                    NaN  786.111    788.33  \n95                    NaN   10.600    788.33  \n96                    NaN  597.610    788.33  \n97                    NaN  788.334    788.33  \n\n[51 rows x 488 columns]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nHow different classifications correlate with each other on true and false predictions:\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 55;\n                var nbb_unformatted_code = \"# Define some initial params\\nk = 5\\ntest_size = 0.2\\n\\n#######################################\\n# Test SVC Classifier\\n#######################################\\n\\nfrom sklearn.svm import SVC\\n\\n_ = test_model(\\n    SVC(probability=True),\\n    StandardScaler(),\\n    toy_df,\\n    \\\"SVC CLASSIFIER\\\",\\n    \\\"TOY DATASET\\\",\\n    test_size,\\n)\";\n                var nbb_formatted_code = \"# Define some initial params\\nk = 5\\ntest_size = 0.2\\n\\n#######################################\\n# Test SVC Classifier\\n#######################################\\n\\nfrom sklearn.svm import SVC\\n\\n_ = test_model(\\n    SVC(probability=True),\\n    StandardScaler(),\\n    toy_df,\\n    \\\"SVC CLASSIFIER\\\",\\n    \\\"TOY DATASET\\\",\\n    test_size,\\n)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": "<IPython.core.display.Javascript object>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some initial params\n",
        "k = 5\n",
        "test_size = 0.2\n",
        "\n",
        "#######################################\n",
        "# Test Decision Tree Classifier\n",
        "#######################################\n",
        "\n",
        "from sklearn import tree\n",
        "\n",
        "_ = test_model(\n",
        "    tree.DecisionTreeClassifier(max_depth=10),\n",
        "    StandardScaler(),\n",
        "    toy_df,\n",
        "    \"DECISION TREE CLASSIFIER\",\n",
        "    \"TOY DATASET\",\n",
        "    test_size,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n******************************************************************\n  Results for DECISION TREE CLASSIFIER with TOY DATASET:\n******************************************************************\nTraining error: 0.475\nValidation error: 0.0\nLoss: 0.2845592158845171\n\nOriginal and predicted labels (first 30 rows):\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>095</th>\n      <th>095_PRED</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>788.830</td>\n      <td>788.830</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>788.330</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>770.921</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>357.000</td>\n      <td>357.000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>788.332</td>\n      <td>788.332</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>788.410</td>\n      <td>788.410</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>928.100</td>\n      <td>928.100</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>788.330</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>226.000</td>\n      <td>22.500</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>427.000</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>788.400</td>\n      <td>788.400</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>333.400</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>785.620</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>140.000</td>\n      <td>22.500</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>439.000</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>520.000</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>783.610</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>788.330</td>\n      <td>788.330</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>657.000</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>613.100</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>182.600</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>618.240</td>\n      <td>618.240</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>788.330</td>\n      <td>788.334</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>788.410</td>\n      <td>788.410</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>846.100</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>990.000</td>\n      <td>990.000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>691.110</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>493.000</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>929.110</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>384.000</td>\n      <td>384.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "        095  095_PRED\n0   788.830   788.830\n1   788.330   182.600\n2   770.921   182.600\n3   357.000   357.000\n4   788.332   788.332\n5   788.410   788.410\n6   928.100   928.100\n7   788.330   182.600\n8   226.000    22.500\n9   427.000   182.600\n10  788.400   788.400\n11  333.400   182.600\n12  785.620   182.600\n13  140.000    22.500\n14  439.000   182.600\n15  520.000   182.600\n16  783.610   182.600\n17  788.330   788.330\n18  657.000   182.600\n19  613.100   182.600\n20  182.600   182.600\n21  618.240   618.240\n22  788.330   788.334\n23  788.410   788.410\n24  846.100   182.600\n25  990.000   990.000\n26  691.110   182.600\n27  493.000   182.600\n28  929.110   182.600\n29  384.000   384.000"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of false predictions: 62\nNumber of right predictions: 38\n\n\nAll false predictions in dataset:\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>record_id</th>\n      <th>084</th>\n      <th>092</th>\n      <th>093</th>\n      <th>094</th>\n      <th>nan</th>\n      <th>näyttelijät</th>\n      <th>elokuvanäyttelijät</th>\n      <th>laulajat</th>\n      <th>iskelmät</th>\n      <th>...</th>\n      <th>resilienssi</th>\n      <th>selviytyminen</th>\n      <th>vaikeudet</th>\n      <th>itsekasvatus</th>\n      <th>elämänhallinta</th>\n      <th>itseluottamus</th>\n      <th>tunnetaidot</th>\n      <th>itsensä toteuttaminen</th>\n      <th>095</th>\n      <th>095_PRED</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>420909204870</td>\n      <td>78.89113</td>\n      <td>78.8911</td>\n      <td>78.8911</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.330</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>420908537805</td>\n      <td>99.10000</td>\n      <td>77.1920</td>\n      <td>NaN</td>\n      <td>77.192</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>770.921</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>420909221339</td>\n      <td>78.89110</td>\n      <td>78.8911</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.330</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>420908060431</td>\n      <td>22.20000</td>\n      <td>22.2000</td>\n      <td>NaN</td>\n      <td>22.200</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>226.000</td>\n      <td>22.500</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>420908985491</td>\n      <td>48.18400</td>\n      <td>48.1840</td>\n      <td>NaN</td>\n      <td>48.184</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>427.000</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>420908733366</td>\n      <td>78.89000</td>\n      <td>78.8910</td>\n      <td>78.8910</td>\n      <td>78.891</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.330</td>\n      <td>788.334</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>420907829579</td>\n      <td>33.25000</td>\n      <td>36.5200</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>363.100</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>420909137083</td>\n      <td>1.40000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>10.600</td>\n      <td>230.000</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>420908988678</td>\n      <td>58.21000</td>\n      <td>58.2100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>597.610</td>\n      <td>182.600</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>420908104984</td>\n      <td>32.50000</td>\n      <td>32.5000</td>\n      <td>32.5000</td>\n      <td>32.500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>325.000</td>\n      <td>182.600</td>\n    </tr>\n  </tbody>\n</table>\n<p>62 rows × 488 columns</p>\n</div>",
            "text/plain": "       record_id       084      092      093     094  nan  näyttelijät  \\\n1   420909204870  78.89113  78.8911  78.8911     NaN  1.0          NaN   \n2   420908537805  99.10000  77.1920      NaN  77.192  NaN          1.0   \n7   420909221339  78.89110  78.8911      NaN     NaN  1.0          NaN   \n8   420908060431  22.20000  22.2000      NaN  22.200  1.0          NaN   \n9   420908985491  48.18400  48.1840      NaN  48.184  1.0          NaN   \n..           ...       ...      ...      ...     ...  ...          ...   \n93  420908733366  78.89000  78.8910  78.8910  78.891  NaN          NaN   \n94  420907829579  33.25000  36.5200      NaN     NaN  NaN          NaN   \n95  420909137083   1.40000      NaN      NaN     NaN  NaN          NaN   \n96  420908988678  58.21000  58.2100      NaN     NaN  NaN          NaN   \n98  420908104984  32.50000  32.5000  32.5000  32.500  NaN          NaN   \n\n    elokuvanäyttelijät  laulajat  iskelmät  ...  resilienssi  selviytyminen  \\\n1                  NaN       NaN       NaN  ...          NaN            NaN   \n2                  1.0       1.0       1.0  ...          NaN            NaN   \n7                  NaN       NaN       NaN  ...          NaN            NaN   \n8                  NaN       NaN       NaN  ...          NaN            NaN   \n9                  NaN       NaN       NaN  ...          NaN            NaN   \n..                 ...       ...       ...  ...          ...            ...   \n93                 NaN       NaN       1.0  ...          NaN            NaN   \n94                 NaN       NaN       NaN  ...          NaN            NaN   \n95                 NaN       NaN       NaN  ...          NaN            NaN   \n96                 NaN       NaN       NaN  ...          NaN            NaN   \n98                 NaN       NaN       NaN  ...          NaN            NaN   \n\n    vaikeudet  itsekasvatus  elämänhallinta  itseluottamus  tunnetaidot  \\\n1         NaN           NaN             NaN            NaN          NaN   \n2         NaN           NaN             NaN            NaN          NaN   \n7         NaN           NaN             NaN            NaN          NaN   \n8         NaN           NaN             NaN            NaN          NaN   \n9         NaN           NaN             NaN            NaN          NaN   \n..        ...           ...             ...            ...          ...   \n93        NaN           NaN             NaN            NaN          NaN   \n94        NaN           NaN             NaN            NaN          NaN   \n95        NaN           NaN             NaN            NaN          NaN   \n96        NaN           NaN             NaN            NaN          NaN   \n98        NaN           NaN             NaN            NaN          NaN   \n\n    itsensä toteuttaminen      095  095_PRED  \n1                     NaN  788.330   182.600  \n2                     NaN  770.921   182.600  \n7                     NaN  788.330   182.600  \n8                     NaN  226.000    22.500  \n9                     NaN  427.000   182.600  \n..                    ...      ...       ...  \n93                    NaN  788.330   788.334  \n94                    NaN  363.100   182.600  \n95                    NaN   10.600   230.000  \n96                    NaN  597.610   182.600  \n98                    NaN  325.000   182.600  \n\n[62 rows x 488 columns]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nHow different classifications correlate with each other on true and false predictions:\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 56;\n                var nbb_unformatted_code = \"# Define some initial params\\nk = 5\\ntest_size = 0.2\\n\\n#######################################\\n# Test Decision Tree Classifier\\n#######################################\\n\\nfrom sklearn import tree\\n\\n_ = test_model(\\n    tree.DecisionTreeClassifier(max_depth=10),\\n    StandardScaler(),\\n    toy_df,\\n    \\\"DECISION TREE CLASSIFIER\\\",\\n    \\\"TOY DATASET\\\",\\n    test_size,\\n)\";\n                var nbb_formatted_code = \"# Define some initial params\\nk = 5\\ntest_size = 0.2\\n\\n#######################################\\n# Test Decision Tree Classifier\\n#######################################\\n\\nfrom sklearn import tree\\n\\n_ = test_model(\\n    tree.DecisionTreeClassifier(max_depth=10),\\n    StandardScaler(),\\n    toy_df,\\n    \\\"DECISION TREE CLASSIFIER\\\",\\n    \\\"TOY DATASET\\\",\\n    test_size,\\n)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": "<IPython.core.display.Javascript object>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replace or remove this!!!\n",
        "Ok, so we can fit a model to the data and it appears to do something (with the small test data we can not necessarily say if it's meaningful).\n",
        "\n",
        "However, how would you take this script into production?\n",
        "How would you scale it, or use it with completely different setup of data and parameters?\n",
        "(well, this is a tiny example, so we could actually easily parameterize a script, but that's rarely the case in real world applications)\n",
        "\n",
        "This is why we need to construct a model class, to hold all of the steps required in separate, tidy functions.\n",
        "Then we can recreate the model and the steps with different data, without copy-pasting or manually editing all the tiny details.\n",
        "\n",
        "Follow along the example - you'll see, that half the work was done in the scripting cells above!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# HERE WE DO THE ACTUAL PREDICTING (We should probably refactor this to \"02_Loss.ipynb\")\n",
        "#\n",
        "\n",
        "# Define some initial params\n",
        "k = 5\n",
        "test_size = 0.2\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn import tree\n",
        "\n",
        "all_classes_df = pd.read_csv(all_classes_data_file, index_col=0)\n",
        "input_df = pd.read_csv(input_data_file, index_col=0)\n",
        "validation_df = input_df\n",
        "final_result = pd.DataFrame()\n",
        "\n",
        "ykl_classes = input_df[\"084\"].unique()\n",
        "\n",
        "for ykl_class in ykl_classes:\n",
        "    items_to_classify_df = input_df[input_df[\"084\"] == ykl_class]\n",
        "    items_to_classify_df, item_keywords_lst = keywords_to_features(\n",
        "        items_to_classify_df.copy()\n",
        "    )\n",
        "    items_to_classify_df = items_to_classify_df.drop([\"095\"], axis=1)\n",
        "\n",
        "    training_set_df = all_classes_df[all_classes_df[\"084\"] == ykl_class]\n",
        "    training_set_df, item_keywords_lst = keywords_to_features(\n",
        "        training_set_df.copy(), item_keywords_lst\n",
        "    )\n",
        "\n",
        "    # print(items_to_classify_df.shape)\n",
        "    # print(training_set_df.shape)\n",
        "\n",
        "    pipe = test_model(\n",
        "        tree.DecisionTreeClassifier(max_depth=10),\n",
        "        StandardScaler(),\n",
        "        training_set_df,\n",
        "        \"DECISION TREE\",\n",
        "        \"WHOLE TRAINING SET\" + str(len(training_set_df)),\n",
        "        test_size,\n",
        "        verbose=False,\n",
        "    )\n",
        "    result_df = predict_hkl_class(pipe, items_to_classify_df)\n",
        "    final_result = final_result.append(result_df)\n",
        "\n",
        "if validation_df is not None:\n",
        "    final_result[\"095_CORRECT\"] = np.nan\n",
        "    for rec_id in final_result[\"record_id\"].tolist():\n",
        "        right_hkl_class = float(\n",
        "            validation_df.loc[validation_df[\"record_id\"] == rec_id][\"095\"]\n",
        "        )\n",
        "        final_result.loc[\n",
        "            final_result.record_id == rec_id, \"095_CORRECT\"\n",
        "        ] = right_hkl_class\n",
        "\n",
        "    # display(final_result)\n",
        "    false_preds = final_result[final_result[\"095_PRED\"] != final_result[\"095_CORRECT\"]]\n",
        "    n_false_preds = len(false_preds)\n",
        "    n_right_preds = len(final_result) - n_false_preds\n",
        "\n",
        "    print(f\"Number of false predictions: {n_false_preds}\")\n",
        "    print(f\"Number of right predictions: {n_right_preds}\")\n",
        "    print(\"\\n\\nAll false predictions in dataset:\")\n",
        "    display(false_preds)\n",
        "\n",
        "final_result.head(100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of false predictions: 13\nNumber of right predictions: 7\n\n\nAll false predictions in dataset:\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>record_id</th>\n      <th>084</th>\n      <th>092</th>\n      <th>093</th>\n      <th>094</th>\n      <th>nan</th>\n      <th>näyttelijät</th>\n      <th>elokuvanäyttelijät</th>\n      <th>laulajat</th>\n      <th>iskelmät</th>\n      <th>...</th>\n      <th>resilienssi</th>\n      <th>selviytyminen</th>\n      <th>vaikeudet</th>\n      <th>itsekasvatus</th>\n      <th>elämänhallinta</th>\n      <th>itseluottamus</th>\n      <th>tunnetaidot</th>\n      <th>itsensä toteuttaminen</th>\n      <th>095_PRED</th>\n      <th>095_CORRECT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>420909209070</td>\n      <td>74.1200</td>\n      <td>74.1200</td>\n      <td>74.12</td>\n      <td>74.1200</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>749.20</td>\n      <td>749.20</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420909216149</td>\n      <td>38.5100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>38.5100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>182.60</td>\n      <td>886.00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420909261004</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.00</td>\n      <td>3.00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420909084554</td>\n      <td>33.2000</td>\n      <td>33.2000</td>\n      <td>NaN</td>\n      <td>33.2000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>332.20</td>\n      <td>333.40</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420909205264</td>\n      <td>14.1000</td>\n      <td>14.1000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>188.10</td>\n      <td>128.70</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420907911672</td>\n      <td>91.1700</td>\n      <td>91.1700</td>\n      <td>91.17</td>\n      <td>91.1700</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>919.10</td>\n      <td>917.00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420908955825</td>\n      <td>20.2000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>210.00</td>\n      <td>211.00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420909225957</td>\n      <td>3.0000</td>\n      <td>NaN</td>\n      <td>3.00</td>\n      <td>3.0000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>31.00</td>\n      <td>32.00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420909207104</td>\n      <td>99.1300</td>\n      <td>39.4000</td>\n      <td>39.40</td>\n      <td>39.4000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>324.00</td>\n      <td>326.40</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420907919299</td>\n      <td>50.1000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>508.60</td>\n      <td>508.62</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420908701357</td>\n      <td>59.5000</td>\n      <td>59.5000</td>\n      <td>NaN</td>\n      <td>59.5000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>617.10</td>\n      <td>617.00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420908625974</td>\n      <td>55.2000</td>\n      <td>NaN</td>\n      <td>55.20</td>\n      <td>55.2000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>498.00</td>\n      <td>492.00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420907914555</td>\n      <td>78.8911</td>\n      <td>78.8911</td>\n      <td>NaN</td>\n      <td>78.8911</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.33</td>\n      <td>788.33</td>\n    </tr>\n  </tbody>\n</table>\n<p>13 rows × 488 columns</p>\n</div>",
            "text/plain": "      record_id      084      092    093      094  nan  näyttelijät  \\\n0  420909209070  74.1200  74.1200  74.12  74.1200  NaN          NaN   \n0  420909216149  38.5100      NaN    NaN  38.5100  NaN          NaN   \n0  420909261004      NaN      NaN    NaN      NaN  NaN          NaN   \n0  420909084554  33.2000  33.2000    NaN  33.2000  NaN          NaN   \n0  420909205264  14.1000  14.1000    NaN      NaN  NaN          NaN   \n0  420907911672  91.1700  91.1700  91.17  91.1700  NaN          NaN   \n0  420908955825  20.2000      NaN    NaN      NaN  NaN          NaN   \n0  420909225957   3.0000      NaN   3.00   3.0000  NaN          NaN   \n0  420909207104  99.1300  39.4000  39.40  39.4000  NaN          NaN   \n0  420907919299  50.1000      NaN    NaN      NaN  NaN          NaN   \n0  420908701357  59.5000  59.5000    NaN  59.5000  NaN          NaN   \n0  420908625974  55.2000      NaN  55.20  55.2000  NaN          NaN   \n0  420907914555  78.8911  78.8911    NaN  78.8911  NaN          NaN   \n\n   elokuvanäyttelijät  laulajat  iskelmät  ...  resilienssi  selviytyminen  \\\n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n\n   vaikeudet  itsekasvatus  elämänhallinta  itseluottamus  tunnetaidot  \\\n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           1.0             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n\n   itsensä toteuttaminen  095_PRED  095_CORRECT  \n0                    NaN    749.20       749.20  \n0                    NaN    182.60       886.00  \n0                    NaN      0.00         3.00  \n0                    NaN    332.20       333.40  \n0                    NaN    188.10       128.70  \n0                    NaN    919.10       917.00  \n0                    NaN    210.00       211.00  \n0                    NaN     31.00        32.00  \n0                    NaN    324.00       326.40  \n0                    NaN    508.60       508.62  \n0                    NaN    617.10       617.00  \n0                    NaN    498.00       492.00  \n0                    NaN    788.33       788.33  \n\n[13 rows x 488 columns]"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": null,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>record_id</th>\n      <th>084</th>\n      <th>092</th>\n      <th>093</th>\n      <th>094</th>\n      <th>nan</th>\n      <th>näyttelijät</th>\n      <th>elokuvanäyttelijät</th>\n      <th>laulajat</th>\n      <th>iskelmät</th>\n      <th>...</th>\n      <th>resilienssi</th>\n      <th>selviytyminen</th>\n      <th>vaikeudet</th>\n      <th>itsekasvatus</th>\n      <th>elämänhallinta</th>\n      <th>itseluottamus</th>\n      <th>tunnetaidot</th>\n      <th>itsensä toteuttaminen</th>\n      <th>095_PRED</th>\n      <th>095_CORRECT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>420908782760</td>\n      <td>89.5000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>89.5000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>875.00</td>\n      <td>875.00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420909209070</td>\n      <td>74.1200</td>\n      <td>74.1200</td>\n      <td>74.12</td>\n      <td>74.1200</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>749.20</td>\n      <td>749.20</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420908623513</td>\n      <td>99.1000</td>\n      <td>99.1000</td>\n      <td>NaN</td>\n      <td>99.1000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>990.00</td>\n      <td>990.00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420909216149</td>\n      <td>38.5100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>38.5100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>182.60</td>\n      <td>886.00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420909261004</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.00</td>\n      <td>3.00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420909084554</td>\n      <td>33.2000</td>\n      <td>33.2000</td>\n      <td>NaN</td>\n      <td>33.2000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>332.20</td>\n      <td>333.40</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420909205264</td>\n      <td>14.1000</td>\n      <td>14.1000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>188.10</td>\n      <td>128.70</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420909076647</td>\n      <td>68.2000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>691.10</td>\n      <td>691.10</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420907911672</td>\n      <td>91.1700</td>\n      <td>91.1700</td>\n      <td>91.17</td>\n      <td>91.1700</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>919.10</td>\n      <td>917.00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420908955825</td>\n      <td>20.2000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>210.00</td>\n      <td>211.00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420909225957</td>\n      <td>3.0000</td>\n      <td>NaN</td>\n      <td>3.00</td>\n      <td>3.0000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>31.00</td>\n      <td>32.00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420909074522</td>\n      <td>42.0000</td>\n      <td>42.0000</td>\n      <td>NaN</td>\n      <td>42.0000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>412.00</td>\n      <td>412.00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420909207104</td>\n      <td>99.1300</td>\n      <td>39.4000</td>\n      <td>39.40</td>\n      <td>39.4000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>324.00</td>\n      <td>326.40</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420907919299</td>\n      <td>50.1000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>508.60</td>\n      <td>508.62</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420908701357</td>\n      <td>59.5000</td>\n      <td>59.5000</td>\n      <td>NaN</td>\n      <td>59.5000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>617.10</td>\n      <td>617.00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420908625974</td>\n      <td>55.2000</td>\n      <td>NaN</td>\n      <td>55.20</td>\n      <td>55.2000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>498.00</td>\n      <td>492.00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420907914555</td>\n      <td>78.8911</td>\n      <td>78.8911</td>\n      <td>NaN</td>\n      <td>78.8911</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>788.33</td>\n      <td>788.33</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420908990569</td>\n      <td>17.3000</td>\n      <td>NaN</td>\n      <td>17.30</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>188.10</td>\n      <td>188.10</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420909105853</td>\n      <td>51.0000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>510.00</td>\n      <td>510.00</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>420908636537</td>\n      <td>22.0000</td>\n      <td>22.0000</td>\n      <td>NaN</td>\n      <td>22.0000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>220.00</td>\n      <td>220.00</td>\n    </tr>\n  </tbody>\n</table>\n<p>20 rows × 488 columns</p>\n</div>",
            "text/plain": "      record_id      084      092    093      094  nan  näyttelijät  \\\n0  420908782760  89.5000      NaN    NaN  89.5000  NaN          NaN   \n0  420909209070  74.1200  74.1200  74.12  74.1200  NaN          NaN   \n0  420908623513  99.1000  99.1000    NaN  99.1000  NaN          NaN   \n0  420909216149  38.5100      NaN    NaN  38.5100  NaN          NaN   \n0  420909261004      NaN      NaN    NaN      NaN  NaN          NaN   \n0  420909084554  33.2000  33.2000    NaN  33.2000  NaN          NaN   \n0  420909205264  14.1000  14.1000    NaN      NaN  NaN          NaN   \n0  420909076647  68.2000      NaN    NaN      NaN  NaN          NaN   \n0  420907911672  91.1700  91.1700  91.17  91.1700  NaN          NaN   \n0  420908955825  20.2000      NaN    NaN      NaN  NaN          NaN   \n0  420909225957   3.0000      NaN   3.00   3.0000  NaN          NaN   \n0  420909074522  42.0000  42.0000    NaN  42.0000  NaN          NaN   \n0  420909207104  99.1300  39.4000  39.40  39.4000  NaN          NaN   \n0  420907919299  50.1000      NaN    NaN      NaN  NaN          NaN   \n0  420908701357  59.5000  59.5000    NaN  59.5000  NaN          NaN   \n0  420908625974  55.2000      NaN  55.20  55.2000  NaN          NaN   \n0  420907914555  78.8911  78.8911    NaN  78.8911  NaN          NaN   \n0  420908990569  17.3000      NaN  17.30      NaN  NaN          NaN   \n0  420909105853  51.0000      NaN    NaN      NaN  NaN          NaN   \n0  420908636537  22.0000  22.0000    NaN  22.0000  NaN          NaN   \n\n   elokuvanäyttelijät  laulajat  iskelmät  ...  resilienssi  selviytyminen  \\\n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n0                 NaN       NaN       NaN  ...          NaN            NaN   \n\n   vaikeudet  itsekasvatus  elämänhallinta  itseluottamus  tunnetaidot  \\\n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           1.0             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n0        NaN           NaN             NaN            NaN          NaN   \n\n   itsensä toteuttaminen  095_PRED  095_CORRECT  \n0                    NaN    875.00       875.00  \n0                    NaN    749.20       749.20  \n0                    NaN    990.00       990.00  \n0                    NaN    182.60       886.00  \n0                    NaN      0.00         3.00  \n0                    NaN    332.20       333.40  \n0                    NaN    188.10       128.70  \n0                    NaN    691.10       691.10  \n0                    NaN    919.10       917.00  \n0                    NaN    210.00       211.00  \n0                    NaN     31.00        32.00  \n0                    NaN    412.00       412.00  \n0                    NaN    324.00       326.40  \n0                    NaN    508.60       508.62  \n0                    NaN    617.10       617.00  \n0                    NaN    498.00       492.00  \n0                    NaN    788.33       788.33  \n0                    NaN    188.10       188.10  \n0                    NaN    510.00       510.00  \n0                    NaN    220.00       220.00  \n\n[20 rows x 488 columns]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 57;\n                var nbb_unformatted_code = \"#\\n# HERE WE DO THE ACTUAL PREDICTING (We should probably refactor this to \\\"02_Loss.ipynb\\\")\\n#\\n\\n# Define some initial params\\nk = 5\\ntest_size = 0.2\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.naive_bayes import ComplementNB\\nfrom sklearn import tree\\n\\nall_classes_df = pd.read_csv(all_classes_data_file, index_col=0)\\ninput_df = pd.read_csv(input_data_file, index_col=0)\\nvalidation_df = input_df\\nfinal_result = pd.DataFrame()\\n\\nykl_classes = input_df[\\\"084\\\"].unique()\\n\\nfor ykl_class in ykl_classes:\\n    items_to_classify_df = input_df[input_df[\\\"084\\\"] == ykl_class]\\n    items_to_classify_df, item_keywords_lst = keywords_to_features(\\n        items_to_classify_df.copy()\\n    )\\n    items_to_classify_df = items_to_classify_df.drop([\\\"095\\\"], axis=1)\\n\\n    training_set_df = all_classes_df[all_classes_df[\\\"084\\\"] == ykl_class]\\n    training_set_df, item_keywords_lst = keywords_to_features(\\n        training_set_df.copy(), item_keywords_lst\\n    )\\n\\n    # print(items_to_classify_df.shape)\\n    # print(training_set_df.shape)\\n\\n    pipe = test_model(\\n        tree.DecisionTreeClassifier(max_depth=10),\\n        StandardScaler(),\\n        training_set_df,\\n        \\\"DECISION TREE\\\",\\n        \\\"WHOLE TRAINING SET\\\" + str(len(training_set_df)),\\n        test_size,\\n        verbose=False,\\n    )\\n    result_df = predict_hkl_class(pipe, items_to_classify_df)\\n    final_result = final_result.append(result_df)\\n\\nif validation_df is not None:\\n    final_result[\\\"095_CORRECT\\\"] = np.nan\\n    for rec_id in final_result[\\\"record_id\\\"].tolist():\\n        right_hkl_class = float(\\n            validation_df.loc[validation_df[\\\"record_id\\\"] == rec_id][\\\"095\\\"]\\n        )\\n        final_result.loc[\\n            final_result.record_id == rec_id, \\\"095_CORRECT\\\"\\n        ] = right_hkl_class\\n\\n    # display(final_result)\\n    false_preds = final_result[final_result[\\\"095_PRED\\\"] != final_result[\\\"095_CORRECT\\\"]]\\n    n_false_preds = len(false_preds)\\n    n_right_preds = len(final_result) - n_false_preds\\n\\n    print(f\\\"Number of false predictions: {n_false_preds}\\\")\\n    print(f\\\"Number of right predictions: {n_right_preds}\\\")\\n    print(\\\"\\\\n\\\\nAll false predictions in dataset:\\\")\\n    display(false_preds)\\n\\nfinal_result.head(100)\";\n                var nbb_formatted_code = \"#\\n# HERE WE DO THE ACTUAL PREDICTING (We should probably refactor this to \\\"02_Loss.ipynb\\\")\\n#\\n\\n# Define some initial params\\nk = 5\\ntest_size = 0.2\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.naive_bayes import ComplementNB\\nfrom sklearn import tree\\n\\nall_classes_df = pd.read_csv(all_classes_data_file, index_col=0)\\ninput_df = pd.read_csv(input_data_file, index_col=0)\\nvalidation_df = input_df\\nfinal_result = pd.DataFrame()\\n\\nykl_classes = input_df[\\\"084\\\"].unique()\\n\\nfor ykl_class in ykl_classes:\\n    items_to_classify_df = input_df[input_df[\\\"084\\\"] == ykl_class]\\n    items_to_classify_df, item_keywords_lst = keywords_to_features(\\n        items_to_classify_df.copy()\\n    )\\n    items_to_classify_df = items_to_classify_df.drop([\\\"095\\\"], axis=1)\\n\\n    training_set_df = all_classes_df[all_classes_df[\\\"084\\\"] == ykl_class]\\n    training_set_df, item_keywords_lst = keywords_to_features(\\n        training_set_df.copy(), item_keywords_lst\\n    )\\n\\n    # print(items_to_classify_df.shape)\\n    # print(training_set_df.shape)\\n\\n    pipe = test_model(\\n        tree.DecisionTreeClassifier(max_depth=10),\\n        StandardScaler(),\\n        training_set_df,\\n        \\\"DECISION TREE\\\",\\n        \\\"WHOLE TRAINING SET\\\" + str(len(training_set_df)),\\n        test_size,\\n        verbose=False,\\n    )\\n    result_df = predict_hkl_class(pipe, items_to_classify_df)\\n    final_result = final_result.append(result_df)\\n\\nif validation_df is not None:\\n    final_result[\\\"095_CORRECT\\\"] = np.nan\\n    for rec_id in final_result[\\\"record_id\\\"].tolist():\\n        right_hkl_class = float(\\n            validation_df.loc[validation_df[\\\"record_id\\\"] == rec_id][\\\"095\\\"]\\n        )\\n        final_result.loc[\\n            final_result.record_id == rec_id, \\\"095_CORRECT\\\"\\n        ] = right_hkl_class\\n\\n    # display(final_result)\\n    false_preds = final_result[final_result[\\\"095_PRED\\\"] != final_result[\\\"095_CORRECT\\\"]]\\n    n_false_preds = len(false_preds)\\n    n_right_preds = len(final_result) - n_false_preds\\n\\n    print(f\\\"Number of false predictions: {n_false_preds}\\\")\\n    print(f\\\"Number of right predictions: {n_right_preds}\\\")\\n    print(\\\"\\\\n\\\\nAll false predictions in dataset:\\\")\\n    display(false_preds)\\n\\nfinal_result.head(100)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": "<IPython.core.display.Javascript object>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observations\n",
        "\n",
        "#### Gaussian Naive Bayes\n",
        " - We can't use float as a type in label (even the casting to type 'Category' didn't help)\n",
        "    - Workaround: multiply the HKLJ-CLASS with a big number and cast it to int\n",
        " - Sklearn libraries do not accept NaN values\n",
        "    - Workaround: replace NaN-values with 0\n",
        " - There are lots of rows where there is no info at all about other library classification\n",
        "    - Use keywords as additional info\n",
        "    - Omit the the rows where there is no class-information at all from other classification systems\n",
        "\n",
        "#### Complement Naive Bayes\n",
        " - This is an enhancement of Multinomial Naive Bayes\n",
        " - We can't use StandardScaler\n",
        "     - For unknown reason algorithm returns error \"negative values in input\"\n",
        "     - Workaround: use MinMaxScaler\n",
        " - Quick testing show much worse results than Gaussian Naive Bayes\n",
        " \n",
        " \n",
        "## NOTE: Sami's modifications end here"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define base class for your ML model\n",
        "\n",
        "You will probably do just fine creating a one simple class that does not inherit anything.\n",
        "You can then use this example base class as a template for your machine learning class. \n",
        "However, this is not always the case, and class inheritance is one of the most useful features of Python (and object oriented programming).\n",
        "This is why we wanted to demonstrate a base class - subclass division.\n",
        "\n",
        "Here we define the base class `MachineLearningModel` that holds some simple functions for handling data, that would be common for all subclasses.\n",
        "If a function only contains a `pass`-statement, it will be defined in the subclass.\n",
        "\n",
        "> **Note**: in this example the model instance contains the data. \n",
        "This is rarely applicable in practice if the data is large.\n",
        "Instead, in most applications the model should be routed to query the data when needed, in a similar way that it would appear as if the model instance contained the data.  "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# export\n",
        "\n",
        "# Define base class for our classifier\n",
        "class MachineLearningModel:\n",
        "    \"\"\"\n",
        "    Overly simplified example for a base class:\n",
        "\n",
        "    data handling operations\n",
        "\n",
        "    handle definitions of other functions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y, n_splits=5, seed=0):\n",
        "        self.n_splits = n_splits  # k-fold n_splits\n",
        "        self.seed = seed  # random state\n",
        "\n",
        "        self.set_data(X, y)  # init model data (see below)\n",
        "\n",
        "    def set_data(self, X, y):\n",
        "        \"\"\"\n",
        "        Set traing and evaluation data\n",
        "        \"\"\"\n",
        "        self.X = X.copy()\n",
        "        self.y = y.copy()\n",
        "\n",
        "        # in addition we separate train and test data:\n",
        "        self.__create_train_test_data()  # see below\n",
        "\n",
        "        return self\n",
        "\n",
        "    def get_data(self) -> (np.ndarray, np.ndarray):\n",
        "        \"\"\"\n",
        "        Get training and evaluation data\n",
        "        \"\"\"\n",
        "        return self.X.copy(), self.y.copy()\n",
        "\n",
        "    def __create_train_test_data(self, n_splits=None, seed=None):\n",
        "        \"\"\"\n",
        "        Create training and testing data\n",
        "        \"\"\"\n",
        "        # you might want to control the seed:\n",
        "        if seed is None:\n",
        "            seed = self.seed\n",
        "\n",
        "        # you might want to control the number of splits\n",
        "        if n_splits is None:\n",
        "            n_splits = self.n_splits\n",
        "\n",
        "        # split train and test data\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            self.X, self.y, test_size=(1 / n_splits), random_state=seed, stratify=self.y\n",
        "        )\n",
        "\n",
        "        return self\n",
        "\n",
        "    def get_train_test_data(self):\n",
        "        \"\"\"\n",
        "        Return X_train, X_test, y_train, y_test\n",
        "        \"\"\"\n",
        "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
        "\n",
        "    def fit(self, X=None, y=None, **fit_params):\n",
        "        \"\"\"\n",
        "        To be defined in the subclass\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        To be defined in the subclass\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def loss(self, X, y):\n",
        "        \"\"\"\n",
        "        To be defined in the subclass\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def get_train_loss(self):\n",
        "        \"\"\"\n",
        "        To be defined in the subclass\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def get_test_loss(self):\n",
        "        \"\"\"\n",
        "        To be defined in the subclass\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def optimize(self):\n",
        "        \"\"\"\n",
        "        To be defined in the subclass\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def get_params(self):\n",
        "        \"\"\"\n",
        "        To be defined in the subclass\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit test base class\n",
        "\n",
        "Purpose of unit testing is to cover all possible excecution paths in code.\n",
        "Unit testing helps developers to avoid and identify bugs in code.\n",
        "\n",
        "To unit test the class, we want to try and call every function of it and execute every different possible execution path.\n",
        "The proportion of possible paths covered is called the test coverage.\n",
        "100% coverage is rarely possible,\n",
        "but it would be good to try and test the functions with good and bad input,\n",
        "and with possible limit values (min, max, zero, None, np.nan, empty list etc.).\n",
        "\n",
        "However, a few simple tests are easier to maintain (and to actually get done in the first place) than exhaustive unit testing.\n",
        "Don't worry about coverage too much!\n",
        "A few well considered tests are better than having none.\n",
        "\n",
        "At the moment, nbdev considers all cells that do not have `# export` tag as tests.\n",
        "Unit tests can be defined with `assert` command - the nbdev git hooks run these commands when you push commits.\n",
        "Cells with `# slow` -tag will be omitted for time savings.\n",
        "\n",
        "The line after `assert` should have a `True` or non-zero value.\n",
        "`False`, 0 or None object will raise an `AssertionError`. Note that `np.nan` does not raise the error.\n",
        "\n",
        "This is handy because you can now keep all your tests in the same file (notebook) with the code and documentation.\n",
        "The downside is, however, that at the moment there is no good solution for monitoring test coverage of notebook developed code. \n",
        "If test coverage measuring is required, one option would be to implement tests with  `pytest` or `unittest` and export the tests to separate test.py file.\n",
        "\n",
        "Let's begin by introducing a couple of super simple unit test examples:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of a simple unit test\n",
        "this_statement_is_true = True\n",
        "assert this_statement_is_true\n",
        "\n",
        "# Another example of simple unit test of a function\n",
        "def return_three():\n",
        "    return 3\n",
        "\n",
        "\n",
        "# unit test return_tree\n",
        "assert return_three() == 3\n",
        "\n",
        "# Third example of simple unit testing of a simple class\n",
        "class SimpleClass:\n",
        "    \"\"\"\n",
        "    Simple class that stores an attribute and has a function to return it\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, parameter):\n",
        "        self.attribute = parameter\n",
        "\n",
        "    def get_attribute(self):\n",
        "        return self.attribute\n",
        "\n",
        "\n",
        "# unit thest init (should return class instance)\n",
        "assert SimpleClass(\"hello world!\")\n",
        "# unit test get_attribute (should return 'Hello world!')\n",
        "assert SimpleClass(\"Hello world!\").get_attribute() == \"Hello world!\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the assert commands run without an error, the tests pass.\n",
        "\n",
        "Now, let's include some tests with our example ML base class:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# test init\n",
        "assert MachineLearningModel(X, y)\n",
        "\n",
        "mlmodel = MachineLearningModel(X, y)\n",
        "\n",
        "# test get_data\n",
        "assert mlmodel.get_data()[0].shape == X.shape\n",
        "assert mlmodel.get_data()[1].iloc[1] == y.iloc[1]\n",
        "\n",
        "# test __create_test_train_data and get_train_test_data\n",
        "assert np.ceil(\n",
        "    10 * mlmodel.get_train_test_data()[-1].shape[0] / mlmodel.get_data()[1].shape[0]\n",
        ") == np.ceil(10 / mlmodel.n_splits)\n",
        "\n",
        "# test set_data (you should be able to change the model data completely)\n",
        "assert (\n",
        "    MachineLearningModel(X, y)  # create model as usual\n",
        "    .set_data(X.iloc[range(X.shape[0] - 1, -1, -1)], y)  # reset data in reverse order\n",
        "    .get_data()[0]  # get data\n",
        "    .iloc[0, 0]\n",
        ") == X.iloc[\n",
        "    -1, 0\n",
        "]  # first element is now reversed to last"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define subclasses & functions\n",
        "\n",
        "A subclass or child class inherits all attributes and functions of a parent class, but may also have additional functions defined.\n",
        "\n",
        "Here we define an example of a subclass of `MachineLearningModel`, the `LogisticRegressionModel` which performs logistic regression:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# export\n",
        "\n",
        "# Create subclass\n",
        "# now you see, that the subclass inherits data handling functions from the base class,\n",
        "# and we do not need to redefine them (although we could if we wanted to!\n",
        "\n",
        "\n",
        "class LogisticRegressionClassifier(MachineLearningModel):\n",
        "    \"\"\"\n",
        "    Logistic regression classifier\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y, n_splits=5, seed=0):\n",
        "\n",
        "        # we need to initialize the parent class with super.init:\n",
        "        super(LogisticRegressionClassifier, self).__init__(\n",
        "            X, y, n_splits=n_splits, seed=seed\n",
        "        )\n",
        "\n",
        "        # define preprocessing, algorithm and pipe\n",
        "        self.scaler = StandardScaler()\n",
        "        self.model = LogisticRegression()\n",
        "        self.pipe = Pipeline([(\"scaler\", self.scaler), (\"estimator\", self.model)])\n",
        "\n",
        "        # cross validation for optimization\n",
        "        self.cv = StratifiedKFold(n_splits=self.n_splits)\n",
        "\n",
        "        # param grid for optimization\n",
        "        self.param_grid = {\n",
        "            \"estimator__C\": np.linspace(0.3, 1.7, 10)  # logspace(-4, 4, 10),\n",
        "        }\n",
        "\n",
        "        # define optimization method for optimizing the model\n",
        "        self.optimization_pipe = GridSearchCV(\n",
        "            estimator=self.pipe,\n",
        "            param_grid=self.param_grid,\n",
        "            scoring=\"accuracy\",\n",
        "            cv=self.cv,\n",
        "            return_train_score=True,\n",
        "        )\n",
        "\n",
        "    def fit(self, X=None, y=None):\n",
        "        \"\"\"\n",
        "        Train and evaluate model\n",
        "        \"\"\"\n",
        "        if X is None or y is None:\n",
        "            self.pipe.fit(self.X_train, self.y_train)\n",
        "        else:  # reset data, recreate training and testing data and recursively call fit\n",
        "            self.set_data(X, y).fit()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Get predicted value at X\n",
        "        \"\"\"\n",
        "        return self.pipe.predict(X)\n",
        "\n",
        "    def loss(self, X, y):\n",
        "        \"\"\"\n",
        "        Return loss (model quality metric) [f1 score]\n",
        "\n",
        "        Note that this may be a different metric than the one that the model optimizer is using (scoring method).\n",
        "        For example for LogisticRegression the scoring method is mean accuracy,\n",
        "        but we want to track f1-score for loss because it is better balanced.\n",
        "        \"\"\"\n",
        "\n",
        "        return f1_score(y, self.predict(X))\n",
        "\n",
        "    def get_train_loss(self):\n",
        "        \"\"\"\n",
        "        Return loss for training data\n",
        "        \"\"\"\n",
        "        return self.loss(self.X_train, self.y_train)\n",
        "\n",
        "    def get_test_loss(self):\n",
        "        \"\"\"\n",
        "        Return loss for testing data\n",
        "        \"\"\"\n",
        "        return self.loss(self.X_test, self.y_test)\n",
        "\n",
        "    def optimize(self):\n",
        "        \"\"\"\n",
        "        Optimize model hyperparameters and fit the model with optimized parameters.\n",
        "\n",
        "        This example is with GridSearchCV, but more efficient algorithms can be implemented in practice.\n",
        "        \"\"\"\n",
        "        self.optimization_pipe.fit(self.X_train, self.y_train)\n",
        "        self.pipe.set_params(\n",
        "            estimator__C=self.optimization_pipe.best_params_[\"estimator__C\"]\n",
        "        )\n",
        "        self.fit()\n",
        "        return self\n",
        "\n",
        "    def get_params(self):\n",
        "        \"\"\"\n",
        "        Return parameters\n",
        "        \"\"\"\n",
        "        return self.pipe.get_params()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit test subclasses"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# test init\n",
        "assert LogisticRegressionClassifier(X, y)\n",
        "lgr_model = LogisticRegressionClassifier(X, y)\n",
        "\n",
        "# test fit\n",
        "try:  # fit should be called before predict or score\n",
        "    lgr_model.predict([1, 1, 1, 1])  # so this will cause an error\n",
        "except:  # but the except statement catches the error\n",
        "    pass  # yes, you can also test what should not work!\n",
        "\n",
        "# there are two ways we can call the fit function: with and without data\n",
        "assert lgr_model.fit()\n",
        "assert lgr_model.fit(X, y)\n",
        "\n",
        "# test predict\n",
        "assert lgr_model.predict(X[::1]).any()\n",
        "# test loss\n",
        "assert lgr_model.get_train_loss()\n",
        "assert lgr_model.get_test_loss()\n",
        "\n",
        "# test get_params\n",
        "assert lgr_model.get_params()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition, we might observe that our model is very slow (this example is not).\n",
        "\n",
        "Then, we could try to evaluate how much time each of the functions,\n",
        "and even the contents of the functions take to identify the bottlenecks.\n",
        "The theory of [order of functions](https://en.wikipedia.org/wiki/Big_O_notation) may also be useful.\n",
        "\n",
        "However, this is something you should only do in the late stages of your project.\n",
        "Remember, thinking time is what matters in data science!\n",
        "Begin with overoptimizing things, and you'll never have results.\n",
        "\n",
        "Anyway, you can easily time functions in notebooks with `%%timeit` [magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html):\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# slow\n",
        "%timeit -n 3 -r 4 LogisticRegressionClassifier(X, y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1.65 ms ± 141 µs per loop (mean ± std. dev. of 4 runs, 3 loops each)\n"
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize model behaviour with toy data\n",
        "\n",
        "Now, with the unit tests we can assume that our model does something right.\n",
        "At least any of the functions do not crash with expected input.\n",
        "\n",
        "As with the data preparation, the last step is to visualize the model performance.\n",
        "With small sample data, we may not see anything interestin,\n",
        "but sometimes already small number of datapoints can reveal interesting properties of the model when visualized.\n",
        "\n",
        "You can also define functions for visualizing the model performance, and export them to the model module,\n",
        "or include them directly as part of your machine learning model class if you see benefits from it.\n",
        "Either way, it's better to test them too with the toy data before the real deal!\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quite often we would like to see how a model performs when the number of data is increased.\n",
        "The two common questions are:\n",
        "\n",
        "1. How much data is needed that the model is accurate?\n",
        "\n",
        "2. How much data we can put in the model and still be able to run it with our resources?\n",
        "\n",
        "Then, we might have to balance between these two.\n",
        "\n",
        "\n",
        "\n",
        "So, for our example, let's loop through a range of data points, fit and time the model at each round"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import time  # library for checking time\n",
        "\n",
        "lgr = LogisticRegressionClassifier(X, y)\n",
        "\n",
        "# dataframe to save results\n",
        "eval_df = pd.DataFrame()\n",
        "\n",
        "for i in range(15, X.shape[0]):\n",
        "    lgr.fit(X.iloc[:i], y.iloc[:i])\n",
        "\n",
        "    begin = time.time()  # measure time before model is optimized\n",
        "    lgr.optimize()\n",
        "    end = time.time()  # measure time after optimization\n",
        "\n",
        "    ret = pd.DataFrame(\n",
        "        {\n",
        "            \"round\": [i - 10],\n",
        "            \"n_obs\": [lgr.get_data()[0].shape[0]],\n",
        "            \"train_loss\": [lgr.get_train_loss()],\n",
        "            \"test_loss\": [lgr.get_test_loss()],\n",
        "            \"optimized_C\": [lgr.get_params()[\"estimator__C\"]],\n",
        "            \"optimization_time\": end - begin,  # time spent in optimization\n",
        "        }\n",
        "    )\n",
        "    eval_df = pd.concat([eval_df, ret], axis=0, ignore_index=True)\n",
        "eval_df.set_index(\"round\", inplace=True)\n",
        "eval_df.head()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/templateenv/lib/python3.8/site-packages/sklearn/model_selection/_split.py:666: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n  warnings.warn((\"The least populated class in y has only %d\"\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": null,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_obs</th>\n      <th>train_loss</th>\n      <th>test_loss</th>\n      <th>optimized_C</th>\n      <th>optimization_time</th>\n    </tr>\n    <tr>\n      <th>round</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>15</td>\n      <td>0.400000</td>\n      <td>0.000000</td>\n      <td>0.300000</td>\n      <td>0.517263</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>16</td>\n      <td>0.750000</td>\n      <td>0.000000</td>\n      <td>0.300000</td>\n      <td>0.490386</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>17</td>\n      <td>0.888889</td>\n      <td>0.000000</td>\n      <td>0.300000</td>\n      <td>0.486138</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>18</td>\n      <td>0.800000</td>\n      <td>0.666667</td>\n      <td>0.455556</td>\n      <td>0.478212</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>19</td>\n      <td>0.727273</td>\n      <td>0.666667</td>\n      <td>0.300000</td>\n      <td>0.480910</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "       n_obs  train_loss  test_loss  optimized_C  optimization_time\nround                                                              \n5         15    0.400000   0.000000     0.300000           0.517263\n6         16    0.750000   0.000000     0.300000           0.490386\n7         17    0.888889   0.000000     0.300000           0.486138\n8         18    0.800000   0.666667     0.455556           0.478212\n9         19    0.727273   0.666667     0.300000           0.480910"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the stratification gives some warnings with such a small test set."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can also visualize the results. Super simple example:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# slow\n",
        "_ = eval_df.drop(\"n_obs\", axis=1).plot()\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABOCklEQVR4nO3dd3hUVfrA8e+ZtEkjCQmhJEAm9N47RBBdgbX3QhXX1VVE17LoLqLu/lZ0XbF3iiDYKIoKiotAQGrooSakkFBDCCG9zJzfHzcJSUjPTO6U83mePJm5c+feN5PknTPnnvccIaVEURRFcXwGvQNQFEVRrEMldEVRFCehErqiKIqTUAldURTFSaiEriiK4iTc9TpxSEiIjIiI0Ov0iqIoDmn37t0XpJQtqnpMt4QeERFBTEyMXqdXFEVxSEKI5OoeU10uiqIoTkIldEVRFCehErqiKIqT0K0PvSpFRUWkpqaSn5+vdyhKEzAajYSHh+Ph4aF3KIriFOwqoaempuLv709ERARCCL3DUWxISkl6ejqpqamYTCa9w1EUp1Brl4sQYoEQ4rwQIraax4UQ4h0hRLwQ4oAQon9Dg8nPzyc4OFglcxcghCA4OFh9GlMUK6pLH/oiYFwNj48HOpV8PQx82JiAVDJ3Hep3rSjWVWuXi5QyWggRUcMutwCLpTYP73YhRKAQorWU8oy1glQURWm0w9/D2So7Gppel3EQNsDqh7VGH3oYkFLufmrJtqsSuhDiYbRWPO3atbPCqRVFUepg71L4/i8ld+zgk6F/K7tN6HUmpfwE+ARg4MCBdreyxqVLl1i2bBl/+ctfat+5nAkTJrBs2TICAwPr9bypU6dy4403cuedd9breYqi1EPiZvhhJkSOhgeWg5vzjqqyxjj0U0DbcvfDS7Y5nEuXLvHBBx9ctb24uLjG561Zs6beyVxRlCZwIQ6+ngjBHeCuz506mYN1WuirgceFEF8BQ4BMa/Sfv/zDIQ6fvtzo4Mrr3qYZc27qUe3js2bN4sSJE/Tt2xcPDw+MRiNBQUEcPXqU48ePc+utt5KSkkJ+fj4zZ87k4YcfBq7MS5Odnc348eMZOXIkW7duJSwsjO+//x5vb+9aY1u/fj3PPPMMxcXFDBo0iA8//BAvLy9mzZrF6tWrcXd35w9/+ANvvPEG3377LS+//DJubm4EBAQQHR1ttddIUZxGTjosvQsM7nD/1+AdqHdENldrQhdCfAmMBkKEEKnAHMADQEr5EbAGmADEA7nANFsFa2tz584lNjaWffv2sXHjRv74xz8SGxtbNk56wYIFNG/enLy8PAYNGsQdd9xBcHBwhWPExcXx5Zdf8umnn3L33XezYsUKJk6cWON58/PzmTp1KuvXr6dz585MnjyZDz/8kEmTJrFq1SqOHj2KEIJLly4B8Morr/DLL78QFhZWtk1RlHKK8uGr+yHrDEz5EYIi9I6oSdRllMt9tTwugcesFlGJmlrSTWXw4MEVil7eeecdVq1aBUBKSgpxcXFXJXSTyUTfvn0BGDBgAElJSbWe59ixY5hMJjp37gzAlClTeP/993n88ccxGo1Mnz6dG2+8kRtvvBGAESNGMHXqVO6++25uv/12K/ykiuJEpITVj0PKdrhrEbQdpHdETUbN5VIDX1/fstsbN27kf//7H9u2bWP//v3069evyqIYLy+vsttubm619r/XxN3dnZ07d3LnnXfy448/Mm6cVg7w0Ucf8a9//YuUlBQGDBhAenp6g8+hKE5n46tw8FsY+yL0uE3vaJqUXZX+683f35+srKwqH8vMzCQoKAgfHx+OHj3K9u3brXbeLl26kJSURHx8PB07dmTJkiVcc801ZGdnk5uby4QJExgxYgSRkZEAnDhxgiFDhjBkyBDWrl1LSkrKVZ8UFMUl7f8KNr0G/SbCyL/qHU2TUwm9nODgYEaMGEHPnj3x9vamZcuWZY+NGzeOjz76iG7dutGlSxeGDh1qtfMajUYWLlzIXXfdVXZR9JFHHuHixYvccsst5OfnI6XkzTffBODZZ58lLi4OKSVjx46lT58+VotFURxW0u/w/eMQMQr+OA9csBJZaF3gTW/gwIGy8opFR44coVu3brrEo+hD/c4Vq0g/AZ+NBd8WMH0deAfpHZHNCCF2SykHVvWY6kNXFMWx5V6EpXeCMMD93zh1Mq+N6nJpAo899hi///57hW0zZ85k2jSHHeGpKPahuAC+egAyT8GUH6C5a0/FrBJ6E3j//ff1DkFRnI+UsHoGnNwKd8yHdkP0jkh3qstFURTHtOl1OPA1jPkH9FLzIYFK6IqiOKID38LGf0Of+yHqGb2jsRsqoSuK4liSt2lT4bYfCTe97ZLDE6ujErqiKI4j/YQ2R0tgO7hnCbh76h2RXVEJvZzqps+ti7feeovc3Nwa94mIiODChQsNOr6iuLzci7Dsbu32/d+AT3N947FDKqGXY+uErihKAxUXwjeT4dJJuHeZNr+5chX7Hba4dhacPWjdY7bqBePnVvtw+fnQr7/+ekJDQ/nmm28oKCjgtttu4+WXXyYnJ4e7776b1NRUzGYzs2fP5ty5c5w+fZoxY8YQEhLChg0bag3lzTffZMGCBQA89NBDPPnkk1Ue+5577qlyTnRFcRlSaisOJW2G2z+F9sP0jshu2W9C10H5+dDXrVvH8uXL2blzJ1JKbr75ZqKjo0lLS6NNmzb89NNPgDZpV0BAAG+++SYbNmwgJCSk1vPs3r2bhQsXsmPHDqSUDBkyhGuuuYaEhISrjp2enl7lnOiK4jI2vwH7l8Ho56H33XpHY9fsN6HX0JJuCuvWrWPdunX069cPgOzsbOLi4hg1ahRPP/00f/vb37jxxhsZNWpUvY+9ZcsWbrvttrLpeW+//XY2b97MuHHjrjp2cXFxlXOiA0gpycwrAiDA2wOhrvYrziZ2Bfz2L+h9D1zzN72jsXuqD70aUkqef/559u3bx759+4iPj2f69Ol07tyZPXv20KtXL/7xj3/wyiuvWO2cVR27ujnRLVJy6lIeJy/mcvJiLnHns7mcV4Rek60pitWd3AGrHoV2w+Hmd9XwxDpQCb2c8vOh33DDDSxYsIDs7GwATp06xfnz5zl9+jQ+Pj5MnDiRZ599lj179lz13NqMGjWK7777jtzcXHJycli1ahWjRo2q8tjZ2dlkZmYyYcIE5s2bx/79+ykyW0hIy+FiTiGh/l60a+6DlJKk9BxOpOWQnV9kmxdIUZrKxUT46j4ICIN7l4K7V+3PUey4y0UH5edDHz9+PPfffz/DhmkXYPz8/Pjiiy+Ij4/n2WefxWAw4OHhwYcffgjAww8/zLhx42jTpk2tF0X79+/P1KlTGTx4MKBdFO3Xrx+//PLLVcfOysqqMCf6q6//h/jz2ZgtknbNfQj00cbhNvP24FJuIecuF5BwIQc/L3daNjPi66V+xU1tw7HzLN2ezIs39qBdsI/e4TievAxteKK0wP3fquGJ9aDmQ3cgGbmFnMrIw90gaB/sg7fn1cnaYpFczC3k/OUCii0W/I0etGrmVeW+9sDZfudLtiUxZ/UhLBJC/b344qEhdG7pr3dYjqO4EJbeoVWDTv4eIkboHZHdUfOhOzgpJWcy80i5mIu3pxsdQ/2qTdAGgyDEz4surfxpFWAkt7CYuPPZJKfnkF9kbuLIXYfFIvnXj4eZ/f0hru0aynePaYno7o+3sT/lkr7BOQop4aenIDFa6zNXybze7LPZ5uCGDBlCQUFBhW1LliyhV69e9T5WsdnCyYu5ZBcUE+znResAI4Y6XBxyMwhC/Y009/XkQlYhF7ILuJyXRaCPJ6HNvPByd6t3LErV8grNPPn1Xn45dI6pwyOYfWN33AyC5Y8M54H523ngsx18NmUgQyPVuq812jIP9n4BUc9B3/v0jsYhqYRuAzt27LDKcfKLzCSn51BoloQFeRPsW/8LQ+4GA60CjIT4eZKWVUB6TiGXcoto7utBqL8RD3f1Ia0x0rIKeGhxDAdSL/Hijd15cOSVBRbaBfvw7Z+HM2n+DqYs2MmHE/tzbdeWNRzNhR1aBetfhp53wpgX9I7GYan/Zjt1Oa+IE+ezMVsgMsS3Qcm8PHc3A60DvenS0p/mvh5czCni2LksTl/Ko9hssVLUriXuXBa3ffA7x89m8fHEARWSealWAUa+/vMwOrf05+HFu1m9/7QOkdq5lF2w6hFoOxRueV8NT2wEldDtjJSSc5fzSUrPwdPdQMdQP6uOVPFwNxAW5EOXVn4EeHuQnl3A0bNZnM3Mp9iiEntdbY2/wO0fbiW/yMLXfx7KH3q0qnbf5r6eLPvTEPq3D2LmV3tZtuNkE0Zq5zKSteGJ/q204YkeRr0jcmgqodsRs0Vy8mIu5y7nE+TjSYcWfnjaqEvE092Nts196NTSH3+jO+ez8jl2Novzl/MxW1RxUk2W705l8oKdtA4w8t1jw+kdHljrc/yNHix+cDCjO7fghVUH+XjTCdsHau/yLmnDE82F2vBE39qnzVBqphK6nSgsNnMiTav2bB3gTXiQNwaD7T96Gj3caB/sS6dQP3w93Tl7WUvsF7IKsKjEXoGUkjfXHeOZb/czNDKY5Y8OJzyo7uPMjR5ufDxpIDf2bs2ra4/yxi/HXLey11wE306B9Hi4ewm06Kx3RE3iRFo2L/9wiCNnLtvk+CqhN1LlaXMnTJhQrwm0Vq9ezcv//D/iz2dTZLYQEeJLC3+ves/LMnr0aCqP6y8vOzubP//5z3To0IEBAwYwevToChdvvT3diQjxpUMLP4weBk5n5nHsXBbpOQVYXDXplFNQbOapr/fxzm/x3D0wnIXTBtHM6FHv43i6G3j73n7cO6gt722I56XVh1zvjVNK+OlpSNiorTgUeY3eEdlUsdnCL4fOMvGzHYz97ya+2J5ss6GsapRLI7311ltMnDgRHx+tpbZmzZo6P1dKyfBrb8DU/xrcDAYign3w8rDNcMKHHnoIk8lEXFwcBoOBxMREDh8+fNV+vl7uRLbwIzu/iLOXCziVkceFrAJCmxkJdNEJwC7lFvLwkt3sTLzIszd04S+jOzTqdXAzCF69vRf+Rnc+3ZxIVn4xr9/ZG3c3F2lfbX0H9nwOI/8K/SbqHY3NXMgu4OtdKSzdnszpzHxaBxh5+vrO3DO4LaH+trlWYLcJ/bWdr3H04lGrHrNr8678bXDtM7ZVnqv81ltvZdy4cQwYMIA9e/bQo0cPFi9ezGeffXbVPOgRERHExMSQnZ3NuHHjGDp0KFu3bmXQoEFMmzaNOXPmcP78eZZ88QXhnXqxcNEi4g7tZ+GnHzKgf/+yGI4dO8bPP//MwIEDmTFjBrGxsRQVFfHSSy9xyy23kJeXx7Rp09i/fz9du3YlLy+v2p/nxIkT7Nixg6VLl2IwaEnDZDJhMl09KqOUn9GDDl7uZOUXc/ZyPikXc0nzcKNlMyPNjO4uk9iT03OYtnAXqRl5vH1vX27pG2aV4woheGFCN5oZPfjvr8fJLijm3fv7OX99wOHV8Osc6H4rXDtb72isTkrJnpOXWLItiTUHz1JotjC8QzAv3tSd67q1tPmbtt0mdL1UN1f5sWPHmD9/PiNGjODBBx/kgw8+4JlnnqlxHvT4+Hi+/fZbFixYwKBBg1i2bBlbtmxh5XffMfulf/LfT7+gmdEDf6M7bgYD+/btA+CHH37g9ddfZ/jw4cyZM4drr72WBQsWcOnSJQYPHsx1113Hxx9/jI+PD0eOHOHAgQP0L/dmUNmhQ4fo27cvbm71SxZCCJp5a/Fl5hVx7nIByek5eHu60aqZET8v507su5Mz+NPiGCxSsvRPQxgUYd05RYQQzBjbCT+jOy//cJjpi2L4eNIA551/59RuWPkwhA+E2z4Cg/N8IskrNPP9vlMs2Z7ModOX8fNy577BbZk0rD0dQ5tu6oc6/eUIIcYBbwNuwGdSyrmVHm8HfA4EluwzS0pZ976HKtSlJW0L1c1V3rZtW0aM0EqRJ06cyDvvvMMzzzxT47FMJlNZdWiPHj0YO3YseUVm/FtHknIymfbNfQjwqdiNERcXx7PPPsuGDRvw8PBg3bp1rF69umyVovz8fE6ePEl0dDRPPPEEAL1796Z3795Wfy1KCSEI9PEkwNuDjNwizl/OJ/FCDr6e7rQKcM4JwH46cIanvtlHmwAjC6cNxhTia7NzTRthwt/owXPL9zNp/g4WTh1MgE/9++ft2qWTsOxe8GsB934JHt56R2QVSRdyWLI9mW9jUricX0yXlv7869ae3NYvTJf/i1rPKIRwA94HrgdSgV1CiNVSyvIdsP8AvpFSfiiE6A6sASJsEK9uKrdE69Iy9fK6UgxkMBgokm6cSMvBzWDADUmAT8UVy7Ozs7n77rv59NNPad26NaB9hFuxYgVdunRpcOw9evRg//79mM3merfSyxNC0NzXk0AfDy7mFHI+q4ATadn4Gz1o2cwLHzudAKw+pJR8HJ3A3LVHGdg+iE8mD6S5r+1Xlr9zQDh+Xm7M+HIv93yyjSXTh9DC30mmjM3PhGX3QHEBTPlBS+oOzGyRbDh6nsXbk4k+noa7QXBDz1ZMHtqewabmun5qrctnnsFAvJQyQUpZCHwF3FJpHwk0K7kdADhsOVx1c5WfPHmSbdu2AbBs2TJGjhwJ1G0edCkluYVm0nMK8PXUhglW9Tt/8MEHmTZtWoVVkG644QbefffdsuFte/fuBSAqKoply5YBEBsby4EDB6o9f4cOHRg4cCBz5swpO05SUlLZUnf1ZRDaBGBdW/rTumQCsPjz2SSkOfYiG0VmCy+simXu2qPc1KcNXzw0pEmSealxPVszf8ogktNzufvjbZy6VP11EYfy/eNw4Tjc/TmEdtU7mgZLzy7gg43xRL2+gYcWx3Ds7GWeuq4zW2ddy/v392dIZLDuXZB1SehhQEq5+6kl28p7CZgohEhFa53PqOpAQoiHhRAxQoiYtLS0BoRre+XnKh8yZAgPPfQQQUFBdOnShffff59u3bqRkZHBo48+ClyZB33MmDFVHq/YbCHxQg4FRWb8jR5EhPhWeWEkOTmZ5cuXs2DBAvr27Uvfvn2JiYlh9uzZFBUV0bt3b3r06MHs2dqFpEcffZTs7Gy6devGiy++yIABA2r8uT777DPOnTtHx44d6dmzJ1OnTiU0NLRRr5XBIGjhb6RrK39aNTNSUGwhKT2HY+eySMsqcKjK06z8IqZ/HsOXO0/y2JgOvH1PX4w2GnFUk6jOLVgyfTAXsgu468OtnEjLbvIYrCphIxxZDaNnQYeq/0fsmZSSvScz+OvX+xg29zde//kYbZt788ED/dnyt2uZeV0nQpvZT3VrrfOhCyHuBMZJKR8quT8JGCKlfLzcPn8tOdZ/hRDDgPlATylltf/RjjQfelJSEjfeeCOxsbH1el5+kZmk9ByKzJKwQO8mbe3pwSIll/OKSM8uJKewGIMQBPp4EOLnVW1ytIff+elLeTy4aBfx57P5v9t6cs+gdrrGA3DodCaT5+8EYPH0wfRoE6BzRA1gMcNHo6AwCx7b5VBl/flFZlbvP82SbckcPJWJr6cbt/cPZ9Kw9rrPb1/TfOh16fQ8BbQtdz+8ZFt504FxAFLKbUIIIxACnK9/uM4hM6+IlIu5GAyCyBBfp7xwWJmh5OJpoI8neYXFpGdrMztezCnE18udED9Pmhntayx77KlMHly0i7xCM4umDWZkJ/soP+/RJoBvHhnGxM92cO8n21k0bRAD2jvYyj17FsP5Q3DX5w6TzE+m5/LFjmS+iUnhUm4RHUP9eOWWHtzWLwz/BhSSNbW6ZJldQCchhAktkd8L3F9pn5PAWGCREKIbYATss0+lASIiIurcOpdScj6rgHOX8/HxdKd9c58mnaLWmnOxN4a3pzvhzd1pZbZwMbeQi9mFJKfn4ulmoLmfJ819PHUvpFl/5BwzvtxLkI8nSx4dQpdW9rWyUIcWfnz7yDAmzd/JxM928snkAYzq5CAXFPMz4bd/URQ+lCUXe3NuzRFa+HvRwt+LED/tews/LwJ99H+Dt1gkm46nsXhbEhuPp2EQght6tGTS0AiGRup7kbO+ak3oUspiIcTjwC9oQxIXSCkPCSFeAWKklKuBp4FPhRBPoV0gnSod9cpYI5gtktSMXDLzigjy8SQssGnmYynPWnOxW4u7m4FQfyMt/Ly4nF9MenYBZzPzOXe5gEBvDwqL9elnX7wtiZdWH6JHmwDmTxloV/2g5YUH+fDNn4cxaf4Opi+K4Z37+jGuZ/UzO9qDgmIzqStfxpSbzp2ZT7E//giebgYKq5im2b1kha0ryd6zLNm38Ddeue/vZfW6h4ycQr6JSeGLHcmkXMyjhb8XM67txP2D29EqwD7/Hmqj1hS1koJiM8npuRQUmWkV4E2In6dDvbM3pfwiM+nZBWTkFnEm+QQf7c9nyvAIxvdshYeNW+1mi+Tfa44wf0si13UL5Z37+jnEcMvM3CKmLtrJ/pRLvH5nH+4cEK53SBVIKdmXcokVe1LZt28PK+VT/OJ2DbGD/s2d/cPpGOpHVkExaVkFZV8Xsq/cTsu+cv9CdmGVM356uRvKknsLPy9CyhL/lZZ/aMl3b8/qL2gfSL3E4m3J/LD/NAXFFgZHNGfSsPbc0KOVzWY3tabG9qErtcjKL+LkRW2CrogQX4foa9OT0cONsCAfWgZYyD3vwYXsTJ74ci+h/l48MKQ99w2xzVwXeYVmZn61l3WHKy4V5wgCfDz4YvoQHl4SwzPf7ic7v4ipI6qfuqGpnMnMY+WeU6zck8qJtBy83A18E/gtbvleTJjxATcFtC7bt5nRg2ZGDzq08KvxmBaLJCO3kAvZhSXJPr8s0Ze+ASSn57I7OYP0nMIqj+Hn5V6upa+1/AN9PNl4PI39KZfw8XTjjgHhTB7Wnq6tmlV5DEekEnojSCm5kF3I2cw8vDzcaB/s4/xzcViRu8GAv9GdDU+PZtPxNBZtTWLe/47z3oY4JvRqzZThEfRrG2iVTzrns/L50+cxHDiVyZybujPNDpJhffl6uTN/yiBmfLmXl344TFZ+MY9f27HJPwnmFZr5+dAZVuw+xe8nLiAlDIoI4k+jIrkpIB7fLzdr87SUS+b1YTAIgv28CC5Z7LwmRWYLF3MKy1r5VX0COHL2MheyCricX0xkC19euqk7tw8Ib9BsmfZOJfQGslgkpy7lkZFbSDOjB22b+zhMa8/eGAyCMV1DGdM1lIS0bBZvS2b57lS+33ea3uEBTBkWwY19Wjf4zTLuXBZTF+7iYk4hn0wayPXdHXddT6OHGx8+0J/nlh/gv78e53J+ES9M6GbzpG6xSHYmXWTF7lTWHDxDTqGZ8CBvZlzbiTv6h9E+2Fcbpvjx/RDQDoY9ZtN4Snm4GWjZzEjLOlwDKSg24+lmcOquUPvvMLJTpcn8+y8+pYU3Zcm8IfOhz507t/YdK9m3b1+FqXobepzqJCUllVWiAsTExJTNHWNLkS38eOnmHmx/YSz/vKUHuYVmnv52P8Nf/Y03fjnGmcz6VU/+XrJUXKHZwjd/HubQybyUu5uBN+7qw+Rh7fl0cyLPrzxos1WmktNzePPX41zzxgbu/WQ7aw6eYUKv1nz18FCinx3DX6/vrCVzgL1fwLmDcP1LdjlXi5e7m1Mnc1AXRRukoMjM8XNZhPh7MaxPN2JiYqqcbdGWFi1aRExMDO+9955Njr9x40beeOMNfvzxR5scv1Rtv3MpJb/Hp7NoaxLrj54rG1I2ZVhErfNmfBuTwvMrDxLZwpcFUwfVa3UhRyCl5I11x3h/wwlu7N2aN+/ua5WLepfzi1hz4Awr9qSyKykDIWBEhxDuGBDGDT1aVX0ROf8yvNsfmkfCg7+ohZ5tyCEvip79978pOGLd+dC9unWl1Qsv1LpfbfOhmzp15eU3P+Cr5Z83eD70pUuXMnjw4AqJuW/fvmUxlM6H7u3tzcyZM8nPz8fb25uFCxdiMpl48cUXycvLY8uWLTz//PPk5eWVHScpKYkHH3yQCxcu0KJFCxYuXEi7du2YOnUqzZo1IyYmhrNnz/L6669z5513VvkazJo1iyNHjtC3b1+mTJlCv379yhL8Sy+9RGJiIgkJCZw8eZJ58+axfft21q5dS1hYGD/88AMeHh7s3r2bv/71r2RnZxMSEsKiRYvKJh2rKyEEIzuFMLJTCCkXc/liezJf7UphzcGzdG3lz9ThEdzSN6zCqAYpJW/+epx3f4tnZMcQPpjY3yn7S4UQPHtDV/yNHsxde5ScgmI+eGBAjSM8qmO2SLbEX2DF7lR+OXSWgmILkS18efaGLtzWL4w2gbW0uDf/F3LS4P6vVTLXkepyqaT8fOjbt2/n008/JSMjg2PHjvGXv/yFfQdj8fL25YcvF/HUkzNp06YNGzZsYMOGDVcdKz4+nqeffpqjR49y9OjRsvnQ33jjDf79739ftf++ffvYt28f//znPxk4cCDDhw+na9eubN68mb179/LKK6/wwgsv4OnpySuvvMI999zDvn37uOeeeyocZ8aMGUyZMoUDBw7wwAMPVOgqOXPmDFu2bOHHH39k1qxZ1b4Oc+fOZdSoUezbt4+nnnrqqsdPnDjBb7/9xurVq5k4cSJjxozh4MGDeHt789NPP1FUVMSMGTNYvnw5u3fv5sEHH+Tvf/97fX4VV2nb3IfnJ3Rj+/NjmXu7Vig1a+VBhr66nn+vOULKxVwKis08+fU+3v0tnnsGtm3wUnGO5JFrOvB/t/Vk4/E0pizcSVZ+UZ2fG3cui1fXHmH43PVMWbCTTcfTuGtgOKv+Mpz1f72Gx8Z0rD2ZX0yE7R9An/sgrOY5hRTbstsWel1a0rZQ23zoqRm5/PGOu1m9dH6tx6pqPnQhBL169SIpKanK51SeD/3s2bNMmTKFuLg4hBAUFdX+z7pt2zZWrlwJwKRJk3juuefKHrv11lsxGAx0796dc+fO1Xqs6owfPx4PDw969eqF2Wxm3LhxAGU/27Fjx4iNjeX6668HwGw217t1Xh1vTzfuHdyOewa1ZWfiRRZvS2b+lkQ+3ZxAmwBvTl3Ks8pScY7kgSHt8fNy5+lv9nP/pzv4/MHB1c4dlJFTyOr9p1mxJ5UDqZm4GQSjO7dgzk3hjO0WWv+Lz7++CAZ3GPuiFX4SpTHsNqHbGyEEhcUWMnKLaGb0wK0Oq61Ung+99L7BYKC4uPiq/auaD3327NmMGTOGVatWkZSUxOjRoxv1c5SPqTHXT8r/LB4eV8q3S382KSU9evQom3LYFoQQDIkMZkhkMGcy81i6/SS/HDrLO/f14+Y+bWx2Xnt1S98w/LzceXTpHu75WJtTvbTischsYcPR86zYk8pvR89TZJZ0a92Mf/yxG7f0DWv43OtJW7TZFMf8HZq53mtub1SXSyU1zYf+8/poAH7+bnm95kOvq6rmQ8/MzCQsTJuteNGiRWXbazrv8OHD+eqrrwBYunRphePVVWN/ri5dupCWllaW0IuKijh06FCDj1eb1gHePHNDF3796zUumcxLje3Wks+nDeb0pTzu+ngrG46d56XVhxjy7/U8vGQ3u5MzmDwsgjVPjGLtzFE8NCqy4cncYoafn4dm4TDs8dr3V2xOJfRKqpsPvXOXLnzy8YfcPmYImZmX6jwfel1VNx/6c889x/PPP0+/fv0qtOrHjBnD4cOH6du3L19//XWFY7377rssXLiQ3r17s2TJEt5+++16x9O7d2/c3Nzo06cP8+bNq/fzPT09Wb58OX/729/o06cPffv2ZevWrfU+jlJ/wzoEs/RPQ8nKL2bawl0s23GSoZHNmT9lINueH8vsG7vTvY0VqiP3LYOzB+D6l8HTuUYQOSo1bLEOkpKSGDf+j3z761Y6t/JT1aBWZK+/c2eQkJZNTFIGf+jRkkAfK8/FX5AF7/SHoPYw/Vc1sqUJOeSwRXtSZLZglpJAHw+VzBWHEdnCj8ha5k1psM1vQs55uO9LlcztiErodeAX0pqV/9tKqLMs2lvOwYMHmTRpUoVtXl5edjcNr2JHMpJg2/vQ+x4Ir7KhqOjE7hK6lNKuhpoVmy2kZxcS4O2Jlw5rTNpar1692Ldvny7ndsEp853Dr3NAGGDsHL0jUSqxq4uiRqOR9PR0u/pHv5BdiEVKQps5X+tcT1JK0tPTMRodcyEBl5W8FQ5/ByOfhIDKa8UrerOrFnp4eDipqamkpdnH6nUWKTmbmY+XhxuJWc69wLMejEYj4eH2tVCDUgOLBX6eBc3CYLjtJ2pT6s+uErqHhwcmk/3MU/32/+KY979E1jwxim7WGOalKI5s/5dwZj/c/qkapmin7KrLxZ5k5Rex4PdEruvW0jpjdhXFkRVkw/qXIWwg9Kx6QjdFf3bVQrcnS7Ynk5lXxBNjO+odimPY/F9IT9A7CnD3gjEvgG/TTmfs9LbMg+xzcM9SqMO0F4o+VEKvQm5hMZ9tTuSazi3oHR6odzj2LyMJ1r8C3s3BQ8+P4hIun4KQTjD0UR3jcDKXTsLWd6HXXdB2kN7RKDVQCb0Ky3ac5GJOoWqd11XiZu37tDUQqnPV59t9ITFaJXRrKh2meN1Lekei1EJ9dqokv8jMR5sSGN4hmAHtm+sdjmNI2gy+LaBFV70jAVMUJP2uTRylNN7J7XBoJYx4AgLUiCR7pxJ6JV/tPMmF7AKeGNtJ71Acg5RaizhilH2UgJuioCBTG42hNE7pMEX/1jBipt7RKHWgEno5BcVa63xwRHOGRgbrHY5jSI+HrDNaIrUHESVTBSdG6xuHMzjwNZzeq3W1ePrqHY1SByqhl7N8dypnL+czQ/Wd111p4rSXhO7fUuv6SdqsdySOrXSYYpv+0OtuvaNR6kgl9BJFZgsfbjxB37aBjOyohrzVWWK0VjnYPFLvSK6IGAXJ26C4UO9IHNfvb2ufvMbNVcMUHYj6TZVYtfcUqRl5PDG2o11NDmbXLBatJWyKso/+81KmKCjKgdN79I7EMV1Kga3vQM87oN0QvaNR6kEldLQZFT/YEE/PsGaM6RKqdziOI+0I5KbbT3dLqYiRgLgynFKpn/+9pH1XwxQdjkrowA8HTpOUnsvjYzqp1nl9lPafR9R/zVKb8mkOrXpC4ia9I3E8J3dA7HIYPgMC2+kdjVJPdUroQohxQohjQoh4IcSsava5WwhxWAhxSAixzLph2o7ZInnvt3i6tPTnD91b6h2OY0mMhiATBLbVO5Krma6BlJ1QlKd3JI7DYoFfnge/VjDiSb2jURqg1oQuhHAD3gfGA92B+4QQ3Svt0wl4HhghpewBPGn9UG1jbewZTqTlMGNsRwwG1TqvM4tZK+Cxt+6WUqYoMBdoSV2pm4PfwqndcN0c8LLR0nWKTdWlhT4YiJdSJkgpC4GvgFsq7fMn4H0pZQaAlPK8dcO0DUtJ67xDC1/G92ytdziO5cx+rYDHXhN6u2Eg3NTwxboqzNH6zlv3hd736h2N0kB1SehhQEq5+6kl28rrDHQWQvwuhNguhBhX1YGEEA8LIWKEEDH2sIjFr0fOcfRsFo9f2xE31TqvH3vtPy9lbAZt+qkCo7r6/R3IOq2GKTo4a/3m3IFOwGjgPuBTIURg5Z2klJ9IKQdKKQe2aNHCSqduGCkl7/4WR/tgH27q3UbXWBxSYrRWwONvx9cdTFFaF0JBtt6R2LfMVG3ceY/boP0wvaNRGqEuCf0UUP6qV3jJtvJSgdVSyiIpZSJwHC3B262Nx9KIPXWZx0Z3xN1NtUjqpbhQm7TJXrtbSpmiwFKsxapU738vg7TAdS/rHYnSSHXJZLuATkIIkxDCE7gXWF1pn+/QWucIIULQumDsYLWDqkkpeXt9HGGB3tzWXy10W2+n92iFO/ba3VKq7RAweKjhizVJ2QUHv4Hhj0NQe72jURqp1oQupSwGHgd+AY4A30gpDwkhXhFC3Fyy2y9AuhDiMLABeFZKmW6roBtrS/wF9qVc4tHRHfBQrfP6S4wGREkBjx3z9IG2g1U/enWk1GZT9GsJI5/SOxrFCuq0wIWUcg2wptK2F8vdlsBfS77s3rvr42nVzMhdA9X8zg2SGA2temkFPPbOFAWbXoO8DPAO0jsa+3JwOZyKgVveBy9/vaNRrMDlmqfbE9LZmXSRR66JxMvdTe9wHE9Rnja22977z0tFjNL6h5O36h2JfSnM1YYptuoNfe7XOxrFSlwuob/7Wxwhfl7cO1iVNTdIyk6tYMdREnr4QHD3Vt0ulW17Dy6nqmGKTsalfpO7kzP4PT6dP0dFYvRQrfMGSdqsFey0c5Dhbe5e0G6omqirvMunYcs86H4LRIzQOxrFilwqob/7WxzNfT15YKhqnTdYYrRWsGNspnckdWcaBecPQbb+xWx2Yf0r2nDO61/ROxLFylwmoR9IvcTGY2lMH2nCx7NO14KVygqytUIdR+luKWW6RvuupgHQfn/7v4Rhj0FQhN7RKFbmMgn9nfXxBHh7MHmYGmvbYCe3ay07R0vorfuCp7/qR5cSfn4efENhpEMMSFPqySUS+uHTl/nfkXNMGxGBv9FD73AcV+ImrVCnrYOtYuPmDu2Hqxb6oZWQsgPGznasLjOlzlwiob+3IQ5/L3emDTfpHYpjS4zWCnU8ffSOpP5MUZAeD5mVZ61wEUV58OscrX6g7wN6R6PYiNMn9LhzWayNPcuU4REE+KjWeYPlZWhT5jpad0up0rhdtZW+7T3ITIEbXgWDGuHlrBwuoZuzsjBfvlzn/d/bEI+3hxsPjlSt80ZJ3gpIx03oLXtqlaKuOHzx8hnYPA+63aSN+FGclsMN97i0YgXn576GW0gIXiYTnh0i8YqMxNMUiVekCffWrRElhRKJF3L4Yf9p/jQqkua+njpH7uASo7UCnbABekfSMAaDNvdM4ibt4qArrR372z/BUqSGKboAh0vovkOHEvrsMxScSKAwIYHLa9ZiKddiF97eeJoi8DJF8nueD6NzvZnauj2WggIMXl46Ru7gEqO1Ah13B34NTdfAkR8gIwmau8gnttN7Yd9SGDETmkfqHY1iYw6X0I1du2Ls2rXsvpQS88WLFCYkaEk+MYGChESy9uxlwJnTDAAubV/MJSHwCA/HM9KElynySss+MhL3IDVpU42y0+D8Yeh1l96RNE75fnRXSOilwxR9QmDUM3pHozQBh0volQkhcA8Oxj04GJ9Bg8q2P7/yAD/sSOTnOyPwP59KYUIiBQknKExIJHfbdmRhYdm+boGBeHbogFekCU9TpJb0IyPxCAtDuKkLSGUXEh21/7xUSGdtqtjEaOg/We9obO/w93ByG9z4lhqm6CIcPqFX5dSlPJbvTuWeYZGED+oF9KnwuDSbKTpz5kqrPiGBgsQEstb/hvni8rL9hKcnnhEReEZq/fOekSVJPyICg48DDt1rqMRorTCndV+9I2kcIbTZFxOjXaMfPWY+BJlc481LAZw0oX+86QQAj47uWOXjws0Nz/BwPMPD8Yuq2OoszsigMDFRS/IJ2vf8w4fJWrcOLJay/dzbtMajVWuEpyfCw0P7Kr3tWXLfw7PcbY8q9zV4ekLJNkNVx6pw3CvbcHdHNFVCStqsTeLk5gR/LqYoiF0OF45Diy56R2M7WecgaQuMeloNU3QhTvAfWtH5y/l8tSuFO/qHExboXe/nuwcF4R4UhE///hW2WwoKKExOpjAhUeunP5FAcVoasrAQS04OsqgIWViofS9/u7BQ696R0lo/okaIKwnf2xuDj4/2Vf62jw8GX+27KHu84vbyX8LbG4Ovr3bM0jeLzFNaQc6AadaNXy+lw/YSo507oR/+XpsHvucdekfiMqTFgszPx5KXhyUvH5mXiyU/H0tuHjI/r2y7JS8Xn4EDMXbubPUYnC6hfxydgNki+Us1rfOGMnh5YezcucG/BGk2V53wa3ojqMO+lsJCZH4Bltxc7Q8mNwdzdhbF589hycnVtufmVrhmUCt39yuJ3lCMIT8Ew5ENGD6PxeBb7s3BxweDj2/ZG4n2cxaD2YwsNl99u9iMNJvBXFyyrfztK49rt6vbfvVtiotLtpnBbEZ4eWHw9kZ4G7U3MKMRg4+39oZlNGI4GoY49xWGWHdtu7FkPx/tcVHhtrd2LC+vpvtEVIk0mys0Dkq/LBUaDaV/GyWP/7oYmd8BueEgiIPaUF4hAKEN4RRc2SYq3UeAQWiNhrJ9Kt8veY4oOV7Jc8rul+5f+tzK+5TfViGWcs+r6jiNfJ4sLMSSl4fMyytJtrnlknDJ9jztvszPw5KbhyW/0vaSfS35WnKWZfvn1/l32vLF2Sqh1+ZCdgFLdyRzS982tAu2rz5u4eaG8PYG7/p/arAGWVxckvBzyyX6HO2PsyTpl38D0N4gcrHEbcFy0Q3p4UNxWhqW5Nwrbx45ORW6oerE3V17Ldzcym7j7oZwq+r21fsKH0+ocl/tNgaBLCjU/glztX+8osxMZG5JaykvD0sOYD4NG16ve9wGQ4UEb/AuSfwlbxLCx7vim4enJ7KouCzJWsoS8dWf3ip8FRVhKSq3X2EhmM31e43LWz+n4c91dUJc+X2X/l6N2m23kGA8jN5VNxyM5f9GSm+XbC9pKBgCAmwSslMl9M82J1JQbOGxMdZtnTsD4e6Om78/bv71WDtSSnirF7QZDPcsqeJhqbV4St4MhOBKknZzQ7hXTMwYDLq1civY9yVy5SNYJq/D0sxUsbVWdrtcCy0v78obRKVWmSUvT3ujK3meLHmzk4WFV1//qPzl4YEwemFo5l9y/cSzmv3LXY/x9NT29ay0r0e5bYe/RWx/B8P0nyAkUvs9SgkWS0nPn3YbKZEWC0i07pny9yvsU/L8yvuU3K96n2qeQ2kcpTFJbVuF41Z1HG1b456Hdq2qlmRrD5/KGsppEnpGTiGLtyVxY+82dGjhp3c4ziEjSZv/Y8TMKh8WQmjdG15e4Ehj+U2jEAZwS4vBraNtZo6UUuqXDLY+Dl16Qg8HWVVKsRqHm8ulOgt+TyS30MzjqnVuPaXzhzv6+PPKAsKheQebzo+uWzK/mACn96iLoS7KKVromXlFLPo9ifE9W9GlVT26FJSaJW3WCnFCar54cyr7FP/Y8g+uCb+GKT2mOMbHVFMUxK4Ac7HVh2P+lPATH+z7gEJLPS5EW0t+FrRtA6e+h+U/0s6/Ha+OepVQn9Cmj0Vpck6R0D/fmkRWQTGPX6ta51YjpdaCjRhVYwHOsYvHePR/j3Ix/yIx52I4l3uOZwc9i0HY+Yc/0yjYvVCbEjjcehOOLT60mP/E/Ifuwd3pHGT9UQy1OvoTGPwhbARSStYlr2Py2sl8dN1HRARENH08SpNy+ISeXVDMgt8Tua5bKD3a2ObKsUu6cByyz9XY3bL73G5mrJ+Bt7s339z0DSvjVvLFkS+4mH+Rf434Fx5udjz/fETpePRNVknoUkrm7ZnHwtiFXNfuOuZGzcXLrYknMks7Br++A+Neg6GPAHBf1/t49H+PMnntZD687kN6hPRo2piUJmXnzajaLdmWzKXcImZc20nvUJxLLf3nv538jT//+meCvYNZMmEJnYM687dBf2Nm/5msSVzDjN9mkFuU24QB15NfKIR2t0o/erGlmNm/z2Zh7ELu6nwXb1zzRtMnc4DYlYCA7reUbeoR0oPF4xfj7e7Ng788yNbTW5s+LqXJOHRCzy0s5rPNCUR1bkGftoF6h+NcEqMhoG2VK8OvjFvJUxufonNQZxaPX0wbvzaAdiHwoV4P8dKwl9h2ZhsPrXuIjPyMJg68HiJGaQtfFze8rzuvOI8nNzzJ9ye+55E+jzB76Gzc9Ci1l1K7JhAxEpq1rvBQREAESyYsIcw/jMfWP8baxLVNH5/SJBw6oS/bcZL0nEKeUH3n1mWxaBdETVEV+s+llHx28DPmbJ3DsNbD+OwPnxFkvHq44h2d72De6HkczzjO5LWTOZ19uimjrztTFBTnwamYBj09syCTP//6Z6JTo/n7kL/zWN/H9LsgfC4W0uOg5+1VPhzqE8qicYvoHdKbv0X/jaVHljZxgEpTcNiEnl9k5uPoBIZFBjMworne4TiX84e0NUTLdbdYpIXXdr3G23veZoJpAu9e+y4+HtVX417b7lo+vv5j0vPSmbRmEnEZcU0Ref1EjABEg7pdzuacZerPU4m9EMt/rvkP93a91/rx1UfsChBu0O2Wandp5tmMj6//mDFtxzB351ze2fOOVnCjOA2HTehf70ohLauAGWNV69zqShNcyYXDInMRszbPYumRpUzsNpFXR71apwueA1oOYNH4RUgkU36ewp5ze2wZdf15B0Hr3vVeZzQhM4HJaydzJucMH173ITdE3GCjAOuotLslcjT4Bte4q9HdyH9H/5c7Ot3Bpwc/5eVtL1NsKW6aOBWbc8iEXlBs5qNNJxgUEcSwyJr/gJUGSIzWCm8CwsgtyuXx3x5nbeJanuz/JM8Neq5eQxI7B3VmyYQlBBuDefjXh9mYstFmYTeIKQpSd0Jh3S7gHkg7wJS1UygwF7DghgUMaW2bStN6ObUHLp2strulMneDO3OGzeFPvf7EirgVPL3xafKL6z6xlGK/6vSfKYQYJ4Q4JoSIF0LMqmG/O4QQUggx0HohXm3F7lOcycxnxrWdHKOIxZGYiyF5K5iiuJh/kem/TGfHmR28MvwVpvea3qDXO8wvjM/Hf07HwI48ueFJVsWtskHgDWS6BsyFkLKj1l23nNrCQ+sews/DjyXjl9A9uHsTBFgHsSvA4AFdb6zzU4QQPNH/CWYNnsWGlA088r9HuFx4ufYnKnat1oQuhHAD3gfGA92B+4QQV/0lCyH8gZlA7f8ZjVBktvDBxnj6tA1kVKcQW57KNZ3ZDwWXOdWmJ1PWTiHuUhxvjXmL2zrd1qjDNjc2Z/4N8xncajAvbn2R+Qfn20f/bbuhWt9zUs3dLj8m/MiM9TNo59+OJROW0K5ZuyYKsBYWCxxaBZ2uB+/Aej/9gW4P8FrUa+xP28+0n6eRlptm/RiVJlOXFvpgIF5KmSClLAS+Aqq68vJP4DXApp/dvtt7itSMPJ64tqNqndtC4iaOe3gwOf4L0vPT+fQPnzK67WirHNrXw5f3x77P+IjxvLXnLf4T8x8ssp7T71qblz+EDajxwuiSw0t4fvPz9GvZj4XjFhLibUcNiZTtkHUaetStu6Uq403jeX/s+6RkpTBp7SSSLydbMUClKdUloYcBKeXup5ZsKyOE6A+0lVL+VNOBhBAPCyFihBAxaWkNawmEBXlz14Bwru2q5qawhT0JvzA1rDUINz4f9zn9QvtZ9fgebh7MjZrL/V3vZ8nhJbyw5QWKzEVWPUe9maK0fuiCrAqbpZTM2z2P13e9znXtruPD6z7E39PO5gqKXQHu3tBlfKMOM7zNcBbesJDcolwmr53MofRDVgpQaUqNvigqhDAAbwJP17avlPITKeVAKeXAFi1aNOh8wzuE8J+7+qjWuQ1sSPqVhy2nCHb3ZcmEJXQKsk31rUEYmDV4FjP7z+SnhJ/0ryo1RYE0Q/K2sk3FlmJe3PoiC2IX6Fv9WRNzsbbUXOc/gFfjp4wurSo1uhl58OcH2XZ6W+1PUuxKXRL6KaBtufvhJdtK+QM9gY1CiCRgKLDa1hdGFetaGbeSJzc9TefCQhb3fqqs+tNW7KqqtO1gcPPU5nVBq/58asNTfBf/nb7Vn7VJ2gw5aVadKrd8Velf1v+FnxN/ttqxFdurS0LfBXQSQpiEEJ7AvcDq0gellJlSyhApZYSUMgLYDtwspWxY+Z3SpCpUfxpb8dnZNII6/aHJzm8XVaUe3tB2CCRGl1V/bkrdpH/1Z20OrQRPP7Dy76t8Velz0c+pqlIHUmtCl1IWA48DvwBHgG+klIeEEK8IIW62dYCK7Vikhdd3vX6l+jPXHZ9WvbWCmyZUoap07STiM+Kb9PwAmKI4l3aYqWsm2U/1Z02KC+HwaugyQXtDsrLSqtLRbUczd+dc3t37rn2MSlJqVKc+dCnlGillZyllBynl/5Vse1FKubqKfUer1rn9K63+/OLIF1r155DZeKTu0uYJ18GAlgNYOG4hUkom/zyZvef3Nun5E1p0ZFKbUM5kn7aP6s/aJGyA/Es2XZnI6G7kzdFvcnun2/nkwCeqqtQBOGSlqNI45as/Z/afqVV/pu7SCmxM1+gWV5fmXcqqSv+07k9sStnUJOc9mHaQKQfepEAYWNCsv31Uf9YmdiUYA6DDtTY9jbvBnZeGvVShqrTAXGDTcyoNpxK6i6lc/flQr4e0PuKkzWBw1wptdFS+qnTmhpl8F/+dTc/3+6nfmb5uOn4e/ixxN9E9db9Nz2cVRfnaykTdbgJ3T5ufrnxV6W8p2jz4qqrUPqmE7kJOZ5+uvvozMRra9NcKbXRWvqp09u+zWRC7wCb9tz8l/MTj6x+/Uv0ZORbSjkL2eaufy6rif4XCrCZfCPqBbg/wetTrqqrUjqmE7iKOZxxn0ppJpOen88n1n1Ss/izI0gpralhurqmVryqdt3seb8S8YdWq0i8Of8GszbMqVn+W/vxWWMXIpmJXgE8IRDT970tVldo3ldBdwJ5ze5j681QAPh/3Of1b9q+4Q/I2rbDGjhI6VKwqXXx4sVWqSqWUvLX7LV7b9drV1Z+t+oBXgH0n9MIcOP6Ltsycmz5LAquqUvulErqT23ByAw//+jDBxuDqqz8TN2mFNW0HN32AtbBmVWmxpZg5W+cwP3Z+1dWfbu7QfnitE3Xp6thaKMqt81S5tqKqSu2TSuhObFXcKp7a+BSdAjtVWPvzKonRWmGNDcYzW4M1qkpLqz9Xxa+qufrTFAUXE+BSytWP2YPYleDfGtoN0zuSq6tKk1RVqd5UQndCpdWfL259kSGthzD/hvlVrv0JQO5FOHvQ7rpbqlK5qvRM9pk6Pa9e1Z+lr4M9ttLzM7ULoj1uAzuZiqBCVemm51h2ZJneIbk0ldCdTPnqz/Gm8bx37Xs1rv1J8u+AdIiEDhWrSieunVhrVem5nHP1W/sztDv4BNtnP/rRn7RagUZMlWsL5atKX935qqoq1ZFK6E6kyFzE85ufL6v+nDtqbu1rfyZGg4ePNmTRQZSvKp3y8xT2nd9X5X6JmYlMWjupfmt/GgwQMVJbZ9TeklLsSghoB+H2N+9d5arSV7a/gtli1jssl6MSupPILcplxm8zWJO45kr1Z13W/kyM1vpjm6BAxZpKq0qDjEFVVpUeTDvI5LWTG7b2pykKLqdqfen2IiddK/fveRvY6WRh5atKlx9fztObVFVpU1MJ3Qlk5Gfw0LqH2HZmW8Xqz9pkn9cKaRyku6WyML8wFo9fTIfADhWqSree2lpS/dnAtT9Lpz+wp26XI6vBUtzkxUT1Vb6qdP3J9Tzy6yNkFWbV/kTFKvQZyNoIlwsvk5mfqXcYduNy0WVmRc/iTM4Z3hr9FmPajan7k0sTlk4TcllDaVXpUxueYvbvs9l5ZidrE9fSIbADH13/UcOWiwvuCH6ttAujA6dZP+iGOLRSi6tVb70jqZMHuj1Ac2NzXtjyAtN+nsb/jfw/fNxruJbjYgKNgTZZ/crhEvqK4yt4c/ebeodhV/w9/fnk+k+uLhiqTWK0VkjTqo9tAmsipVWlf9/yd35I+IFBrQbx9pi3G/4PI4T2qSVhg9aPrncXR9Y5SNoCo57RP5Z6GG8aT4BXAE9ueJI7f7hT73Dsyuyhs7m7y91WP67DJfRRYaPsa5FeO9C/ZX/C/MJq37GypM0QMUK3ikNrKq0qva3TbfRv2b/xy8WZouDgN1qXVGg36wTZUIe/B2mx++6WqgxvM5xvb/qWA2kH9A7FrvQK6WWT4zrcf3LHoI50DOqodxiO71KKdtFv0J/0jsRqDMLAsDZWKrgp7YZK3Kx/Qo9doQ2nDO2qbxwN1L5Ze9o3a693GC5BXRR1VaWFMw56QdTmgiIgsF3ZOqO6yUyFlO26l/orjkEldFeVuFkroAmt5wgQV2KK0vquLdab5bHeDq3SvttZMZFin1RCd0VSahdEI0ZqhTRK1SKitGXezh3UL4bYFdC6LwR30C8GxWGo/2ZXdDFBK5xR3S01K+tH12k8+sUEOL3XIS+GKvpQCd0VlfWf67d+qENo1gaCO+mX0GNXat973FbzfopSQiV0V5QYrU3BGqxGC9XKFAXJW6GRC2s0yKFV2rTGgW2b/tyKQ1IJ3dWU9Z+PcqgiFd2YRkFhNpze17TnPX8UzsWq7halXlRCdzVpRyEnTfWf11VEaT96Ew9fPLQSENpSc4pSRyqhu5pENf68XnxDoGXPpu1Hl1LrP48YCf6tmu68isNTCd3VJG7SCmaCVOVenUWMgpQdUNxEU8GePQjpcaqYSKk3ldBdicWiFcqo1nn9mKKgOB9SdzXN+Q6tBOEG3VR3i1I/KqG7knMHtUIZNVyxftoPB2Fomm4XKbViog5jwDfY9udTnIpK6K6kNCFFOO7857rwDoTWfa5cf7ClU7vh0klV6q80iEroriQxWiuUadZa70gcjylK63IpzLHteWJXgpsndP2jbc+jOKU6JXQhxDghxDEhRLwQYlYVj/9VCHFYCHFACLFeCKGuuNkbc5FWIKP6zxvGFAWWIji53XbnsFi0YqKO12mfChSlnmpN6EIIN+B9YDzQHbhPCFF5ir69wEApZW9gOfC6tQNVGun0Pq1AxoGXm9NV26FgcLdtP3rKdsg6rYqJlAarSwt9MBAvpUyQUhYCXwEVLr9LKTdIKXNL7m4Hwq0bptJopYUxqv+8Ybz8IGzglXlwbCF2Bbh7Q+dxtjuH4tTqktDDgJRy91NLtlVnOrC2qgeEEA8LIWKEEDFpaWl1j1JpvKTNWoGMr1q+r8FMUdrsh7ZYpNxcrC011/kG7c1DURrAqhdFhRATgYHAf6p6XEr5iZRyoJRyYIsWLax5aqUmxQVa36/qP28cU5S2tmfyVusfO2mzNiWD6m5RGqEuCf0UUH66t/CSbRUIIa4D/g7cLKVsopI6pU5Sd2mFMaq7pXHCB4Gbl22GL8auAE8/6HS99Y+tuIy6JPRdQCchhEkI4QncC6wuv4MQoh/wMVoyP2/9MJVGSYzWCmPaD9c7EsfmYYR2Q6x/YbS4EI78oA1V9PC27rEVl1JrQpdSFgOPA78AR4BvpJSHhBCvCCFuLtntP4Af8K0QYp8QYnU1h1P0kLhZW8ZMDYVrPFOUVnGbk269YyZs0Cp4VXeL0kjuddlJSrkGWFNp24vlbl9n5bgUaynM0bpchv1F70icQ0TJdYjkLdab2jZ2JRgDIXKMdY6nuCxVKersTm7XCmLUBVHrCOsPHr7W63YpyoOjP0G3m8Dd0zrHVFyWSujOLmmzVhDTbpjekTgHNw/tWoS1Enrcr1CYpabKVaxCJXRnlxitFcR4+uodifMwjYILx+HymcYf69BK8Am50pWjKI2gErozy8/UCmFUd4t1lb6eSVsad5yCbDj2M/S4FdzqdDlLUWqkErozS96mFcKohG5drXqDMaDx64we/xmK89RUuYrVqITuzBKjtUKY8EF6R+JcDG7QfmTj+9FjV4J/a3V9Q7EaldCdWWK0VgjjYdQ7EudjioJLyZCR3LDn52dC/K/Q4zYwqH9DxTrUX5Kzyr2oFcCo7hbbKOtHb+A0AEd/AnOhKiZSrEoldGdVmmjU+qG2EdpNG53S0G6X2BUQ2A7CBlg3LsWlqYTurBKjtQKYNv30jsQ5CaENX0zcrC3sXB856ZCwUbsYKoRNwlNck0rozioxWiuAcfPQOxLnZYrSVhhKP1G/5x1ZDZZi1d2iWJ1K6M4o66xW+KL6z22rtDurvsMXD62E4I7Qqpf1Y1Jcmkrozqh0vm61fqhtNY8E/zb1uzCadVb7/fS8Q3W3KFanErozStykFb606q13JM5NCO1TUOJmsFjq9pzD3wNSFRMpNqESujNK2qytTmRw0zsS52eKgtwLkHakbvvHroTQHhDa1bZxKS5JJXRnk5EMGUlqubmmUtqtVZfhi5mpkLJdzayo2IxK6M6mbPy5uiDaJALbQVBE3dYZPbRK+64SumIjKqE7m8TNWsFLaDe9I3Edpiht5kWLueb9YldodQHNI5smLsXlqITuTKTUPvqbRqkRFE0pIgoKMuHM/ur3ST+hTWWsxp4rNqQSujNJP6EVuqjulqZV2o9e0/DF0u6WHrfZPh7FZamE7kySSi7MqflbmpZ/KwjpUvOF0diV0HYoBIQ3XVyKy1EJ3ZkkRkOzMNVHqwdTlLagiLno6sfOH4Xzh9TFUMXmVEJ3FhaLdkE0QvWf68I0Copy4NSeqx87tBKEAbrf2uRhKa5FJXRnkXZEK3BR/ef6iKhmPLqUWndL+xHg37Lp41JcikrozkLN36Ivn+baZFuVJ+o6exDS49ToFqVJqITuLBKjtQKXwHZ6R+K6IqIgZScU5V/ZFrsCDO7Q7Wb94lJchkrozsBi1gpbVHeLvkxRYC6A1J3afSm1/vPI0eAbrGtoimtQCd0ZnD2gFbao4Yr6aj8chNuVfvRTu+HSSdXdojQZldCdQWkCiRipbxyuztgM2vS98vuIXQluntD1j7qGpbgOldCdQWK0Vtji30rvSBRTlNYyz7+sdbd0vF6bm15RmoBK6I7OXKQVtKj+c/tgitLWC90yD7LOqGIipUnVKaELIcYJIY4JIeKFELOqeNxLCPF1yeM7hBARVo9UqdqpPVpBixquaB/aDgWDB2x9F9y9ofM4vSNSXEitCV0I4Qa8D4wHugP3CSG6V9ptOpAhpewIzANes3agSjXK+s9VQrcLnj4QPggsRdBlHHj56R2R4kLc67DPYCBeSpkAIIT4CrgFOFxun1uAl0puLwfeE0IIKaW0YqyaPUtg23tWP6zDunxaK2jxaa53JEopUxSc3KrWDVWaXF0SehiQUu5+KjCkun2klMVCiEwgGLhQfichxMPAwwDt2jWwAManObTo0rDnOqMWXaD3PXpHoZTXf5LWDdb5Br0jUVxMXRK61UgpPwE+ARg4cGDDWu9d/6iGgSn2LSAc/vAvvaNQXFBdLoqeAtqWux9esq3KfYQQ7kAAkG6NABVFUZS6qUtC3wV0EkKYhBCewL3A6kr7rAamlNy+E/jNJv3niqIoSrVq7XIp6RN/HPgFcAMWSCkPCSFeAWKklKuB+cASIUQ8cBEt6SuKoihNqE596FLKNcCaStteLHc7H7jLuqEpiqIo9aEqRRVFUZyESuiKoihOQiV0RVEUJ6ESuqIoipMQeo0uFEKkAcm6nNx6QqhUDevi1OtxhXotKlKvR0WNeT3aSylbVPWAbgndGQghYqSUA/WOw16o1+MK9VpUpF6Pimz1eqguF0VRFCehErqiKIqTUAm9cT7ROwA7o16PK9RrUZF6PSqyyeuh+tAVRVGchGqhK4qiOAmV0BVFUZyESugNJIQIFEIsF0IcFUIcEUIM0zsmvQghnhJCHBJCxAohvhRCGPWOqSkJIRYIIc4LIWLLbWsuhPhVCBFX8j1IzxibUjWvx39K/lcOCCFWCSECdQyxyVT1WpR77GkhhBRChFjrfCqhN9zbwM9Syq5AH+CIzvHoQggRBjwBDJRS9kSbYtnVpk9eBIyrtG0WsF5K2QlYX3LfVSzi6tfjV6CnlLI3cBx4vqmD0skirn4tEEK0Bf4AnLTmyVRCbwAhRAAQhTYPPFLKQinlJV2D0pc74F2yWpUPcFrneJqUlDIabR2A8m4BPi+5/Tlwa1PGpKeqXg8p5TopZXHJ3e1oK585vWr+NgDmAc8BVh2VohJ6w5iANGChEGKvEOIzIYSv3kHpQUp5CngDraVxBsiUUq7TNyq70FJKeabk9lmgpZ7B2JkHgbV6B6EXIcQtwCkp5X5rH1sl9IZxB/oDH0op+wE5uNZH6jIlfcO3oL3JtQF8hRAT9Y3KvpQsx6jGBwNCiL8DxcBSvWPRgxDCB3gBeLG2fRtCJfSGSQVSpZQ7Su4vR0vwrug6IFFKmSalLAJWAsN1jskenBNCtAYo+X5e53h0J4SYCtwIPODCaw53QGv87BdCJKF1Pe0RQrSyxsFVQm8AKeVZIEUI0aVk01jgsI4h6ekkMFQI4SOEEGivhUteIK6k/MLpU4DvdYxFd0KIcWh9xjdLKXP1jkcvUsqDUspQKWWElDICrXHYvySnNJpK6A03A1gqhDgA9AX+rW84+ij5lLIc2AMcRPubcqkybyHEl8A2oIsQIlUIMR2YC1wvhIhD+xQzV88Ym1I1r8d7gD/wqxBinxDiI12DbCLVvBa2O5/rfvJRFEVxLqqFriiK4iRUQlcURXESKqEriqI4CZXQFUVRnIRK6IqiKE5CJXRFsQIhRJI1Z81TlIZQCV1xGUKj/uYVp6X+uBWnJoSIEEIcE0IsBmKB+SXzth8UQtxTss9oIcSP5Z7zXkmZemnL+2UhxJ6S53Qt2R4shFhXMg/8Z4Bo+p9OUSpSCV1xBZ2AD9AmRApHm7/+OuA/pfOt1OKClLI/8CHwTMm2OcAWKWUPYBXQzupRK0o9qYSuuIJkKeV2YCTwpZTSLKU8B2wCBtXh+StLvu8GIkpuRwFfAEgpfwIyrBqxojSASuiKK8ip5fFiKv4vVF5Cr6Dkuxlt6mRFsUsqoSuuZDNwjxDCTQjRAq2VvRNIBroLIbxK1rocW4djRQP3AwghxgMus2aoYr9Ua0NxJauAYcB+tAUnniudtlQI8Q3aRdNEYG8djvUy8KUQ4hCwFSuvDakoDaFmW1QURXESqstFURTFSaiEriiK4iRUQlcURXESKqEriqI4CZXQFUVRnIRK6IqiKE5CJXRFURQn8f8jdCYPg9OwoAAAAABJRU5ErkJggg==\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, it appears that with very little data the model is overfitting a lot (observe test loss of 0, meaning that every single prediction went wrong at that round).\n",
        "Remember that in you application you might be interested in completely different measures!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output of this notebook\n",
        "\n",
        "The result of this notebook is a collection methods ready for evaluation with the real data.\n",
        "\n",
        "You should export classes and functions to `model.py` with `# nbdev_build_lib` (workflows will do this automatically)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## You can move on to loss notebook!"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "ml_libclass",
      "language": "python",
      "display_name": "Python 3.8 (ml_libclass_mk)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "ml_libclass"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}