{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"# default_exp data\\n%load_ext nb_black\\n# nb_black if running in jupyter\\n# lab_black\\n%load_ext autoreload\\n# automatically reload python modules if there are changes in the\\n%autoreload 2\";\n",
       "                var nbb_formatted_code = \"# default_exp data\\n%load_ext nb_black\\n# nb_black if running in jupyter\\n# lab_black\\n%load_ext autoreload\\n# automatically reload python modules if there are changes in the\\n%autoreload 2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# default_exp data\n",
    "%load_ext nb_black\n",
    "# nb_black if running in jupyter\n",
    "# lab_black\n",
    "%load_ext autoreload\n",
    "# automatically reload python modules if there are changes in the\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"# hide\\nfrom nbdev.showdoc import *\";\n",
       "                var nbb_formatted_code = \"# hide\\nfrom nbdev.showdoc import *\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> You should begin your work by cleaning up your data and possibly defining tools for doing it repeateadly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***input***: raw data\n",
    "\n",
    "***output***: clean and tidy dataset for ML model / simulation / analytics + toy dataset for testing\n",
    "\n",
    "***description:***\n",
    "\n",
    "This is the first notebook of your machine learning project. In this notebook, you will load the data, inspect, clean and make it tidy. \n",
    "You will define the data points and their features and labels. The output of this notebook is a clean, tidy dataset ready for analysis and machine learning.\n",
    "You can also do a basic statistical analysis of the data to better understand it.\n",
    "For any functions you define for handling the data, remember to mark their cells with `# export` -comment,\n",
    "so that they will be included in the data.py-module build based on this notebook.\n",
    "You can also include unit tests for your own functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant modules\n",
    "\n",
    "Import python modules you need for handling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"import matplotlib.pyplot as plt\\nimport numpy as np\\nfrom numpy import nan as Nan\\nimport pandas as pd\\nfrom sklearn import datasets\\n\\n# from pandas.api.types import CategoricalDtype\";\n",
       "                var nbb_formatted_code = \"import matplotlib.pyplot as plt\\nimport numpy as np\\nfrom numpy import nan as Nan\\nimport pandas as pd\\nfrom sklearn import datasets\\n\\n# from pandas.api.types import CategoricalDtype\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import nan as Nan\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "# from pandas.api.types import CategoricalDtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define notebook parameters\n",
    "\n",
    "Define input, output and additional parameters of this notebook, the information needed for running the notebook.\n",
    "In your own project, you can do this step in the later iterations of the work,\n",
    "when you know what is required.\n",
    "In this cell, only assing values to variables directly: `variable_name = value`.\n",
    "**Do not derive any information in this cell as it will mess up the parameterization** - do it in the cell below.\n",
    "\n",
    "The cell below has the tag 'parameters' - this is for the notebook parameterization tool 'papermill'\n",
    "that allows you execute complete notebooks as python functions.\n",
    "The values you define here will become the default values of the parameterized notebook, but you can also run the notebook with completely different setup.\n",
    "More on this in the `workflow` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"# This cell is tagged with 'parameters'\\noutput_clean_filepath = (\\n    \\\"data/preprocessed_data/dataset_clean_hki_lib_book_classification.csv\\\"\\n)\\noutput_toy_filepath = (\\n    \\\"data/preprocessed_data/dataset_toy_hki_lib_book_classification.csv\\\"\\n)\\nseed = 0\";\n",
       "                var nbb_formatted_code = \"# This cell is tagged with 'parameters'\\noutput_clean_filepath = (\\n    \\\"data/preprocessed_data/dataset_clean_hki_lib_book_classification.csv\\\"\\n)\\noutput_toy_filepath = (\\n    \\\"data/preprocessed_data/dataset_toy_hki_lib_book_classification.csv\\\"\\n)\\nseed = 0\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This cell is tagged with 'parameters'\n",
    "output_clean_filepath = (\n",
    "    \"data/preprocessed_data/dataset_clean_hki_lib_book_classification.csv\"\n",
    ")\n",
    "output_toy_filepath = (\n",
    "    \"data/preprocessed_data/dataset_toy_hki_lib_book_classification.csv\"\n",
    ")\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define any immediate derivative operations, righ below the parameters cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"# set seed\\nnp.random.seed(seed)\";\n",
       "                var nbb_formatted_code = \"# set seed\\nnp.random.seed(seed)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set seed\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "We are using a csv-file exported from database of Helsinki City library system. We could also load the data straight from database or from online resources, if needed and possible. Please note, that you should not add your datasets to git, as it is not intended for data version control and tracking large datafiles exceed the limits of it. The 'data'-folder of this template is ignored by git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"#\\n# Read the raw data of library volumes. The fields are separated by commas.\\n#\\n\\ndf_raw_data = pd.read_csv(\\\"data/raw_data/sample4.csv\\\")\";\n",
       "                var nbb_formatted_code = \"#\\n# Read the raw data of library volumes. The fields are separated by commas.\\n#\\n\\ndf_raw_data = pd.read_csv(\\\"data/raw_data/sample4.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Read the raw data of library volumes. The fields are separated by commas.\n",
    "#\n",
    "\n",
    "df_raw_data = pd.read_csv(\"data/raw_data/sample4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe the data\n",
    "\n",
    "The database uses so called MARC-system. There are multiple rows for each volume(=item). The file consists of three columns:\n",
    "\n",
    "id (id of the Volume that is item e.g. a book)\n",
    "content (the metadata belonging to this volume)\n",
    "marc_tag (specifies what the value of the 'content'-field in this row means), for example:\n",
    "marc_tag '650' means that content field holds a keyword\n",
    "marc_tag '095' means that content field holds Helsinki-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(820271, 7)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"df_raw_data.shape\";\n",
       "                var nbb_formatted_code = \"df_raw_data.shape\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>084</th>\n",
       "      <th>092</th>\n",
       "      <th>093</th>\n",
       "      <th>094</th>\n",
       "      <th>095</th>\n",
       "      <th>650</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>420907795010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>420907795011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>420907795013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>420907795014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.791</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>420907795016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      record_id  084  092  093  094    095  650\n",
       "0  420907795010  NaN  NaN  NaN  NaN  180.2  NaN\n",
       "1  420907795011  NaN  NaN  NaN  1.4    NaN  NaN\n",
       "2  420907795013  NaN  NaN  NaN  1.4    NaN  NaN\n",
       "3  420907795014  NaN  NaN  NaN  NaN  1.791  NaN\n",
       "4  420907795016  NaN  1.4  NaN  1.4    NaN  NaN"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"# view the data\\ndf_raw_data.head()\";\n",
       "                var nbb_formatted_code = \"# view the data\\ndf_raw_data.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view the data\n",
    "df_raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 820271 entries, 0 to 820270\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   record_id  820271 non-null  int64 \n",
      " 1   084        271200 non-null  object\n",
      " 2   092        428182 non-null  object\n",
      " 3   093        157261 non-null  object\n",
      " 4   094        422208 non-null  object\n",
      " 5   095        552728 non-null  object\n",
      " 6   650        664145 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 43.8+ MB\n",
      "\n",
      "\n",
      "NO HKI CLASS:\n",
      "       record_id  084  092  093  094    095  650\n",
      "0  420907795010  NaN  NaN  NaN  NaN  180.2  NaN\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"df_raw_data.info()\\n\\n# CHECK THIS!\\n# An example in dataset 'Sample4.csv, where there is no HKI-CLASS.\\n# Instead the row has 093- and 092 Classes and some keywords\\nprint(\\\"\\\\n\\\\nNO HKI CLASS:\\\\n\\\", df_raw_data.loc[df_raw_data[\\\"record_id\\\"] == 420907795010])\";\n",
       "                var nbb_formatted_code = \"df_raw_data.info()\\n\\n# CHECK THIS!\\n# An example in dataset 'Sample4.csv, where there is no HKI-CLASS.\\n# Instead the row has 093- and 092 Classes and some keywords\\nprint(\\\"\\\\n\\\\nNO HKI CLASS:\\\\n\\\", df_raw_data.loc[df_raw_data[\\\"record_id\\\"] == 420907795010])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_raw_data.info()\n",
    "\n",
    "# CHECK THIS!\n",
    "# An example in dataset 'Sample4.csv, where there is no HKI-CLASS.\n",
    "# Instead the row has 093- and 092 Classes and some keywords\n",
    "print(\"\\n\\nNO HKI CLASS:\\n\", df_raw_data.loc[df_raw_data[\"record_id\"] == 420907795010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so there are 13 features and one label (the last column in our dataset) in the data. Let's construct these into dataframe column names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data and make it tidy\n",
    "\n",
    "For analytics & ML purposes, we should make the data tidy. This means that\n",
    "\n",
    " 1. Every column is a variable (feature or label)\n",
    " 2. Every row is an observation (data point).\n",
    " 3. Every cell is a single value (int, float, cathegorical, str, but no nested structures like lists or dictionaries)\n",
    "\n",
    "\n",
    " - Our **features** consist of keywords and different classifications. Marc_tags for this features are:\n",
    "   - 084 = PLC - Finnish Public Libraries Classification System (YKL)\n",
    "   - 092 = City of Espoo classification\n",
    "   - 093 = City of Kauniainen classification\n",
    "   - 094 = City of Vantaa classification\n",
    "   - 650 = Keyword (subject/index term/concept)\n",
    " - Our **label** is Helsinki City Library Classification System and we call it HCLCS (HKLJ) (095 in raw data).\n",
    " - Our **datapoint** is a library item (book, cd, dvd...) identified by id column in raw data.\n",
    " \n",
    "~~First we get only those rows that contain keywords (marc_tag = 650).~~\n",
    "\n",
    "Filtering the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 820271 entries, 0 to 820270\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   record_id  820271 non-null  int64 \n",
      " 1   084        271200 non-null  object\n",
      " 2   092        428182 non-null  object\n",
      " 3   093        157261 non-null  object\n",
      " 4   094        422208 non-null  object\n",
      " 5   095        552728 non-null  object\n",
      " 6   650        664145 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 43.8+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 176899 entries, 10 to 820064\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   record_id  176899 non-null  int64 \n",
      " 1   084        176899 non-null  object\n",
      " 2   092        100785 non-null  object\n",
      " 3   093        40354 non-null   object\n",
      " 4   094        111927 non-null  object\n",
      " 5   095        176899 non-null  object\n",
      " 6   650        158786 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 10.8+ MB\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 117;\n",
       "                var nbb_unformatted_code = \"#\\n# Get only those rows that has ykl- and hklj-classes\\n#\\n\\n# Filtter\\u00f6id\\u00e4\\u00e4n kaunokirjallisuus pois (\\\"095\\\":n luokka 1.4) ja vaaditaan etteiv\\u00e4t 084 ja 095 ole tyhji\\u00e4.\\ndf_raw_data.info()\\ndf1 = df_raw_data[:]\\ndf1 = df1[\\n    (\\n        (df_raw_data[\\\"084\\\"].notnull())\\n        & (df_raw_data[\\\"095\\\"].notnull())\\n        & (df_raw_data[\\\"095\\\"] != \\\"1.4\\\")\\n    )\\n]\\n\\n# 084-luokituksia (YKL) voi olla useita samalla niteelle (datariville). Valitaan ensimm\\u00e4inen arvo.\\ndf1.loc[:, (\\\"084\\\")] = df1[\\\"084\\\"].apply(lambda x: x.split(\\\",\\\")[0])\\n\\n# print(df1.head(10))\\n# df1.head()\\ndf1.info()\";\n",
       "                var nbb_formatted_code = \"#\\n# Get only those rows that has ykl- and hklj-classes\\n#\\n\\n# Filtter\\u00f6id\\u00e4\\u00e4n kaunokirjallisuus pois (\\\"095\\\":n luokka 1.4) ja vaaditaan etteiv\\u00e4t 084 ja 095 ole tyhji\\u00e4.\\ndf_raw_data.info()\\ndf1 = df_raw_data[:]\\ndf1 = df1[\\n    (\\n        (df_raw_data[\\\"084\\\"].notnull())\\n        & (df_raw_data[\\\"095\\\"].notnull())\\n        & (df_raw_data[\\\"095\\\"] != \\\"1.4\\\")\\n    )\\n]\\n\\n# 084-luokituksia (YKL) voi olla useita samalla niteelle (datariville). Valitaan ensimm\\u00e4inen arvo.\\ndf1.loc[:, (\\\"084\\\")] = df1[\\\"084\\\"].apply(lambda x: x.split(\\\",\\\")[0])\\n\\n# print(df1.head(10))\\n# df1.head()\\ndf1.info()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Get only those rows that has ykl- and hklj-classes\n",
    "#\n",
    "\n",
    "# Filtteröidään kaunokirjallisuus pois (\"095\":n luokka 1.4) ja vaaditaan etteivät 084 ja 095 ole tyhjiä.\n",
    "df_raw_data.info()\n",
    "df1 = df_raw_data[:]\n",
    "df1 = df1[\n",
    "    (\n",
    "        (df_raw_data[\"084\"].notnull())\n",
    "        & (df_raw_data[\"095\"].notnull())\n",
    "        & (df_raw_data[\"095\"] != \"1.4\")\n",
    "    )\n",
    "]\n",
    "\n",
    "# 084-luokituksia (YKL) voi olla useita samalla niteelle (datariville). Valitaan ensimmäinen arvo.\n",
    "df1.loc[:, (\"084\")] = df1[\"084\"].apply(lambda x: x.split(\",\")[0])\n",
    "\n",
    "# print(df1.head(10))\n",
    "# df1.head()\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 95;\n",
       "                var nbb_unformatted_code = \"# Fixer function which scrapes \\\"real\\\" part from broken string.\\n# I.e. broken: '59.562209'  -> fixed '59.5622',\\n#      broken: '59.562109*' -> fixed '59.562',\\n#      broken: '89.41038'   -> fixed '89.41'\\ndef fix_class(wrong_value, notations):\\n    wrong_value = wrong_value.split(\\\",\\\")[\\n        0\\n    ]  # take the first value. cell could contain multiple values\\n    # print(\\\"wrong value: \\\", wrong_value, type(wrong_value))\\n    found = False\\n    try:\\n        for index, char in enumerate(wrong_value):\\n            if char == \\\".\\\":  # no need to test decimal point -value\\n                continue\\n            slice_of_test_value = len(wrong_value) - (index + 1)\\n            test_value = str(wrong_value[0:slice_of_test_value])\\n            if test_value in notations:\\n                #print(\\\"korjataan: \\\", wrong_value, \\\"=>\\\", test_value)\\n                return test_value\\n    except Exception as e:\\n        print(\\\"wrong value: \\\", wrong_value, \\\" e: \\\", e)\\n        pass\\n    # print(\\\"palautetaan np.nan\\\")\\n    return np.nan\";\n",
       "                var nbb_formatted_code = \"# Fixer function which scrapes \\\"real\\\" part from broken string.\\n# I.e. broken: '59.562209'  -> fixed '59.5622',\\n#      broken: '59.562109*' -> fixed '59.562',\\n#      broken: '89.41038'   -> fixed '89.41'\\ndef fix_class(wrong_value, notations):\\n    wrong_value = wrong_value.split(\\\",\\\")[\\n        0\\n    ]  # take the first value. cell could contain multiple values\\n    # print(\\\"wrong value: \\\", wrong_value, type(wrong_value))\\n    found = False\\n    try:\\n        for index, char in enumerate(wrong_value):\\n            if char == \\\".\\\":  # no need to test decimal point -value\\n                continue\\n            slice_of_test_value = len(wrong_value) - (index + 1)\\n            test_value = str(wrong_value[0:slice_of_test_value])\\n            if test_value in notations:\\n                # print(\\\"korjataan: \\\", wrong_value, \\\"=>\\\", test_value)\\n                return test_value\\n    except Exception as e:\\n        print(\\\"wrong value: \\\", wrong_value, \\\" e: \\\", e)\\n        pass\\n    # print(\\\"palautetaan np.nan\\\")\\n    return np.nan\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fixer function which scrapes \"real\" part from broken string.\n",
    "# I.e. broken: '59.562209'  -> fixed '59.5622',\n",
    "#      broken: '59.562109*' -> fixed '59.562',\n",
    "#      broken: '89.41038'   -> fixed '89.41'\n",
    "def fix_class(wrong_value, notations):\n",
    "    wrong_value = wrong_value.split(\",\")[\n",
    "        0\n",
    "    ]  # take the first value. cell could contain multiple values\n",
    "    # print(\"wrong value: \", wrong_value, type(wrong_value))\n",
    "    found = False\n",
    "    try:\n",
    "        for index, char in enumerate(wrong_value):\n",
    "            if char == \".\":  # no need to test decimal point -value\n",
    "                continue\n",
    "            slice_of_test_value = len(wrong_value) - (index + 1)\n",
    "            test_value = str(wrong_value[0:slice_of_test_value])\n",
    "            if test_value in notations:\n",
    "                # print(\"korjataan: \", wrong_value, \"=>\", test_value)\n",
    "                return test_value\n",
    "    except Exception as e:\n",
    "        print(\"wrong value: \", wrong_value, \" e: \", e)\n",
    "        pass\n",
    "    # print(\"palautetaan np.nan\")\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 58;\n",
       "                var nbb_unformatted_code = \"import re\\n\\n\\ndef remove_form_tags(wrong_value, tags):\\n    # print(\\\"Found tag in: \\\", wrong_value)\\n    for tag in tags:\\n        pattern = tag + \\\"(?!.*\\\" + tag + \\\")\\\"\\n        s = re.search(pattern, wrong_value)\\n        if s:\\n            position = s.start()\\n            if wrong_value[position - 1] == \\\".\\\":\\n                korjaus = wrong_value[\\n                    : s.start() - 1\\n                ]  # remove '.' if it the last character\\n            else:\\n                korjaus = wrong_value[: s.start()]\\n            #print(\\\"korjataan: \\\", wrong_value, \\\"=>\\\", korjaus)\\n            return korjaus\\n    #print(\\\"palautetaan np.nan\\\")\\n    return np.nan\";\n",
       "                var nbb_formatted_code = \"import re\\n\\n\\ndef remove_form_tags(wrong_value, tags):\\n    # print(\\\"Found tag in: \\\", wrong_value)\\n    for tag in tags:\\n        pattern = tag + \\\"(?!.*\\\" + tag + \\\")\\\"\\n        s = re.search(pattern, wrong_value)\\n        if s:\\n            position = s.start()\\n            if wrong_value[position - 1] == \\\".\\\":\\n                korjaus = wrong_value[\\n                    : s.start() - 1\\n                ]  # remove '.' if it the last character\\n            else:\\n                korjaus = wrong_value[: s.start()]\\n            # print(\\\"korjataan: \\\", wrong_value, \\\"=>\\\", korjaus)\\n            return korjaus\\n    # print(\\\"palautetaan np.nan\\\")\\n    return np.nan\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_form_tags(wrong_value, tags):\n",
    "    # print(\"Found tag in: \", wrong_value)\n",
    "    for tag in tags:\n",
    "        pattern = tag + \"(?!.*\" + tag + \")\"\n",
    "        s = re.search(pattern, wrong_value)\n",
    "        if s:\n",
    "            position = s.start()\n",
    "            if wrong_value[position - 1] == \".\":\n",
    "                korjaus = wrong_value[\n",
    "                    : s.start() - 1\n",
    "                ]  # remove '.' if it the last character\n",
    "            else:\n",
    "                korjaus = wrong_value[: s.start()]\n",
    "            # print(\"korjataan: \", wrong_value, \"=>\", korjaus)\n",
    "            return korjaus\n",
    "    # print(\"palautetaan np.nan\")\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 118;\n",
       "                var nbb_unformatted_code = \"# Trim string values of all feature columns (084, 092, 093, 094 and 095)\\ndf_obj = df1.select_dtypes([\\\"object\\\"])\\ndf1[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\";\n",
       "                var nbb_formatted_code = \"# Trim string values of all feature columns (084, 092, 093, 094 and 095)\\ndf_obj = df1.select_dtypes([\\\"object\\\"])\\ndf1[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trim string values of all feature columns (084, 092, 093, 094 and 095)\n",
    "df_obj = df1.select_dtypes([\"object\"])\n",
    "df1[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['084', '092', '093', '094']\n",
      "           record_id     084     092  093      094       095  \\\n",
      "31      420907795064  78.893     NaN  NaN  78.8936    788.48   \n",
      "60      420907795142  67.451  67.451  NaN      NaN    654.91   \n",
      "76      420907795172    99.1    99.1  NaN     99.1     990.1   \n",
      "94      420907795208    79.8    79.8  NaN      NaN     797.2   \n",
      "98      420907795215      65      65  NaN       65       798   \n",
      "...              ...     ...     ...  ...      ...       ...   \n",
      "602133  420909003406     NaN     NaN  NaN      NaN   024.409   \n",
      "616244  420909025166     NaN     NaN  NaN      NaN    384.09   \n",
      "730995  420909182410     NaN     NaN  NaN      NaN    004.09   \n",
      "753606  420909206159     NaN     NaN  NaN      NaN  384.4109   \n",
      "793315  420909247126     NaN     NaN  NaN      NaN   384.709   \n",
      "\n",
      "                                                      650  \n",
      "31      tangot,viihdemusiikki,latinalaisamerikkalainen...  \n",
      "60             koira,rodut,lemmikkieläimet,dalmatiankoira  \n",
      "76                               henkilöhistoria,historia  \n",
      "94       pelit,leikit,seuraleikit,ulkoleikit,joukkuepelit  \n",
      "98                  askartelu,käsityöt,ruokaohjeet,lapset  \n",
      "...                                                   ...  \n",
      "602133  kirjastopalvelut,erikoiskirjastot,näkövammaise...  \n",
      "616244  media,viestintä,joukkoviestintä,joukkoviestime...  \n",
      "730995  sanomalehdet,lehtikustantajat,lehdistö,henkilö...  \n",
      "753606  yleisradioyhtiöt,uutisvälitys,uutiset,suomenki...  \n",
      "793315  lehti-ilmoitukset,sanomalehdet,kontakti-ilmoit...  \n",
      "\n",
      "[126347 rows x 7 columns]\n",
      "----\n",
      "1864\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 121;\n",
       "                var nbb_unformatted_code = \"# YKL hierarchy\\ndf_ykl = pd.read_csv(\\n    \\\"data/raw_data/ykl_simple.csv\\\",\\n    dtype={\\\"notation\\\": str, \\\"prefLabel\\\": str},\\n)\\n\\n# YKL form tags\\ndf_ykl_ml = pd.read_csv(\\n    \\\"data/raw_data/ykl_ml.csv\\\",\\n    dtype={\\\"notation\\\": str, \\\"prefLabel\\\": str},\\n)\\n# print(df_ykl_ml.info())\\n\\n# Read notations (unique 084-classes) into list\\nnotations = df_ykl[\\\"notation\\\"]\\n# NaNs are valid \\\"correct\\\" values in this syntax -> append it to notations Series\\nnotations = notations.append(pd.Series([Nan], index=[2384]))\\n# Convert df to list\\nnotations = notations.values.tolist()\\n\\n# Select preferred columns for cleaning purposes(084, 092, 093 and 094)\\ncolumns = list(df1)[1:-2]\\nprint(columns)\\n\\n# form_tags = df_ykl_ml[\\\"notation\\\"].str.cat(sep=\\\"|\\\")\\nform_tags = df_ykl_ml[\\\"notation\\\"].values.tolist()  # .wrap(width=10)  # .str.join(\\\",\\\")\\ntags = form_tags\\n# Create regex pattern for class-value to test if class is 'form tag'\\npattern = \\\"(?:\\\\d{2}\\\\.\\\\d*\\\"\\nform_tags = \\\"|\\\".join([pattern + \\\"{}$)\\\".format(value) for value in form_tags])\\n# print(form_tags)\\n\\n# Iterate over selected columns\\nfor i in columns:\\n    # print(i, type(i))\\n    df_split1A = df1[df1[i].isin(notations)]\\n    df_split1B = df1[~df1[i].isin(notations)]\\n    df_split2 = df_split1B[df_split1B[i].str.fullmatch(form_tags)]\\n    # df_incorrect_ml = df_incorrect[~df_incorrect[i].str.fullmatch(form_tags)]\\n    # print(df_correct.shape[0])\\n    # print(df_incorrect.shape[0])\\n    # print(df_correct_ml.shape[0])\\n    # print(df_incorrect_ml.shape[0])\\n    # print(df_correct_ml)\\n    # print(df_incorrect_ml.head(10))\\n\\n    # Remove \\\"form tags\\\"\\n    df_split2.loc[:, (i)] = df_split2.apply(\\n        lambda x: remove_form_tags(x[i], tags), axis=1\\n    )\\n\\n    df_split3A = df_split2[df_split2[i].isin(notations)]\\n    df_split3B = df_split2[~df_split2[i].isin(notations)]\\n\\n    # Fix the remaining ones\\n    df_split3B.loc[:, (i)] = df_split3B.apply(\\n        lambda x: fix_class(x[i], notations), axis=1\\n    )\\n\\n    # Merge splitted data\\n    df1 = pd.concat([df_split1A, df_split3A, df_split3B], axis=0)\\n\\nprint(df1)\\nprint(\\\"----\\\")\\nprint(len(df1[\\\"084\\\"].unique()))\";\n",
       "                var nbb_formatted_code = \"# YKL hierarchy\\ndf_ykl = pd.read_csv(\\n    \\\"data/raw_data/ykl_simple.csv\\\",\\n    dtype={\\\"notation\\\": str, \\\"prefLabel\\\": str},\\n)\\n\\n# YKL form tags\\ndf_ykl_ml = pd.read_csv(\\n    \\\"data/raw_data/ykl_ml.csv\\\",\\n    dtype={\\\"notation\\\": str, \\\"prefLabel\\\": str},\\n)\\n# print(df_ykl_ml.info())\\n\\n# Read notations (unique 084-classes) into list\\nnotations = df_ykl[\\\"notation\\\"]\\n# NaNs are valid \\\"correct\\\" values in this syntax -> append it to notations Series\\nnotations = notations.append(pd.Series([Nan], index=[2384]))\\n# Convert df to list\\nnotations = notations.values.tolist()\\n\\n# Select preferred columns for cleaning purposes(084, 092, 093 and 094)\\ncolumns = list(df1)[1:-2]\\nprint(columns)\\n\\n# form_tags = df_ykl_ml[\\\"notation\\\"].str.cat(sep=\\\"|\\\")\\nform_tags = df_ykl_ml[\\\"notation\\\"].values.tolist()  # .wrap(width=10)  # .str.join(\\\",\\\")\\ntags = form_tags\\n# Create regex pattern for class-value to test if class is 'form tag'\\npattern = \\\"(?:\\\\d{2}\\\\.\\\\d*\\\"\\nform_tags = \\\"|\\\".join([pattern + \\\"{}$)\\\".format(value) for value in form_tags])\\n# print(form_tags)\\n\\n# Iterate over selected columns\\nfor i in columns:\\n    # print(i, type(i))\\n    df_split1A = df1[df1[i].isin(notations)]\\n    df_split1B = df1[~df1[i].isin(notations)]\\n    df_split2 = df_split1B[df_split1B[i].str.fullmatch(form_tags)]\\n    # df_incorrect_ml = df_incorrect[~df_incorrect[i].str.fullmatch(form_tags)]\\n    # print(df_correct.shape[0])\\n    # print(df_incorrect.shape[0])\\n    # print(df_correct_ml.shape[0])\\n    # print(df_incorrect_ml.shape[0])\\n    # print(df_correct_ml)\\n    # print(df_incorrect_ml.head(10))\\n\\n    # Remove \\\"form tags\\\"\\n    df_split2.loc[:, (i)] = df_split2.apply(\\n        lambda x: remove_form_tags(x[i], tags), axis=1\\n    )\\n\\n    df_split3A = df_split2[df_split2[i].isin(notations)]\\n    df_split3B = df_split2[~df_split2[i].isin(notations)]\\n\\n    # Fix the remaining ones\\n    df_split3B.loc[:, (i)] = df_split3B.apply(\\n        lambda x: fix_class(x[i], notations), axis=1\\n    )\\n\\n    # Merge splitted data\\n    df1 = pd.concat([df_split1A, df_split3A, df_split3B], axis=0)\\n\\nprint(df1)\\nprint(\\\"----\\\")\\nprint(len(df1[\\\"084\\\"].unique()))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YKL hierarchy\n",
    "df_ykl = pd.read_csv(\n",
    "    \"data/raw_data/ykl_simple.csv\",\n",
    "    dtype={\"notation\": str, \"prefLabel\": str},\n",
    ")\n",
    "\n",
    "# YKL form tags\n",
    "df_ykl_ml = pd.read_csv(\n",
    "    \"data/raw_data/ykl_ml.csv\",\n",
    "    dtype={\"notation\": str, \"prefLabel\": str},\n",
    ")\n",
    "# print(df_ykl_ml.info())\n",
    "\n",
    "# Read notations (unique 084-classes) into list\n",
    "notations = df_ykl[\"notation\"]\n",
    "# NaNs are valid \"correct\" values in this syntax -> append it to notations Series\n",
    "notations = notations.append(pd.Series([Nan], index=[2384]))\n",
    "# Convert df to list\n",
    "notations = notations.values.tolist()\n",
    "\n",
    "# Select preferred columns for cleaning purposes(084, 092, 093 and 094)\n",
    "columns = list(df1)[1:-2]\n",
    "print(columns)\n",
    "\n",
    "# form_tags = df_ykl_ml[\"notation\"].str.cat(sep=\"|\")\n",
    "form_tags = df_ykl_ml[\"notation\"].values.tolist()  # .wrap(width=10)  # .str.join(\",\")\n",
    "tags = form_tags\n",
    "# Create regex pattern for class-value to test if class is 'form tag'\n",
    "pattern = \"(?:\\d{2}\\.\\d*\"\n",
    "form_tags = \"|\".join([pattern + \"{}$)\".format(value) for value in form_tags])\n",
    "# print(form_tags)\n",
    "\n",
    "# Iterate over selected columns\n",
    "for i in columns:\n",
    "    # print(i, type(i))\n",
    "    df_split1A = df1[df1[i].isin(notations)]\n",
    "    df_split1B = df1[~df1[i].isin(notations)]\n",
    "    df_split2 = df_split1B[df_split1B[i].str.fullmatch(form_tags)]\n",
    "    # df_incorrect_ml = df_incorrect[~df_incorrect[i].str.fullmatch(form_tags)]\n",
    "    # print(df_correct.shape[0])\n",
    "    # print(df_incorrect.shape[0])\n",
    "    # print(df_correct_ml.shape[0])\n",
    "    # print(df_incorrect_ml.shape[0])\n",
    "    # print(df_correct_ml)\n",
    "    # print(df_incorrect_ml.head(10))\n",
    "\n",
    "    # Remove \"form tags\"\n",
    "    df_split2.loc[:, (i)] = df_split2.apply(\n",
    "        lambda x: remove_form_tags(x[i], tags), axis=1\n",
    "    )\n",
    "\n",
    "    df_split3A = df_split2[df_split2[i].isin(notations)]\n",
    "    df_split3B = df_split2[~df_split2[i].isin(notations)]\n",
    "\n",
    "    # Fix the remaining ones\n",
    "    df_split3B.loc[:, (i)] = df_split3B.apply(\n",
    "        lambda x: fix_class(x[i], notations), axis=1\n",
    "    )\n",
    "\n",
    "    # Merge splitted data\n",
    "    df1 = pd.concat([df_split1A, df_split3A, df_split3B], axis=0)\n",
    "\n",
    "print(df1)\n",
    "print(\"----\")\n",
    "print(len(df1[\"084\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2055\n",
      "(?:\\d{3}\\.\\d*03$)|(?:\\d{3}\\.\\d*05$)|(?:\\d{3}\\.\\d*09$)|(?:\\d{3}\\.\\d*081$)|(?:\\d{3}\\.\\d*083$)|(?:\\d{3}\\.\\d*084$)\n",
      "-------------\n",
      "After splitting\n",
      "       record_id     084     092  093      094     095  \\\n",
      "31  420907795064  78.893     NaN  NaN  78.8936  788.48   \n",
      "60  420907795142  67.451  67.451  NaN      NaN  654.91   \n",
      "76  420907795172    99.1    99.1  NaN     99.1   990.1   \n",
      "94  420907795208    79.8    79.8  NaN      NaN   797.2   \n",
      "98  420907795215      65      65  NaN       65     798   \n",
      "\n",
      "                                                  650  \n",
      "31  tangot,viihdemusiikki,latinalaisamerikkalainen...  \n",
      "60         koira,rodut,lemmikkieläimet,dalmatiankoira  \n",
      "76                           henkilöhistoria,historia  \n",
      "94   pelit,leikit,seuraleikit,ulkoleikit,joukkuepelit  \n",
      "98              askartelu,käsityöt,ruokaohjeet,lapset  \n",
      "119912\n",
      "0\n",
      "0\n",
      "-------------\n",
      "Tulostetaan\n",
      "Empty DataFrame\n",
      "Columns: [record_id, 084, 092, 093, 094, 095, 650]\n",
      "Index: []\n",
      "----\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 119912 entries, 31 to 6909\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   record_id  112981 non-null  float64\n",
      " 1   084        112578 non-null  object \n",
      " 2   092        67246 non-null   object \n",
      " 3   093        23541 non-null   object \n",
      " 4   094        71372 non-null   object \n",
      " 5   095        119891 non-null  object \n",
      " 6   650        107091 non-null  object \n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 7.3+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 112817 entries, 31 to 273007\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   record_id  112817 non-null  int64 \n",
      " 1   084        112817 non-null  object\n",
      " 2   092        67344 non-null   object\n",
      " 3   093        23595 non-null   object\n",
      " 4   094        71484 non-null   object\n",
      " 5   095        112796 non-null  object\n",
      " 6   650        106927 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 6.9+ MB\n",
      "None\n",
      "----\n",
      "1790\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 115;\n",
       "                var nbb_unformatted_code = \"# Read HKLJ classes from csv-file\\ndf_hklj = pd.read_csv(\\n    \\\"data/raw_data/hklj_simple.csv\\\",\\n    dtype={\\\"notation\\\": str, \\\"prefLabel\\\": str},\\n)\\n\\n# Read HKLJ form tags from csv-file\\ndf_hklj_ml = pd.read_csv(\\n    \\\"data/raw_data/hklj_ml.csv\\\",\\n    dtype={\\\"notation\\\": str, \\\"prefLabel\\\": str},\\n)\\n\\n# Read notations (unique 084-classes) into list\\nnotations = df_hklj[\\\"notation\\\"]\\nprint(len(notations))\\n\\n# NaNs are valid \\\"correct\\\" values in this syntax -> append it to notations Series\\n# notations = notations.append(pd.Series([Nan], index=[2232]))\\nnotations = notations.append(pd.Series([Nan], index=[2055]))\\n# Convert df to list\\nnotations = notations.values.tolist()\\n# print(notations)\\n\\n# form_tags = df_hklj_ml[\\\"notation\\\"].str.cat(sep=\\\"|\\\")\\nform_tags = df_hklj_ml[\\\"notation\\\"].values.tolist()  # .wrap(width=10)  # .str.join(\\\",\\\")\\ntags = form_tags\\n\\n# Create regex pattern for class-value to test if class is 'form tag'\\npattern = \\\"(?:\\\\d{3}\\\\.\\\\d*\\\"\\nform_tags = \\\"|\\\".join([pattern + \\\"{}$)\\\".format(value) for value in form_tags])\\nprint(form_tags)\\n\\n# Split data to separate dataframes\\ndf_split1A = df1[df1[\\\"095\\\"].isin(notations)]\\ndf_split1B = df1[~df1[\\\"095\\\"].isin(notations)]\\ndf_split2 = df_split1B[df_split1B[\\\"095\\\"].str.fullmatch(form_tags)]\\n# df_incorrect_ml = df_incorrect[~df_incorrect[\\\"095\\\"].str.fullmatch(form_tags)]\\n\\nprint(\\\"-------------\\\")\\nprint(\\\"After splitting\\\")\\nprint(df_correct.head())\\nprint(len(df_split1A))\\nprint(len(df_split1B))\\nprint(len(df_split2))\\n\\n# print(len(df_incorrect_ml))\\nprint(\\\"-------------\\\")\\n# print(df_incorrect.head())\\n# print(\\\"-------------\\\")\\n\\n# Remove \\\"form tags\\\"\\ndf_split2.loc[:, (\\\"095\\\")] = df_split2.apply(\\n    lambda x: remove_form_tags(x[\\\"095\\\"], tags), axis=1\\n)\\n\\ndf_split3A = df_split2[df_split2[\\\"095\\\"].isin(notations)]\\ndf_split3B = df_split2[~df_split2[\\\"095\\\"].isin(notations)]\\n\\n# Fix the remaining ones\\ndf_split3B.loc[:, (\\\"095\\\")] = df_split3B.apply(\\n    lambda x: fix_class(x[\\\"095\\\"], notations), axis=1\\n)\\n\\n# print(len(df_split3A))\\n# print(len(df_split3B))\\n\\n# Merge splitted data\\n# df_095 = pd.concat([df_split1A, df_split3A, df_split3B], axis=0)\\ndf1 = pd.concat([df_split1A, df_split3A, df_split3B], axis=0)\\nprint(\\\"Tulostetaan\\\")\\nprint(df1[df1[\\\"095\\\"] == \\\"41.0\\\"])\\n# print(df1[df1[\\\"095\\\"] == \\\"788.48\\\"])\\nprint(\\\"----\\\")\\n\\nprint(df1.info())\\nprint(df_095.info())\\nprint(\\\"----\\\")\\nprint(len(df_095[\\\"095\\\"].unique()))\";\n",
       "                var nbb_formatted_code = \"# Read HKLJ classes from csv-file\\ndf_hklj = pd.read_csv(\\n    \\\"data/raw_data/hklj_simple.csv\\\",\\n    dtype={\\\"notation\\\": str, \\\"prefLabel\\\": str},\\n)\\n\\n# Read HKLJ form tags from csv-file\\ndf_hklj_ml = pd.read_csv(\\n    \\\"data/raw_data/hklj_ml.csv\\\",\\n    dtype={\\\"notation\\\": str, \\\"prefLabel\\\": str},\\n)\\n\\n# Read notations (unique 084-classes) into list\\nnotations = df_hklj[\\\"notation\\\"]\\nprint(len(notations))\\n\\n# NaNs are valid \\\"correct\\\" values in this syntax -> append it to notations Series\\n# notations = notations.append(pd.Series([Nan], index=[2232]))\\nnotations = notations.append(pd.Series([Nan], index=[2055]))\\n# Convert df to list\\nnotations = notations.values.tolist()\\n# print(notations)\\n\\n# form_tags = df_hklj_ml[\\\"notation\\\"].str.cat(sep=\\\"|\\\")\\nform_tags = df_hklj_ml[\\\"notation\\\"].values.tolist()  # .wrap(width=10)  # .str.join(\\\",\\\")\\ntags = form_tags\\n\\n# Create regex pattern for class-value to test if class is 'form tag'\\npattern = \\\"(?:\\\\d{3}\\\\.\\\\d*\\\"\\nform_tags = \\\"|\\\".join([pattern + \\\"{}$)\\\".format(value) for value in form_tags])\\nprint(form_tags)\\n\\n# Split data to separate dataframes\\ndf_split1A = df1[df1[\\\"095\\\"].isin(notations)]\\ndf_split1B = df1[~df1[\\\"095\\\"].isin(notations)]\\ndf_split2 = df_split1B[df_split1B[\\\"095\\\"].str.fullmatch(form_tags)]\\n# df_incorrect_ml = df_incorrect[~df_incorrect[\\\"095\\\"].str.fullmatch(form_tags)]\\n\\nprint(\\\"-------------\\\")\\nprint(\\\"After splitting\\\")\\nprint(df_correct.head())\\nprint(len(df_split1A))\\nprint(len(df_split1B))\\nprint(len(df_split2))\\n\\n# print(len(df_incorrect_ml))\\nprint(\\\"-------------\\\")\\n# print(df_incorrect.head())\\n# print(\\\"-------------\\\")\\n\\n# Remove \\\"form tags\\\"\\ndf_split2.loc[:, (\\\"095\\\")] = df_split2.apply(\\n    lambda x: remove_form_tags(x[\\\"095\\\"], tags), axis=1\\n)\\n\\ndf_split3A = df_split2[df_split2[\\\"095\\\"].isin(notations)]\\ndf_split3B = df_split2[~df_split2[\\\"095\\\"].isin(notations)]\\n\\n# Fix the remaining ones\\ndf_split3B.loc[:, (\\\"095\\\")] = df_split3B.apply(\\n    lambda x: fix_class(x[\\\"095\\\"], notations), axis=1\\n)\\n\\n# print(len(df_split3A))\\n# print(len(df_split3B))\\n\\n# Merge splitted data\\n# df_095 = pd.concat([df_split1A, df_split3A, df_split3B], axis=0)\\ndf1 = pd.concat([df_split1A, df_split3A, df_split3B], axis=0)\\nprint(\\\"Tulostetaan\\\")\\nprint(df1[df1[\\\"095\\\"] == \\\"41.0\\\"])\\n# print(df1[df1[\\\"095\\\"] == \\\"788.48\\\"])\\nprint(\\\"----\\\")\\n\\nprint(df1.info())\\nprint(df_095.info())\\nprint(\\\"----\\\")\\nprint(len(df_095[\\\"095\\\"].unique()))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read HKLJ classes from csv-file\n",
    "df_hklj = pd.read_csv(\n",
    "    \"data/raw_data/hklj_simple.csv\",\n",
    "    dtype={\"notation\": str, \"prefLabel\": str},\n",
    ")\n",
    "\n",
    "# Read HKLJ form tags from csv-file\n",
    "df_hklj_ml = pd.read_csv(\n",
    "    \"data/raw_data/hklj_ml.csv\",\n",
    "    dtype={\"notation\": str, \"prefLabel\": str},\n",
    ")\n",
    "\n",
    "# Read notations (unique 084-classes) into list\n",
    "notations = df_hklj[\"notation\"]\n",
    "print(len(notations))\n",
    "\n",
    "# NaNs are valid \"correct\" values in this syntax -> append it to notations Series\n",
    "# notations = notations.append(pd.Series([Nan], index=[2232]))\n",
    "notations = notations.append(pd.Series([Nan], index=[2055]))\n",
    "# Convert df to list\n",
    "notations = notations.values.tolist()\n",
    "# print(notations)\n",
    "\n",
    "# form_tags = df_hklj_ml[\"notation\"].str.cat(sep=\"|\")\n",
    "form_tags = df_hklj_ml[\"notation\"].values.tolist()  # .wrap(width=10)  # .str.join(\",\")\n",
    "tags = form_tags\n",
    "\n",
    "# Create regex pattern for class-value to test if class is 'form tag'\n",
    "pattern = \"(?:\\d{3}\\.\\d*\"\n",
    "form_tags = \"|\".join([pattern + \"{}$)\".format(value) for value in form_tags])\n",
    "print(form_tags)\n",
    "\n",
    "# Split data to separate dataframes\n",
    "df_split1A = df1[df1[\"095\"].isin(notations)]\n",
    "df_split1B = df1[~df1[\"095\"].isin(notations)]\n",
    "df_split2 = df_split1B[df_split1B[\"095\"].str.fullmatch(form_tags)]\n",
    "# df_incorrect_ml = df_incorrect[~df_incorrect[\"095\"].str.fullmatch(form_tags)]\n",
    "\n",
    "print(\"-------------\")\n",
    "print(\"After splitting\")\n",
    "print(df_correct.head())\n",
    "print(len(df_split1A))\n",
    "print(len(df_split1B))\n",
    "print(len(df_split2))\n",
    "\n",
    "# print(len(df_incorrect_ml))\n",
    "print(\"-------------\")\n",
    "# print(df_incorrect.head())\n",
    "# print(\"-------------\")\n",
    "\n",
    "# Remove \"form tags\"\n",
    "df_split2.loc[:, (\"095\")] = df_split2.apply(\n",
    "    lambda x: remove_form_tags(x[\"095\"], tags), axis=1\n",
    ")\n",
    "\n",
    "df_split3A = df_split2[df_split2[\"095\"].isin(notations)]\n",
    "df_split3B = df_split2[~df_split2[\"095\"].isin(notations)]\n",
    "\n",
    "# Fix the remaining ones\n",
    "df_split3B.loc[:, (\"095\")] = df_split3B.apply(\n",
    "    lambda x: fix_class(x[\"095\"], notations), axis=1\n",
    ")\n",
    "\n",
    "# print(len(df_split3A))\n",
    "# print(len(df_split3B))\n",
    "\n",
    "# Merge splitted data\n",
    "# df_095 = pd.concat([df_split1A, df_split3A, df_split3B], axis=0)\n",
    "df1 = pd.concat([df_split1A, df_split3A, df_split3B], axis=0)\n",
    "print(\"Tulostetaan\")\n",
    "print(df1[df1[\"095\"] == \"41.0\"])\n",
    "# print(df1[df1[\"095\"] == \"788.48\"])\n",
    "print(\"----\")\n",
    "\n",
    "print(df1.info())\n",
    "print(df_095.info())\n",
    "print(\"----\")\n",
    "print(len(df_095[\"095\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 119912 entries, 31 to 6909\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   record_id  112981 non-null  float64\n",
      " 1   084        112578 non-null  object \n",
      " 2   092        67246 non-null   object \n",
      " 3   093        23541 non-null   object \n",
      " 4   094        71372 non-null   object \n",
      " 5   095        119891 non-null  object \n",
      " 6   650        107091 non-null  object \n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 7.3+ MB\n",
      "None\n",
      "----\n",
      "1786\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 112578 entries, 31 to 564105\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   record_id  112578 non-null  float64\n",
      " 1   084        112578 non-null  object \n",
      " 2   092        67184 non-null   object \n",
      " 3   093        23518 non-null   object \n",
      " 4   094        71297 non-null   object \n",
      " 5   095        112578 non-null  object \n",
      " 6   650        106689 non-null  object \n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 6.9+ MB\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 116;\n",
       "                var nbb_unformatted_code = \"df2 = df1[:]\\n\\n# Replace empty strings with np.nan objects\\n# df2 = df2[\\\"084\\\"].replace(\\\"\\\", np.nan, inplace=False)\\ndf2 = df2[df2[\\\"084\\\"].astype(bool)]\\nprint(df2.info())\\n# Filter again null values\\ndf2 = df2[((df2[\\\"084\\\"].notnull()) & (df2[\\\"095\\\"].notnull()))]\\n# df1 = df1[df1[\\\"084\\\"].notnull()]\\n# & (df_raw_data[\\\"095\\\"].notnull()))]\\n\\nprint(\\\"----\\\")\\nprint(len(df2[\\\"095\\\"].unique()))\\n\\n# print(df1.head(10))\\n# df1.head()\\ndf2.info()\\ndf2.to_csv(\\\"data_output.csv\\\")\";\n",
       "                var nbb_formatted_code = \"df2 = df1[:]\\n\\n# Replace empty strings with np.nan objects\\n# df2 = df2[\\\"084\\\"].replace(\\\"\\\", np.nan, inplace=False)\\ndf2 = df2[df2[\\\"084\\\"].astype(bool)]\\nprint(df2.info())\\n# Filter again null values\\ndf2 = df2[((df2[\\\"084\\\"].notnull()) & (df2[\\\"095\\\"].notnull()))]\\n# df1 = df1[df1[\\\"084\\\"].notnull()]\\n# & (df_raw_data[\\\"095\\\"].notnull()))]\\n\\nprint(\\\"----\\\")\\nprint(len(df2[\\\"095\\\"].unique()))\\n\\n# print(df1.head(10))\\n# df1.head()\\ndf2.info()\\ndf2.to_csv(\\\"data_output.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2 = df1[:]\n",
    "\n",
    "# Replace empty strings with np.nan objects\n",
    "# df2 = df2[\"084\"].replace(\"\", np.nan, inplace=False)\n",
    "df2 = df2[df2[\"084\"].astype(bool)]\n",
    "print(df2.info())\n",
    "# Filter again null values\n",
    "df2 = df2[((df2[\"084\"].notnull()) & (df2[\"095\"].notnull()))]\n",
    "# df1 = df1[df1[\"084\"].notnull()]\n",
    "# & (df_raw_data[\"095\"].notnull()))]\n",
    "\n",
    "print(\"----\")\n",
    "print(len(df2[\"095\"].unique()))\n",
    "\n",
    "# print(df1.head(10))\n",
    "# df1.head()\n",
    "df2.info()\n",
    "df2.to_csv(\"data_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 123;\n",
       "                var nbb_unformatted_code = \"import pandas_schema\\nfrom pandas_schema import Column, Schema\\nfrom pandas_schema.validation import (\\n    CustomElementValidation,\\n    LeadingWhitespaceValidation,\\n    TrailingWhitespaceValidation,\\n    CanConvertValidation,\\n    MatchesPatternValidation,\\n    InRangeValidation,\\n    InListValidation,\\n)\\n\\nnull_validation = [\\n    CustomElementValidation(lambda d: d is not np.nan, \\\"this field cannot be null\\\")\\n]\\n\\nschema = Schema(\\n    [\\n        Column(\\\"record_id\\\", null_validation),\\n        Column(\\n            \\\"084\\\",\\n            [\\n                LeadingWhitespaceValidation(),\\n                TrailingWhitespaceValidation(),\\n                MatchesPatternValidation(r\\\"^(?:\\\\S)|\\\\d{1,2}(?:$|.\\\\d{0,}$)\\\"),\\n                # MatchesPatternValidation(r\\\"(?:\\\\d{2}\\\\.\\\\d*$)\\\"),\\n            ],\\n        ),\\n        Column(\\\"092\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n        Column(\\\"093\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n        Column(\\\"094\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n        Column(\\\"095\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n        Column(\\\"650\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n    ]\\n)\\n\\nerrors = schema.validate(df2)\\nprint(len(errors))\\nfor error in errors[:15]:\\n    print(error)\\n# print(df2.loc[640, [\\\"record_id\\\", \\\"084\\\", \\\"650\\\"]])\";\n",
       "                var nbb_formatted_code = \"import pandas_schema\\nfrom pandas_schema import Column, Schema\\nfrom pandas_schema.validation import (\\n    CustomElementValidation,\\n    LeadingWhitespaceValidation,\\n    TrailingWhitespaceValidation,\\n    CanConvertValidation,\\n    MatchesPatternValidation,\\n    InRangeValidation,\\n    InListValidation,\\n)\\n\\nnull_validation = [\\n    CustomElementValidation(lambda d: d is not np.nan, \\\"this field cannot be null\\\")\\n]\\n\\nschema = Schema(\\n    [\\n        Column(\\\"record_id\\\", null_validation),\\n        Column(\\n            \\\"084\\\",\\n            [\\n                LeadingWhitespaceValidation(),\\n                TrailingWhitespaceValidation(),\\n                MatchesPatternValidation(r\\\"^(?:\\\\S)|\\\\d{1,2}(?:$|.\\\\d{0,}$)\\\"),\\n                # MatchesPatternValidation(r\\\"(?:\\\\d{2}\\\\.\\\\d*$)\\\"),\\n            ],\\n        ),\\n        Column(\\\"092\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n        Column(\\\"093\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n        Column(\\\"094\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n        Column(\\\"095\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n        Column(\\\"650\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n    ]\\n)\\n\\nerrors = schema.validate(df2)\\nprint(len(errors))\\nfor error in errors[:15]:\\n    print(error)\\n# print(df2.loc[640, [\\\"record_id\\\", \\\"084\\\", \\\"650\\\"]])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas_schema\n",
    "from pandas_schema import Column, Schema\n",
    "from pandas_schema.validation import (\n",
    "    CustomElementValidation,\n",
    "    LeadingWhitespaceValidation,\n",
    "    TrailingWhitespaceValidation,\n",
    "    CanConvertValidation,\n",
    "    MatchesPatternValidation,\n",
    "    InRangeValidation,\n",
    "    InListValidation,\n",
    ")\n",
    "\n",
    "null_validation = [\n",
    "    CustomElementValidation(lambda d: d is not np.nan, \"this field cannot be null\")\n",
    "]\n",
    "\n",
    "schema = Schema(\n",
    "    [\n",
    "        Column(\"record_id\", null_validation),\n",
    "        Column(\n",
    "            \"084\",\n",
    "            [\n",
    "                LeadingWhitespaceValidation(),\n",
    "                TrailingWhitespaceValidation(),\n",
    "                MatchesPatternValidation(r\"^(?:\\S)|\\d{1,2}(?:$|.\\d{0,}$)\"),\n",
    "                # MatchesPatternValidation(r\"(?:\\d{2}\\.\\d*$)\"),\n",
    "            ],\n",
    "        ),\n",
    "        Column(\"092\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\n",
    "        Column(\"093\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\n",
    "        Column(\"094\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\n",
    "        Column(\"095\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\n",
    "        Column(\"650\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "errors = schema.validate(df2)\n",
    "print(len(errors))\n",
    "for error in errors[:15]:\n",
    "    print(error)\n",
    "# print(df2.loc[640, [\"record_id\", \"084\", \"650\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT USE THIS CELL - IT IS NOT WORKING!\n",
    "\n",
    "##df2 = df1[:]\n",
    "# df2 = df_cut[:]\n",
    "# df2 = df_uncut[:]\n",
    "df2 = df_F[:]\n",
    "# df2['084'] = pd.to_numeric(df2[\"084\"], errors='coerce')\n",
    "# df2['084'] = df2['084'].astype(str)\n",
    "c1 = df2.sort_values(by=[\"084\"])\n",
    "c2 = c1[\"084\"].unique()\n",
    "# c1.sort_values()\n",
    "# \"\"\"\n",
    "print(len(c2))\n",
    "# print(c2)\n",
    "# \"\"\"\n",
    "\n",
    "print(\"NV: \", notations.values)\n",
    "\"\"\"\n",
    "for row in c2[:10]:\n",
    "    # print(type(row))\n",
    "    if row not in notations.values:\n",
    "        print(row)\n",
    "\"\"\"\n",
    "\n",
    "c3 = c1[~c1[\"084\"].isin(notations)]\n",
    "\n",
    "print(c3[\"084\"].isnull().sum())\n",
    "\n",
    "measurer = np.vectorize(len)\n",
    "res1 = measurer(df2.values.astype(str)).max(axis=0)\n",
    "print(\"Measures: \", res1)\n",
    "\n",
    "df2 = df2[(df2[\"084\"].notnull())]\n",
    "\n",
    "df2[\"084_length\"] = df2[\"084\"].str.len()\n",
    "# s = df2[\"084\"].str.len().sort_values()\n",
    "# df2.sort_values(\"084_length\", ascending=True, inplace=True)\n",
    "# print(df2.head(2000))\n",
    "\n",
    "luokat_lkm = df2[\"084\"].value_counts().nlargest(10)\n",
    "print(luokat_lkm)\n",
    "luokat_lkm.plot(kind=\"bar\")\n",
    "\n",
    "\"\"\"\n",
    "# s_temp = luokat_lkm\n",
    "s_temp = df2[\"084_length\"].sort_values(ascending=False)\n",
    "\n",
    "# Count how many rows are not in the top ten\n",
    "not_top_ten = len(s_temp) - 10\n",
    "print(not_top_ten)\n",
    "\n",
    "# Sum the values not in the top ten\n",
    "not_top_ten_sum = s_temp.tail(not_top_ten).sum()\n",
    "print(not_top_ten_sum)\n",
    "\n",
    "# Get the top ten values\n",
    "s_top = s_temp.head(10)\n",
    "# s_top = s_temp[:10]\n",
    "print(s_top)\n",
    "\n",
    "# Append the sum of not-top-ten values to the Series\n",
    "s_top[10] = not_top_ten_sum\n",
    "# s_top.append(not_top_ten_sum)\n",
    "\n",
    "# Plot pie chart\n",
    "_ = s_top.plot.pie()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# hist = df2.hist(column=\"084_length\", bins=7)\n",
    "# hist = luokat_lkm.hist()\n",
    "# plot = luokat_lkm.plot.pie()\n",
    "\n",
    "# df2.to_csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all distinct keywoards into an array\n",
    "keywords = []\n",
    "# for i in range(len(df1[:10])):\n",
    "for index, row in df1.iterrows():\n",
    "    # print(i)\n",
    "    # k = df1.loc[i+1, \"650\"]\n",
    "    try:\n",
    "        kwords = row[\"650\"].split(\",\")\n",
    "    except Exception as e:\n",
    "        # print(row[\"650\"])\n",
    "        kwords = None\n",
    "    if kwords:\n",
    "        for k in kwords:\n",
    "            if k not in keywords:\n",
    "                keywords.append(k)\n",
    "        # print(index, row[\"650\"])\n",
    "    # if index > 500000:\n",
    "    #    break\n",
    "print(\"---\")\n",
    "print(keywords)\n",
    "# print(len(keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, all columns that contain 'object' variables, contain elements that can not be described in sigle datatype.\n",
    "These may be nested structures or mixed datatypes. We saw earlier that there were a lot of question marks in the dataset.\n",
    "Let's try to replace these with np.nan, meaning a missing value, and then converting the columns with 'object' datatype to numericals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace '?' with np nan\n",
    "df.replace(\"?\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert columns with 'object' type to float\n",
    "df = df.astype(\n",
    "    {\n",
    "        \"x4trestbps\": float,\n",
    "        \"x6fbs\": float,\n",
    "        \"x7restecg\": float,\n",
    "        \"x8thalach\": float,\n",
    "        \"x9exang\": float,\n",
    "        \"x10oldpeak\": float,\n",
    "        \"x11slope\": float,\n",
    "        \"x12ca\": float,\n",
    "        \"x13thal\": float,\n",
    "    }\n",
    ")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now there are no 'object' datatypes - if there were, we would know that there are still some malicius values in the cells that would have to be cleaned off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, we want to omit all categorical features in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the features \"x2sex\", \"x3cp\", \"x5chol\", \"x7restecg\" and \"x9exang\" only contain limited number of values,\n",
    " so we assume them categorical. These, we can drop from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns by name\n",
    "df = df.drop(\n",
    "    [\"x2sex\", \"x3cp\", \"x5chol\", \"x7restecg\", \"x9exang\"],\n",
    "    axis=1,  # drop columns with categorical variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how much of the data is missing in each variable left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nan values per column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok,  there are many missing variables.\n",
    "\n",
    "Now we have two alternatives: we can either get rid of the missing values, or use such robust methods that allow missing values in the data.\n",
    "\n",
    "For removing the missing values we can either drop the columns with most missing data or try to impute the missing values from the data.\n",
    "\n",
    "If there was very little data, we would most likely want to impute the missing data,\n",
    "for example with use the Scikit-learn imputer: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.\n",
    "If you do imputing of missing data, please consider the effects on data quality.\n",
    "\n",
    "In this example, we just drop the columns with most na data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.drop(\n",
    "        [\"x6fbs\", \"x11slope\", \"x12ca\", \"x13thal\"],  # drop columns with most na values\n",
    "        axis=1,\n",
    "    )\n",
    "    .dropna()\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data\n",
    "\n",
    "Once the dataset is looking clean, it is time to visualize it.\n",
    "Visualization is probably the most powerful tool of data science.\n",
    "\n",
    "Visualization depends highly on the data, but usually you should begin by looking at two things: distribution and correlation.\n",
    "\n",
    "Let's make histograms of the variables, and a trellis of scatterplots for visualizing correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# ^(this cell is exported to ml_project_template/ml_project_template/data.py)\n",
    "# you could also define another module to export to.\n",
    "# however, all modules that you export to, must have a notebook with the same name and header!\n",
    "\n",
    "# function for drawing histograms of a dataframe\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_histogram(df):\n",
    "    \"\"\"\n",
    "    Plot histograms of a dataframe\n",
    "    \"\"\"\n",
    "    n_cols = len(df.columns)\n",
    "    col_names = df.columns.values\n",
    "    fig, axs = plt.subplots(\n",
    "        1, n_cols, figsize=(3 * n_cols, 3), constrained_layout=True, sharey=\"row\"\n",
    "    )\n",
    "    for i in range(n_cols):\n",
    "        ax = axs[i]\n",
    "        col_name = col_names[i]\n",
    "        x = df[col_name]\n",
    "        x.plot(ax=ax, kind=\"hist\")\n",
    "        xmin, xmax = min(x), max(x)\n",
    "    for i in range(n_cols):  # to have equal tick lines in each plot\n",
    "        ax = axs[i]\n",
    "        col_name = col_names[i]\n",
    "        x = df[col_name]\n",
    "        xmin, xmax = min(x), max(x)\n",
    "        ax.hlines(\n",
    "            y=axs[df.apply(lambda x: x.max()).argmin()].get_yticks()[\n",
    "                1:-1\n",
    "            ],  # select ticks from the fig with smallest max value\n",
    "            xmin=xmin,\n",
    "            xmax=xmax,\n",
    "            colors=\"white\",\n",
    "            alpha=1,\n",
    "            linewidth=2,\n",
    "        )\n",
    "        ax.set_xlabel(col_name)\n",
    "        ax.set_title(f\"{i})\", loc=\"left\")\n",
    "        ax.spines[\"left\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histogram of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_histogram(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot 4) we see the distribution of different heart disease diagnosis.\n",
    "Remember, that the diagnosis 0 is negative, meaning no heart disease, and the rest are positive, meaning different heart diagnoses.\n",
    "For sake of simplicity, we will replace all different positive diagnoses with a one positive indicator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace different heart diagnoses with value 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.y1num.replace([1, 2, 3, 4], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the distributions again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_histogram(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a trellis plot of the features. Now we can visualize the diagnosis result with the glyph (color and shape of the marker).\n",
    "\n",
    "But wait, don't we just want to plot each of the features against the label?\n",
    "You could do that, too, but in the initial analysis phase it is a good practice to compare all variables against each other.\n",
    "It can reveal all kinds of interesting correlations that may affect your choises later.\n",
    "\n",
    "This will make quite a plot to digest, I agree.\n",
    "For a raport or a presentation you might want to focus on just a few features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# Make a multiple of scatter plots\n",
    "def plot_trellis(df, legend_title=\"y\", true_label=\"True\", false_label=\"False\"):\n",
    "    \"\"\"\n",
    "    Make a trellis plot of a dataframe against a binary y value in last column\n",
    "    \"\"\"\n",
    "    n_cols = df.shape[1] - 1\n",
    "    col_names = df.columns.values\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        n_cols, n_cols, figsize=(n_cols * 3, n_cols * 3), constrained_layout=True\n",
    "    )\n",
    "    for i in range(n_cols):\n",
    "        for j in range(n_cols):\n",
    "            ax = axs[i, j]\n",
    "            if i != j:\n",
    "                # first plot negative cases\n",
    "                df[df.iloc[:, -1] == 0].plot(\n",
    "                    ax=ax,\n",
    "                    x=col_names[j],\n",
    "                    y=col_names[i],\n",
    "                    kind=\"scatter\",\n",
    "                    color=\"b\",\n",
    "                    marker=\"o\",\n",
    "                    alpha=0.5,\n",
    "                    label=false_label,\n",
    "                )\n",
    "                # then positive cases\n",
    "                df[df.iloc[:, -1] != 0].plot(\n",
    "                    ax=ax,\n",
    "                    x=col_names[j],\n",
    "                    y=col_names[i],\n",
    "                    kind=\"scatter\",\n",
    "                    color=\"r\",\n",
    "                    marker=\"x\",\n",
    "                    alpha=0.5,\n",
    "                    label=true_label,\n",
    "                )\n",
    "                # Hide the right and top spines\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.set_title(f\"{i*n_cols+j})\", loc=\"left\")\n",
    "                ax.legend(title=legend_title)\n",
    "            else:\n",
    "                ax.annotate(xy=(0, 0.5), text=col_names[i], fontsize=20)\n",
    "                ax.axis(\"off\")  # hide the box\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_trellis(\n",
    "    df, legend_title=\"diagnose\", true_label=\"positive\", false_label=\"negative\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some intermediate conclusions based on data visualization\n",
    "\n",
    "Can you find anything already based on pure visualization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already based on the visualization we can draw a couple of conclusions:\n",
    "\n",
    "1. the four features we selected have some correlations, but are all well scattered - thus we can assume that they are somewhat independent and tell different things about the patients.\n",
    "This is a good reason to include these four features in the ML model.\n",
    "\n",
    "2. We can already draw conclusions on how individual features effect the diagnosis risk: for example age 50-70 and talach < 150 increase the risk of a positive diagnosis.\n",
    "\n",
    "3. Visualization is a great way to detect outliers. We see that the feature 'oldpeak' has suspiciously many zero values.\n",
    "These may be correct, but if this was a serious analysis I would definitely do my research on the feature to determine how the zeros should be interpret and treated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suffle Dataset\n",
    "\n",
    "Before saving the dataset for further use, it is a good practice to suffle it.\n",
    "This is, of course, assuming the order of the data is not meaningful (as with for example time series data).\n",
    "\n",
    "Random samplin large datasets consumes time and resources, so if you have a pre-suffled dataset you can just read in more data that is already randomized.\n",
    "Also, quite often datasets are intentionally or carelessly saved with some obvious or latent order that might include odd biases to your further analysis.\n",
    "Suffling helps you to get rid of these. Suffling a large dataset may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to suffle the dataset sample a fraction of 1 from it\n",
    "df = df.sample(frac=1).reset_index(drop=True)  # suffle and re-index dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save clean and tidy data for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset to output path defined in the beginning of this notebook\n",
    "df.to_csv(output_clean_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create small toy dataset for developing and testing the ML methods\n",
    "\n",
    "Best way to test your ML methods in development is with data as close to the real data as possible.\n",
    "However, to save you from frustrating long runtimes in development, it is better to create a small sample dataset.\n",
    "You can also create unit tests with this sample dataset.\n",
    "\n",
    "In this example we make a stratified sample that contains randomly picked elements with labels in same proportions as in the data.\n",
    "This increases the chances immediately noticing how our models perform with the real data and tail values included.\n",
    "This is only applicable if the label is categorical, and useful if there are many categories.\n",
    "\n",
    "You can also just make a small sample without considering the proportions `df.sample(frac = 0.1).to_csv(output_toy_filepath)`.\n",
    "\n",
    "If the order of the data matters, just cut out a small continuous proportion of the data for toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create simple stratified sample of the notebook\n",
    "n_toy = 30  # number of samples\n",
    "toy_df = (\n",
    "    df.groupby(\"y1num\", group_keys=False)\n",
    "    .apply(lambda x: x.sample(int(np.rint(n_toy * x.shape[0] / df.shape[0]))))\n",
    "    .sample(frac=1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# alternatively:\n",
    "# toy_df = df.sample(frac = n_toy/df.shape[0]) # if simple stratification is not applicable\n",
    "# toy_df = df.iloc[:n_toy,:] # with time series data. consider the size of n_toy\n",
    "toy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save toy dataset\n",
    "toy_df.to_csv(output_toy_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can now move on to the model notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (hki_lib_book_cl)",
   "language": "python",
   "name": "hki_lib_book_cl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
