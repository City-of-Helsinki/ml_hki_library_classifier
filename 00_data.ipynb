{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 29;\n",
       "                var nbb_unformatted_code = \"# default_exp data\\n%load_ext nb_black\\n# nb_black if running in jupyter\\n# lab_black\\n%load_ext autoreload\\n# automatically reload python modules if there are changes in the\\n%autoreload 2\";\n",
       "                var nbb_formatted_code = \"# default_exp data\\n%load_ext nb_black\\n# nb_black if running in jupyter\\n# lab_black\\n%load_ext autoreload\\n# automatically reload python modules if there are changes in the\\n%autoreload 2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# default_exp data\n",
    "%load_ext nb_black\n",
    "# nb_black if running in jupyter\n",
    "# lab_black\n",
    "%load_ext autoreload\n",
    "# automatically reload python modules if there are changes in the\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 30;\n",
       "                var nbb_unformatted_code = \"# hide\\nfrom nbdev.showdoc import *\";\n",
       "                var nbb_formatted_code = \"# hide\\nfrom nbdev.showdoc import *\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> You should begin your work by cleaning up your data and possibly defining tools for doing it repeateadly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***input***: raw data\n",
    "\n",
    "***output***: clean and tidy dataset for ML model / simulation / analytics + toy dataset for testing\n",
    "\n",
    "***description:***\n",
    "\n",
    "This is the first notebook of your machine learning project. In this notebook, you will load the data, inspect, clean and make it tidy. \n",
    "You will define the data points and their features and labels. The output of this notebook is a clean, tidy dataset ready for analysis and machine learning.\n",
    "You can also do a basic statistical analysis of the data to better understand it.\n",
    "For any functions you define for handling the data, remember to mark their cells with `# export` -comment,\n",
    "so that they will be included in the data.py-module build based on this notebook.\n",
    "You can also include unit tests for your own functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant modules\n",
    "\n",
    "Import python modules you need for handling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 31;\n",
       "                var nbb_unformatted_code = \"import matplotlib.pyplot as plt\\nimport numpy as np\\nfrom numpy import nan as Nan\\nimport pandas as pd\\nfrom sklearn import datasets\\n\\n# from pandas.api.types import CategoricalDtype\";\n",
       "                var nbb_formatted_code = \"import matplotlib.pyplot as plt\\nimport numpy as np\\nfrom numpy import nan as Nan\\nimport pandas as pd\\nfrom sklearn import datasets\\n\\n# from pandas.api.types import CategoricalDtype\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import nan as Nan\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "# from pandas.api.types import CategoricalDtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define notebook parameters\n",
    "\n",
    "Define input, output and additional parameters of this notebook, the information needed for running the notebook.\n",
    "In your own project, you can do this step in the later iterations of the work,\n",
    "when you know what is required.\n",
    "In this cell, only assing values to variables directly: `variable_name = value`.\n",
    "**Do not derive any information in this cell as it will mess up the parameterization** - do it in the cell below.\n",
    "\n",
    "The cell below has the tag 'parameters' - this is for the notebook parameterization tool 'papermill'\n",
    "that allows you execute complete notebooks as python functions.\n",
    "The values you define here will become the default values of the parameterized notebook, but you can also run the notebook with completely different setup.\n",
    "More on this in the `workflow` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 32;\n",
       "                var nbb_unformatted_code = \"# This cell is tagged with 'parameters'\\noutput_clean_filepath = (\\n    \\\"data/preprocessed_data/dataset_clean_hki_lib_book_classification.csv\\\"\\n)\\noutput_toy_filepath = (\\n    \\\"data/preprocessed_data/dataset_toy_hki_lib_book_classification.csv\\\"\\n)\\nseed = 0\";\n",
       "                var nbb_formatted_code = \"# This cell is tagged with 'parameters'\\noutput_clean_filepath = (\\n    \\\"data/preprocessed_data/dataset_clean_hki_lib_book_classification.csv\\\"\\n)\\noutput_toy_filepath = (\\n    \\\"data/preprocessed_data/dataset_toy_hki_lib_book_classification.csv\\\"\\n)\\nseed = 0\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This cell is tagged with 'parameters'\n",
    "output_clean_filepath = (\n",
    "    \"data/preprocessed_data/dataset_clean_hki_lib_book_classification.csv\"\n",
    ")\n",
    "output_toy_filepath = (\n",
    "    \"data/preprocessed_data/dataset_toy_hki_lib_book_classification.csv\"\n",
    ")\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define any immediate derivative operations, righ below the parameters cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 33;\n",
       "                var nbb_unformatted_code = \"# set seed\\nnp.random.seed(seed)\";\n",
       "                var nbb_formatted_code = \"# set seed\\nnp.random.seed(seed)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set seed\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "We are using a csv-file exported from database of Helsinki City library system. We could also load the data straight from database or from online resources, if needed and possible. Please note, that you should not add your datasets to git, as it is not intended for data version control and tracking large datafiles exceed the limits of it. The 'data'-folder of this template is ignored by git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 34;\n",
       "                var nbb_unformatted_code = \"#\\n# Read the raw data of library volumes. The fields are separated by commas.\\n#\\n\\ndf_raw_data = pd.read_csv(\\\"data/raw_data/sample4.csv\\\")\";\n",
       "                var nbb_formatted_code = \"#\\n# Read the raw data of library volumes. The fields are separated by commas.\\n#\\n\\ndf_raw_data = pd.read_csv(\\\"data/raw_data/sample4.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Read the raw data of library volumes. The fields are separated by commas.\n",
    "#\n",
    "\n",
    "df_raw_data = pd.read_csv(\"data/raw_data/sample4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe the data\n",
    "\n",
    "The database uses so called MARC-system. There are multiple rows for each volume(=item). The file consists of three columns:\n",
    "\n",
    "id (id of the Volume that is item e.g. a book)\n",
    "content (the metadata belonging to this volume)\n",
    "marc_tag (specifies what the value of the 'content'-field in this row means), for example:\n",
    "marc_tag '650' means that content field holds a keyword\n",
    "marc_tag '095' means that content field holds Helsinki-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(820271, 7)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 35;\n",
       "                var nbb_unformatted_code = \"df_raw_data.shape\";\n",
       "                var nbb_formatted_code = \"df_raw_data.shape\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>084</th>\n",
       "      <th>092</th>\n",
       "      <th>093</th>\n",
       "      <th>094</th>\n",
       "      <th>095</th>\n",
       "      <th>650</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>420907795010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>420907795011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>420907795013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>420907795014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.791</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>420907795016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      record_id  084  092  093  094    095  650\n",
       "0  420907795010  NaN  NaN  NaN  NaN  180.2  NaN\n",
       "1  420907795011  NaN  NaN  NaN  1.4    NaN  NaN\n",
       "2  420907795013  NaN  NaN  NaN  1.4    NaN  NaN\n",
       "3  420907795014  NaN  NaN  NaN  NaN  1.791  NaN\n",
       "4  420907795016  NaN  1.4  NaN  1.4    NaN  NaN"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 36;\n",
       "                var nbb_unformatted_code = \"# view the data\\ndf_raw_data.head()\";\n",
       "                var nbb_formatted_code = \"# view the data\\ndf_raw_data.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view the data\n",
    "df_raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 820271 entries, 0 to 820270\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   record_id  820271 non-null  int64 \n",
      " 1   084        271200 non-null  object\n",
      " 2   092        428182 non-null  object\n",
      " 3   093        157261 non-null  object\n",
      " 4   094        422208 non-null  object\n",
      " 5   095        552728 non-null  object\n",
      " 6   650        664145 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 43.8+ MB\n",
      "\n",
      "\n",
      "NO HKI CLASS:\n",
      "         record_id    084    092  093    094  095  \\\n",
      "875  420907796975  68.25  68.25  NaN  68.25  NaN   \n",
      "\n",
      "                                                   650  \n",
      "875  ruokaohjeet,ruokaperinne,ruokakulttuuri,arktin...  \n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 37;\n",
       "                var nbb_unformatted_code = \"df_raw_data.info()\\n\\n# An example in dataset 'Sample4.csv, where there is no HKI-CLASS.\\n# Instead the row has 084- and 092 Classes and some keywords\\n\\nprint(\\\"\\\\n\\\\nNO HKI CLASS:\\\\n\\\", df_raw_data.loc[df_raw_data[\\\"record_id\\\"] == 420907796975])\";\n",
       "                var nbb_formatted_code = \"df_raw_data.info()\\n\\n# An example in dataset 'Sample4.csv, where there is no HKI-CLASS.\\n# Instead the row has 084- and 092 Classes and some keywords\\n\\nprint(\\\"\\\\n\\\\nNO HKI CLASS:\\\\n\\\", df_raw_data.loc[df_raw_data[\\\"record_id\\\"] == 420907796975])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_raw_data.info()\n",
    "\n",
    "# An example in dataset 'Sample4.csv, where there is no HKI-CLASS.\n",
    "# Instead the row has 084- and 092 Classes and some keywords\n",
    "\n",
    "print(\"\\n\\nNO HKI CLASS:\\n\", df_raw_data.loc[df_raw_data[\"record_id\"] == 420907796975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so there are 13 features and one label (the last column in our dataset) in the data. Let's construct these into dataframe column names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data and make it tidy\n",
    "\n",
    "For analytics & ML purposes, we should make the data tidy. This means that\n",
    "\n",
    " 1. Every column is a variable (feature or label)\n",
    " 2. Every row is an observation (data point).\n",
    " 3. Every cell is a single value (int, float, cathegorical, str, but no nested structures like lists or dictionaries)\n",
    "\n",
    "\n",
    " - Our **features** consist of keywords and different classifications. Marc_tags for this features are:\n",
    "   - 084 = PLC - Finnish Public Libraries Classification System (YKL)\n",
    "   - 092 = City of Espoo classification\n",
    "   - 093 = City of Kauniainen classification\n",
    "   - 094 = City of Vantaa classification\n",
    "   - 650 = Keyword (subject/index term/concept)\n",
    " - Our **label** is Helsinki City Library Classification System and we call it HCLCS (HKLJ) (095 in raw data).\n",
    " - Our **datapoint** is a library item (book, cd, dvd...) identified by id column in raw data.\n",
    " \n",
    "~~First we get only those rows that contain keywords (marc_tag = 650).~~\n",
    "\n",
    "Filtering the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 38;\n",
       "                var nbb_unformatted_code = \"#\\n# Get only those rows that has ykl- and hklj-classes\\n#\\n\\n# Filtter\\u00f6id\\u00e4\\u00e4n kaunokirjallisuus pois (\\\"095\\\":n luokka 1.4) ja vaaditaan etteiv\\u00e4t 084 ja 095 ole tyhji\\u00e4.\\ndf1 = df_raw_data[:]\\ndf1 = df1[\\n    (\\n        (df_raw_data[\\\"084\\\"].notnull())\\n        & (df_raw_data[\\\"095\\\"].notnull())\\n        & (df_raw_data[\\\"095\\\"] != \\\"1.4\\\")\\n    )\\n]\\n\\n# 084-luokituksia (YKL) voi olla useita samalla niteelle (datariville). Valitaan ensimm\\u00e4inen arvo.\\ndf1.loc[:, (\\\"084\\\")] = df1[\\\"084\\\"].apply(lambda x: x.split(\\\",\\\")[0])\";\n",
       "                var nbb_formatted_code = \"#\\n# Get only those rows that has ykl- and hklj-classes\\n#\\n\\n# Filtter\\u00f6id\\u00e4\\u00e4n kaunokirjallisuus pois (\\\"095\\\":n luokka 1.4) ja vaaditaan etteiv\\u00e4t 084 ja 095 ole tyhji\\u00e4.\\ndf1 = df_raw_data[:]\\ndf1 = df1[\\n    (\\n        (df_raw_data[\\\"084\\\"].notnull())\\n        & (df_raw_data[\\\"095\\\"].notnull())\\n        & (df_raw_data[\\\"095\\\"] != \\\"1.4\\\")\\n    )\\n]\\n\\n# 084-luokituksia (YKL) voi olla useita samalla niteelle (datariville). Valitaan ensimm\\u00e4inen arvo.\\ndf1.loc[:, (\\\"084\\\")] = df1[\\\"084\\\"].apply(lambda x: x.split(\\\",\\\")[0])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Get only those rows that has ykl- and hklj-classes\n",
    "#\n",
    "\n",
    "# Filtteröidään kaunokirjallisuus pois (\"095\":n luokka 1.4) ja vaaditaan etteivät 084 ja 095 ole tyhjiä.\n",
    "df1 = df_raw_data[:]\n",
    "df1 = df1[\n",
    "    (\n",
    "        (df_raw_data[\"084\"].notnull())\n",
    "        & (df_raw_data[\"095\"].notnull())\n",
    "        & (df_raw_data[\"095\"] != \"1.4\")\n",
    "    )\n",
    "]\n",
    "\n",
    "# 084-luokituksia (YKL) voi olla useita samalla niteelle (datariville). Valitaan ensimmäinen arvo.\n",
    "df1.loc[:, (\"084\")] = df1[\"084\"].apply(lambda x: x.split(\",\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 39;\n",
       "                var nbb_unformatted_code = \"# Fixer function which scrapes \\\"real\\\" part from broken string.\\n# I.e. broken: '59.562209'  -> fixed '59.5622',\\n#      broken: '59.562109*' -> fixed '59.562',\\n#      broken: '89.41038'   -> fixed '89.41'\\ndef fix_class(wrong_value, notations):\\n    # take the first value. cell could contain multiple values\\n    wrong_value = wrong_value.split(\\\",\\\")[0]\\n    found = False\\n    try:\\n        for index, char in enumerate(wrong_value):\\n            if char == \\\".\\\":  # no need to test decimal point -value\\n                continue\\n            slice_of_test_value = len(wrong_value) - (index + 1)\\n            test_value = str(wrong_value[0:slice_of_test_value])\\n            if test_value in notations:\\n                return test_value\\n    except Exception as e:\\n        print(\\\"wrong value: \\\", wrong_value, \\\" e: \\\", e)\\n        pass\\n    return np.nan\";\n",
       "                var nbb_formatted_code = \"# Fixer function which scrapes \\\"real\\\" part from broken string.\\n# I.e. broken: '59.562209'  -> fixed '59.5622',\\n#      broken: '59.562109*' -> fixed '59.562',\\n#      broken: '89.41038'   -> fixed '89.41'\\ndef fix_class(wrong_value, notations):\\n    # take the first value. cell could contain multiple values\\n    wrong_value = wrong_value.split(\\\",\\\")[0]\\n    found = False\\n    try:\\n        for index, char in enumerate(wrong_value):\\n            if char == \\\".\\\":  # no need to test decimal point -value\\n                continue\\n            slice_of_test_value = len(wrong_value) - (index + 1)\\n            test_value = str(wrong_value[0:slice_of_test_value])\\n            if test_value in notations:\\n                return test_value\\n    except Exception as e:\\n        print(\\\"wrong value: \\\", wrong_value, \\\" e: \\\", e)\\n        pass\\n    return np.nan\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fixer function which scrapes \"real\" part from broken string.\n",
    "# I.e. broken: '59.562209'  -> fixed '59.5622',\n",
    "#      broken: '59.562109*' -> fixed '59.562',\n",
    "#      broken: '89.41038'   -> fixed '89.41'\n",
    "def fix_class(wrong_value, notations):\n",
    "    # take the first value. cell could contain multiple values\n",
    "    wrong_value = wrong_value.split(\",\")[0]\n",
    "    found = False\n",
    "    try:\n",
    "        for index, char in enumerate(wrong_value):\n",
    "            if char == \".\":  # no need to test decimal point -value\n",
    "                continue\n",
    "            slice_of_test_value = len(wrong_value) - (index + 1)\n",
    "            test_value = str(wrong_value[0:slice_of_test_value])\n",
    "            if test_value in notations:\n",
    "                return test_value\n",
    "    except Exception as e:\n",
    "        print(\"wrong value: \", wrong_value, \" e: \", e)\n",
    "        pass\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 40;\n",
       "                var nbb_unformatted_code = \"# Function which removes \\\"form tags\\\" (\\\"muotoluokka\\\") from string.\\n\\nimport re\\n\\n\\ndef remove_form_tags(wrong_value, tags):\\n    for tag in tags:\\n        pattern = tag + \\\"(?!.*\\\" + tag + \\\")\\\"\\n        s = re.search(pattern, wrong_value)\\n        if s:\\n            position = s.start()\\n\\n            # remove '.' if it the last character\\n            if wrong_value[position - 1] == \\\".\\\":\\n                korjaus = wrong_value[: s.start() - 1]\\n            else:\\n                korjaus = wrong_value[: s.start()]\\n            return korjaus\\n    return np.nan\";\n",
       "                var nbb_formatted_code = \"# Function which removes \\\"form tags\\\" (\\\"muotoluokka\\\") from string.\\n\\nimport re\\n\\n\\ndef remove_form_tags(wrong_value, tags):\\n    for tag in tags:\\n        pattern = tag + \\\"(?!.*\\\" + tag + \\\")\\\"\\n        s = re.search(pattern, wrong_value)\\n        if s:\\n            position = s.start()\\n\\n            # remove '.' if it the last character\\n            if wrong_value[position - 1] == \\\".\\\":\\n                korjaus = wrong_value[: s.start() - 1]\\n            else:\\n                korjaus = wrong_value[: s.start()]\\n            return korjaus\\n    return np.nan\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function which removes \"form tags\" (\"muotoluokka\") from string.\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def remove_form_tags(wrong_value, tags):\n",
    "    for tag in tags:\n",
    "        pattern = tag + \"(?!.*\" + tag + \")\"\n",
    "        s = re.search(pattern, wrong_value)\n",
    "        if s:\n",
    "            position = s.start()\n",
    "\n",
    "            # remove '.' if it the last character\n",
    "            if wrong_value[position - 1] == \".\":\n",
    "                korjaus = wrong_value[: s.start() - 1]\n",
    "            else:\n",
    "                korjaus = wrong_value[: s.start()]\n",
    "            return korjaus\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 41;\n",
       "                var nbb_unformatted_code = \"# Trim (remove spaces) string values of all feature columns (084, 092, 093, 094 and 095)\\n\\ndf_obj = df1.select_dtypes([\\\"object\\\"])\\ndf1[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\";\n",
       "                var nbb_formatted_code = \"# Trim (remove spaces) string values of all feature columns (084, 092, 093, 094 and 095)\\n\\ndf_obj = df1.select_dtypes([\\\"object\\\"])\\ndf1[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trim (remove spaces) string values of all feature columns (084, 092, 093, 094 and 095)\n",
    "\n",
    "df_obj = df1.select_dtypes([\"object\"])\n",
    "df1[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/ml-sami-libclas/lib/python3.8/site-packages/pandas/core/indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 42;\n",
       "                var nbb_unformatted_code = \"# YKL hierarchy\\ndf_ykl = pd.read_csv(\\n    \\\"data/raw_data/ykl_simple.csv\\\",\\n    dtype={\\\"notation\\\": str, \\\"prefLabel\\\": str},\\n)\\n\\n# YKL form tags\\ndf_ykl_ml = pd.read_csv(\\n    \\\"data/raw_data/ykl_ml.csv\\\",\\n    dtype={\\\"notation\\\": str, \\\"prefLabel\\\": str},\\n)\\n\\n# Read notations (unique 084-classes) into list\\nnotations = df_ykl[\\\"notation\\\"]\\n# NaNs are valid \\\"correct\\\" values in this syntax -> append it to notations Series\\nnotations = notations.append(pd.Series([Nan], index=[2384]))\\n# Convert df to list\\nnotations = notations.values.tolist()\\n\\n# Select preferred columns for cleaning purposes(084, 092, 093 and 094)\\ncolumns = list(df1)[1:-2]\\n\\nform_tags = df_ykl_ml[\\\"notation\\\"].values.tolist()  # .wrap(width=10)  # .str.join(\\\",\\\")\\ntags = form_tags\\n\\n# Create regex pattern for class-value to test if class is 'form tag'\\npattern = \\\"(?:\\\\d{2}\\\\.\\\\d*\\\"\\nform_tags = \\\"|\\\".join([pattern + \\\"{}$)\\\".format(value) for value in form_tags])\\n\\n# Iterate over selected columns (084, 092, 093 and 094)\\nfor i in columns:\\n    df_split1A = df1[df1[i].isin(notations)]\\n    df_split1B = df1[~df1[i].isin(notations)]\\n    df_split2 = df_split1B[df_split1B[i].str.fullmatch(form_tags)]\\n\\n    # Remove \\\"form tags\\\"\\n    df_split2.loc[:, (i)] = df_split2.apply(\\n        lambda x: remove_form_tags(x[i], tags), axis=1\\n    )\\n\\n    df_split3A = df_split2[df_split2[i].isin(notations)]\\n    df_split3B = df_split2[~df_split2[i].isin(notations)]\\n\\n    # Fix the remaining ones\\n    df_split3B.loc[:, (i)] = df_split3B.apply(\\n        lambda x: fix_class(x[i], notations), axis=1\\n    )\\n\\n    # Merge splitted data\\n    df1 = pd.concat([df_split1A, df_split3A, df_split3B], axis=0)\";\n",
       "                var nbb_formatted_code = \"# YKL hierarchy\\ndf_ykl = pd.read_csv(\\n    \\\"data/raw_data/ykl_simple.csv\\\",\\n    dtype={\\\"notation\\\": str, \\\"prefLabel\\\": str},\\n)\\n\\n# YKL form tags\\ndf_ykl_ml = pd.read_csv(\\n    \\\"data/raw_data/ykl_ml.csv\\\",\\n    dtype={\\\"notation\\\": str, \\\"prefLabel\\\": str},\\n)\\n\\n# Read notations (unique 084-classes) into list\\nnotations = df_ykl[\\\"notation\\\"]\\n# NaNs are valid \\\"correct\\\" values in this syntax -> append it to notations Series\\nnotations = notations.append(pd.Series([Nan], index=[2384]))\\n# Convert df to list\\nnotations = notations.values.tolist()\\n\\n# Select preferred columns for cleaning purposes(084, 092, 093 and 094)\\ncolumns = list(df1)[1:-2]\\n\\nform_tags = df_ykl_ml[\\\"notation\\\"].values.tolist()  # .wrap(width=10)  # .str.join(\\\",\\\")\\ntags = form_tags\\n\\n# Create regex pattern for class-value to test if class is 'form tag'\\npattern = \\\"(?:\\\\d{2}\\\\.\\\\d*\\\"\\nform_tags = \\\"|\\\".join([pattern + \\\"{}$)\\\".format(value) for value in form_tags])\\n\\n# Iterate over selected columns (084, 092, 093 and 094)\\nfor i in columns:\\n    df_split1A = df1[df1[i].isin(notations)]\\n    df_split1B = df1[~df1[i].isin(notations)]\\n    df_split2 = df_split1B[df_split1B[i].str.fullmatch(form_tags)]\\n\\n    # Remove \\\"form tags\\\"\\n    df_split2.loc[:, (i)] = df_split2.apply(\\n        lambda x: remove_form_tags(x[i], tags), axis=1\\n    )\\n\\n    df_split3A = df_split2[df_split2[i].isin(notations)]\\n    df_split3B = df_split2[~df_split2[i].isin(notations)]\\n\\n    # Fix the remaining ones\\n    df_split3B.loc[:, (i)] = df_split3B.apply(\\n        lambda x: fix_class(x[i], notations), axis=1\\n    )\\n\\n    # Merge splitted data\\n    df1 = pd.concat([df_split1A, df_split3A, df_split3B], axis=0)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YKL hierarchy\n",
    "df_ykl = pd.read_csv(\n",
    "    \"data/raw_data/ykl_simple.csv\",\n",
    "    dtype={\"notation\": str, \"prefLabel\": str},\n",
    ")\n",
    "\n",
    "# YKL form tags\n",
    "df_ykl_ml = pd.read_csv(\n",
    "    \"data/raw_data/ykl_ml.csv\",\n",
    "    dtype={\"notation\": str, \"prefLabel\": str},\n",
    ")\n",
    "\n",
    "# Read notations (unique 084-classes) into list\n",
    "notations = df_ykl[\"notation\"]\n",
    "# NaNs are valid \"correct\" values in this syntax -> append it to notations Series\n",
    "notations = notations.append(pd.Series([Nan], index=[2384]))\n",
    "# Convert df to list\n",
    "notations = notations.values.tolist()\n",
    "\n",
    "# Select preferred columns for cleaning purposes(084, 092, 093 and 094)\n",
    "columns = list(df1)[1:-2]\n",
    "\n",
    "form_tags = df_ykl_ml[\"notation\"].values.tolist()  # .wrap(width=10)  # .str.join(\",\")\n",
    "tags = form_tags\n",
    "\n",
    "# Create regex pattern for class-value to test if class is 'form tag'\n",
    "pattern = \"(?:\\d{2}\\.\\d*\"\n",
    "form_tags = \"|\".join([pattern + \"{}$)\".format(value) for value in form_tags])\n",
    "\n",
    "# Iterate over selected columns (084, 092, 093 and 094)\n",
    "for i in columns:\n",
    "    df_split1A = df1[df1[i].isin(notations)]\n",
    "    df_split1B = df1[~df1[i].isin(notations)]\n",
    "    df_split2 = df_split1B[df_split1B[i].str.fullmatch(form_tags)]\n",
    "\n",
    "    # Remove \"form tags\"\n",
    "    df_split2.loc[:, (i)] = df_split2.apply(\n",
    "        lambda x: remove_form_tags(x[i], tags), axis=1\n",
    "    )\n",
    "\n",
    "    df_split3A = df_split2[df_split2[i].isin(notations)]\n",
    "    df_split3B = df_split2[~df_split2[i].isin(notations)]\n",
    "\n",
    "    # Fix the remaining ones\n",
    "    df_split3B.loc[:, (i)] = df_split3B.apply(\n",
    "        lambda x: fix_class(x[i], notations), axis=1\n",
    "    )\n",
    "\n",
    "    # Merge splitted data\n",
    "    df1 = pd.concat([df_split1A, df_split3A, df_split3B], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 43;\n",
       "                var nbb_unformatted_code = \"# Read HKLJ classes from csv-file\\ndf_hklj = pd.read_csv(\\n    \\\"data/raw_data/hklj_simple.csv\\\",\\n    dtype={\\\"notation\\\": str, \\\"prefLabel\\\": str},\\n)\\n\\n# Read HKLJ form tags from csv-file\\ndf_hklj_ml = pd.read_csv(\\n    \\\"data/raw_data/hklj_ml.csv\\\",\\n    dtype={\\\"notation\\\": str, \\\"prefLabel\\\": str},\\n)\\n\\n# Read notations (unique 084-classes) into list\\nnotations = df_hklj[\\\"notation\\\"]\\n\\n# NaNs are valid \\\"correct\\\" values in this syntax -> append it to notations Series\\n# notations = notations.append(pd.Series([Nan], index=[2232]))\\nnotations = notations.append(pd.Series([Nan], index=[2055]))\\n# Convert df to list\\nnotations = notations.values.tolist()\\n\\n# form_tags = df_hklj_ml[\\\"notation\\\"].str.cat(sep=\\\"|\\\")\\nform_tags = df_hklj_ml[\\\"notation\\\"].values.tolist()  # .wrap(width=10)  # .str.join(\\\",\\\")\\ntags = form_tags\\n\\n# Create regex pattern for class-value to test if class is 'form tag'\\npattern = \\\"(?:\\\\d{3}\\\\.\\\\d*\\\"\\nform_tags = \\\"|\\\".join([pattern + \\\"{}$)\\\".format(value) for value in form_tags])\\n\\n# Split data to separate dataframes\\ndf_split1A = df1[df1[\\\"095\\\"].isin(notations)]\\ndf_split1B = df1[~df1[\\\"095\\\"].isin(notations)]\\ndf_split2 = df_split1B[df_split1B[\\\"095\\\"].str.fullmatch(form_tags)]\\n\\n# Remove \\\"form tags\\\"\\ndf_split2.loc[:, (\\\"095\\\")] = df_split2.apply(\\n    lambda x: remove_form_tags(x[\\\"095\\\"], tags), axis=1\\n)\\n\\ndf_split3A = df_split2[df_split2[\\\"095\\\"].isin(notations)]\\ndf_split3B = df_split2[~df_split2[\\\"095\\\"].isin(notations)]\\n\\n# Fix the remaining ones\\ndf_split3B.loc[:, (\\\"095\\\")] = df_split3B.apply(\\n    lambda x: fix_class(x[\\\"095\\\"], notations), axis=1\\n)\\n\\n# Merge splitted data\\ndf1 = pd.concat([df_split1A, df_split3A, df_split3B], axis=0)\";\n",
       "                var nbb_formatted_code = \"# Read HKLJ classes from csv-file\\ndf_hklj = pd.read_csv(\\n    \\\"data/raw_data/hklj_simple.csv\\\",\\n    dtype={\\\"notation\\\": str, \\\"prefLabel\\\": str},\\n)\\n\\n# Read HKLJ form tags from csv-file\\ndf_hklj_ml = pd.read_csv(\\n    \\\"data/raw_data/hklj_ml.csv\\\",\\n    dtype={\\\"notation\\\": str, \\\"prefLabel\\\": str},\\n)\\n\\n# Read notations (unique 084-classes) into list\\nnotations = df_hklj[\\\"notation\\\"]\\n\\n# NaNs are valid \\\"correct\\\" values in this syntax -> append it to notations Series\\n# notations = notations.append(pd.Series([Nan], index=[2232]))\\nnotations = notations.append(pd.Series([Nan], index=[2055]))\\n# Convert df to list\\nnotations = notations.values.tolist()\\n\\n# form_tags = df_hklj_ml[\\\"notation\\\"].str.cat(sep=\\\"|\\\")\\nform_tags = df_hklj_ml[\\\"notation\\\"].values.tolist()  # .wrap(width=10)  # .str.join(\\\",\\\")\\ntags = form_tags\\n\\n# Create regex pattern for class-value to test if class is 'form tag'\\npattern = \\\"(?:\\\\d{3}\\\\.\\\\d*\\\"\\nform_tags = \\\"|\\\".join([pattern + \\\"{}$)\\\".format(value) for value in form_tags])\\n\\n# Split data to separate dataframes\\ndf_split1A = df1[df1[\\\"095\\\"].isin(notations)]\\ndf_split1B = df1[~df1[\\\"095\\\"].isin(notations)]\\ndf_split2 = df_split1B[df_split1B[\\\"095\\\"].str.fullmatch(form_tags)]\\n\\n# Remove \\\"form tags\\\"\\ndf_split2.loc[:, (\\\"095\\\")] = df_split2.apply(\\n    lambda x: remove_form_tags(x[\\\"095\\\"], tags), axis=1\\n)\\n\\ndf_split3A = df_split2[df_split2[\\\"095\\\"].isin(notations)]\\ndf_split3B = df_split2[~df_split2[\\\"095\\\"].isin(notations)]\\n\\n# Fix the remaining ones\\ndf_split3B.loc[:, (\\\"095\\\")] = df_split3B.apply(\\n    lambda x: fix_class(x[\\\"095\\\"], notations), axis=1\\n)\\n\\n# Merge splitted data\\ndf1 = pd.concat([df_split1A, df_split3A, df_split3B], axis=0)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read HKLJ classes from csv-file\n",
    "df_hklj = pd.read_csv(\n",
    "    \"data/raw_data/hklj_simple.csv\",\n",
    "    dtype={\"notation\": str, \"prefLabel\": str},\n",
    ")\n",
    "\n",
    "# Read HKLJ form tags from csv-file\n",
    "df_hklj_ml = pd.read_csv(\n",
    "    \"data/raw_data/hklj_ml.csv\",\n",
    "    dtype={\"notation\": str, \"prefLabel\": str},\n",
    ")\n",
    "\n",
    "# Read notations (unique 084-classes) into list\n",
    "notations = df_hklj[\"notation\"]\n",
    "\n",
    "# NaNs are valid \"correct\" values in this syntax -> append it to notations Series\n",
    "# notations = notations.append(pd.Series([Nan], index=[2232]))\n",
    "notations = notations.append(pd.Series([Nan], index=[2055]))\n",
    "# Convert df to list\n",
    "notations = notations.values.tolist()\n",
    "\n",
    "# form_tags = df_hklj_ml[\"notation\"].str.cat(sep=\"|\")\n",
    "form_tags = df_hklj_ml[\"notation\"].values.tolist()  # .wrap(width=10)  # .str.join(\",\")\n",
    "tags = form_tags\n",
    "\n",
    "# Create regex pattern for class-value to test if class is 'form tag'\n",
    "pattern = \"(?:\\d{3}\\.\\d*\"\n",
    "form_tags = \"|\".join([pattern + \"{}$)\".format(value) for value in form_tags])\n",
    "\n",
    "# Split data to separate dataframes\n",
    "df_split1A = df1[df1[\"095\"].isin(notations)]\n",
    "df_split1B = df1[~df1[\"095\"].isin(notations)]\n",
    "df_split2 = df_split1B[df_split1B[\"095\"].str.fullmatch(form_tags)]\n",
    "\n",
    "# Remove \"form tags\"\n",
    "df_split2.loc[:, (\"095\")] = df_split2.apply(\n",
    "    lambda x: remove_form_tags(x[\"095\"], tags), axis=1\n",
    ")\n",
    "\n",
    "df_split3A = df_split2[df_split2[\"095\"].isin(notations)]\n",
    "df_split3B = df_split2[~df_split2[\"095\"].isin(notations)]\n",
    "\n",
    "# Fix the remaining ones\n",
    "df_split3B.loc[:, (\"095\")] = df_split3B.apply(\n",
    "    lambda x: fix_class(x[\"095\"], notations), axis=1\n",
    ")\n",
    "\n",
    "# Merge splitted data\n",
    "df1 = pd.concat([df_split1A, df_split3A, df_split3B], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 44;\n",
       "                var nbb_unformatted_code = \"# Copy dataframe\\ndf2 = df1[:]\\n\\n# Filter out rows which has empty strings (=\\\"\\\"). You can do it by using astype(bool)-method.\\ndf2 = df2[df2[\\\"084\\\"].astype(bool)]\\n\\n# Filter again null values\\ndf2 = df2[((df2[\\\"084\\\"].notnull()) & (df2[\\\"095\\\"].notnull()))]\\n\\n# Write dataframe to csv-file\\ndf2.to_csv(\\\"data_output.csv\\\")\";\n",
       "                var nbb_formatted_code = \"# Copy dataframe\\ndf2 = df1[:]\\n\\n# Filter out rows which has empty strings (=\\\"\\\"). You can do it by using astype(bool)-method.\\ndf2 = df2[df2[\\\"084\\\"].astype(bool)]\\n\\n# Filter again null values\\ndf2 = df2[((df2[\\\"084\\\"].notnull()) & (df2[\\\"095\\\"].notnull()))]\\n\\n# Write dataframe to csv-file\\ndf2.to_csv(\\\"data_output.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Copy dataframe\n",
    "df2 = df1[:]\n",
    "\n",
    "# Filter out rows which has empty strings (=\"\"). You can do it by using astype(bool)-method.\n",
    "df2 = df2[df2[\"084\"].astype(bool)]\n",
    "\n",
    "# Filter again null values\n",
    "df2 = df2[((df2[\"084\"].notnull()) & (df2[\"095\"].notnull()))]\n",
    "\n",
    "# Write dataframe to csv-file\n",
    "df2.to_csv(\"data_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of errors: 0\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 45;\n",
       "                var nbb_unformatted_code = \"# Create schema and validate values against that schema.\\n\\nimport pandas_schema\\nfrom pandas_schema import Column, Schema\\nfrom pandas_schema.validation import (\\n    CustomElementValidation,\\n    LeadingWhitespaceValidation,\\n    TrailingWhitespaceValidation,\\n    CanConvertValidation,\\n    MatchesPatternValidation,\\n    InRangeValidation,\\n    InListValidation,\\n)\\n\\nnull_validation = [\\n    CustomElementValidation(lambda d: d is not np.nan, \\\"this field cannot be null\\\")\\n]\\n\\nschema = Schema(\\n    [\\n        Column(\\\"record_id\\\", null_validation),\\n        Column(\\n            \\\"084\\\",\\n            [\\n                LeadingWhitespaceValidation(),\\n                TrailingWhitespaceValidation(),\\n                MatchesPatternValidation(r\\\"^(?:\\\\S)|\\\\d{1,2}(?:$|.\\\\d{0,}$)\\\"),\\n            ],\\n        ),\\n        Column(\\\"092\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n        Column(\\\"093\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n        Column(\\\"094\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n        Column(\\\"095\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n        Column(\\\"650\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n    ]\\n)\\n\\nerrors = schema.validate(df2)\\n\\n# Count of validation errors\\nprint('Count of errors:', len(errors))\\n\\n# Print first 15 validation errros\\nfor error in errors[:15]:\\n    print(error)\";\n",
       "                var nbb_formatted_code = \"# Create schema and validate values against that schema.\\n\\nimport pandas_schema\\nfrom pandas_schema import Column, Schema\\nfrom pandas_schema.validation import (\\n    CustomElementValidation,\\n    LeadingWhitespaceValidation,\\n    TrailingWhitespaceValidation,\\n    CanConvertValidation,\\n    MatchesPatternValidation,\\n    InRangeValidation,\\n    InListValidation,\\n)\\n\\nnull_validation = [\\n    CustomElementValidation(lambda d: d is not np.nan, \\\"this field cannot be null\\\")\\n]\\n\\nschema = Schema(\\n    [\\n        Column(\\\"record_id\\\", null_validation),\\n        Column(\\n            \\\"084\\\",\\n            [\\n                LeadingWhitespaceValidation(),\\n                TrailingWhitespaceValidation(),\\n                MatchesPatternValidation(r\\\"^(?:\\\\S)|\\\\d{1,2}(?:$|.\\\\d{0,}$)\\\"),\\n            ],\\n        ),\\n        Column(\\\"092\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n        Column(\\\"093\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n        Column(\\\"094\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n        Column(\\\"095\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n        Column(\\\"650\\\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\\n    ]\\n)\\n\\nerrors = schema.validate(df2)\\n\\n# Count of validation errors\\nprint(\\\"Count of errors:\\\", len(errors))\\n\\n# Print first 15 validation errros\\nfor error in errors[:15]:\\n    print(error)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create schema and validate values against that schema.\n",
    "\n",
    "import pandas_schema\n",
    "from pandas_schema import Column, Schema\n",
    "from pandas_schema.validation import (\n",
    "    CustomElementValidation,\n",
    "    LeadingWhitespaceValidation,\n",
    "    TrailingWhitespaceValidation,\n",
    "    CanConvertValidation,\n",
    "    MatchesPatternValidation,\n",
    "    InRangeValidation,\n",
    "    InListValidation,\n",
    ")\n",
    "\n",
    "null_validation = [\n",
    "    CustomElementValidation(lambda d: d is not np.nan, \"this field cannot be null\")\n",
    "]\n",
    "\n",
    "schema = Schema(\n",
    "    [\n",
    "        Column(\"record_id\", null_validation),\n",
    "        Column(\n",
    "            \"084\",\n",
    "            [\n",
    "                LeadingWhitespaceValidation(),\n",
    "                TrailingWhitespaceValidation(),\n",
    "                MatchesPatternValidation(r\"^(?:\\S)|\\d{1,2}(?:$|.\\d{0,}$)\"),\n",
    "            ],\n",
    "        ),\n",
    "        Column(\"092\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\n",
    "        Column(\"093\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\n",
    "        Column(\"094\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\n",
    "        Column(\"095\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\n",
    "        Column(\"650\", [LeadingWhitespaceValidation(), TrailingWhitespaceValidation()]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "errors = schema.validate(df2)\n",
    "\n",
    "# Count of validation errors\n",
    "print('Count of errors:', len(errors))\n",
    "\n",
    "# Print first 15 validation errros\n",
    "for error in errors[:15]:\n",
    "    print(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of library items and classes is big (hundreds of thousands items and thousands of classes). To limit the problem we make the following assumptions:\n",
    " - We always know the YKL-classification of the new library item, that we want classify with HKL-classification sytem.\n",
    " - Training/validation set can and need only contain rows that match the YKL-classification of the new library item.\n",
    " \n",
    "Thus we pick only those rows that match the YKL-class of the new library item. In first implementation just select a random YKL-class. However the training/validation dataset can still be quite big. You can restrict the sixe of the dataset even more by setting the variable MAX_TRAININGSET_SIZE below appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****\n",
      "Found 3107 rows with YKL-CLASS=78.8911\n",
      "Number of keywords: 365\n",
      "*****\n",
      "\n",
      "\n",
      "*****\n",
      "Final dataset: 3000 rows with YKL-CLASS=78.8911\n",
      "Number of keywords in final dataset is: 365\n",
      "*****\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>084</th>\n",
       "      <th>092</th>\n",
       "      <th>093</th>\n",
       "      <th>094</th>\n",
       "      <th>095</th>\n",
       "      <th>rock</th>\n",
       "      <th>popmusiikki</th>\n",
       "      <th>kokoelmat</th>\n",
       "      <th>tabulatuurinuottikirjoitus</th>\n",
       "      <th>...</th>\n",
       "      <th>heavy metal</th>\n",
       "      <th>säveltäminen</th>\n",
       "      <th>huuliharppu</th>\n",
       "      <th>rockfestivaalit</th>\n",
       "      <th>ruisrock</th>\n",
       "      <th>ilosaarirock</th>\n",
       "      <th>tapahtumat</th>\n",
       "      <th>elävä musiikki</th>\n",
       "      <th>tyylit</th>\n",
       "      <th>hakuteokset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>420909247702</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>786.231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>420908439649</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>788.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>420908552378</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>0</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>788.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>420909189008</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>0</td>\n",
       "      <td>788.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>420908424085</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>0</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>788.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      record_id      084      092      093      094      095  rock  \\\n",
       "0  420909247702  78.8911  78.8911        0        0  786.231   0.0   \n",
       "1  420908439649  78.8911  78.8911        0        0   788.33   1.0   \n",
       "2  420908552378  78.8911  78.8911        0  78.8911   788.33   1.0   \n",
       "3  420909189008  78.8911  78.8911  78.8911        0   788.33   0.0   \n",
       "4  420908424085  78.8911  78.8911        0  78.8911   788.33   1.0   \n",
       "\n",
       "   popmusiikki  kokoelmat  tabulatuurinuottikirjoitus  ...  heavy metal  \\\n",
       "0          0.0        0.0                         0.0  ...          0.0   \n",
       "1          0.0        1.0                         0.0  ...          0.0   \n",
       "2          0.0        0.0                         0.0  ...          0.0   \n",
       "3          0.0        0.0                         0.0  ...          0.0   \n",
       "4          0.0        0.0                         0.0  ...          0.0   \n",
       "\n",
       "   säveltäminen  huuliharppu  rockfestivaalit  ruisrock  ilosaarirock  \\\n",
       "0           0.0          0.0              0.0       0.0           0.0   \n",
       "1           0.0          0.0              0.0       0.0           0.0   \n",
       "2           0.0          0.0              0.0       0.0           0.0   \n",
       "3           0.0          0.0              0.0       0.0           0.0   \n",
       "4           0.0          0.0              0.0       0.0           0.0   \n",
       "\n",
       "   tapahtumat  elävä musiikki  tyylit  hakuteokset  \n",
       "0         0.0             0.0     0.0          0.0  \n",
       "1         0.0             0.0     0.0          0.0  \n",
       "2         0.0             0.0     0.0          0.0  \n",
       "3         0.0             0.0     0.0          0.0  \n",
       "4         0.0             0.0     0.0          0.0  \n",
       "\n",
       "[5 rows x 371 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 49;\n",
       "                var nbb_unformatted_code = \"# Restrict the trainingset size if necessary\\nMAX_TRAININGSET_SIZE = 3000\\n\\n# Select a random YKL-class get all rows matching it\\nsample_row = df2.sample(frac=1)\\nitem_ykl_class = sample_row.iloc[0][\\\"084\\\"]\\none_ykl_class_df = df2.loc[df2[\\\"084\\\"] == item_ykl_class]\\none_ykl_class_df = one_ykl_class_df.reset_index(drop=True)\\n\\n# Read all distinct keywords into an array\\nkeywords = []\\nfor i in range(len(one_ykl_class_df)):\\n    item_keywords_str = (str)(one_ykl_class_df.iloc[(i), 6])\\n    item_keywords_lst = item_keywords_str.split(\\\",\\\")\\n    for word in item_keywords_lst:\\n        word = word.strip().lower()\\n        if word not in keywords:\\n            keywords.append(word)\\n\\n# Sanity check printing\\nprint(f\\\"\\\\n*****\\\")\\nprint(f\\\"Found {one_ykl_class_df.shape[0]} rows with YKL-CLASS={item_ykl_class}\\\")\\nprint(f\\\"Number of keywords: {len(keywords)}\\\")\\nprint(f\\\"*****\\\\n\\\")\\n\\n# Restrict the training/validation set MAX_TRAININGSET_SIZE rows\\nif one_ykl_class_df.shape[0] < MAX_TRAININGSET_SIZE:\\n    MAX_TRAININGSET_SIZE = one_ykl_class_df.shape[0]\\none_ykl_class_df = one_ykl_class_df.sample(MAX_TRAININGSET_SIZE)\\n\\n# Add keyword columns with keyword as a title (value will be 0 or 1 depending if the keyword belongs to the volume or not)\\n# NOTE: \\\"A Pandas Series is like a column in a table\\\" (https://www.w3schools.com/python/pandas/pandas_series.asp)\\nfor i in range(len(keywords)):\\n    one_ykl_class_df[keywords[i]] = pd.Series([], dtype=\\\"int64\\\")\\none_ykl_class_df = one_ykl_class_df.reset_index(drop=True)\\n\\n# Create\\nfor i in range(len(one_ykl_class_df)):\\n    item_keywords_str = (str)(one_ykl_class_df.iloc[(i), 6])\\n    item_keywords_lst = item_keywords_str.split(\\\",\\\")\\n    for word in item_keywords_lst:\\n        word = word.strip().lower()\\n        one_ykl_class_df.at[i, word] = 1\\n\\n# Drop column with comma-separated keywords and fill NaN with 0\\none_ykl_class_df = one_ykl_class_df.drop([\\\"650\\\"], axis=1)\\none_ykl_class_df = one_ykl_class_df.fillna(0)\\n\\n# Sanity check: printing final dataset for training and validating the model\\nprint(f\\\"\\\\n*****\\\")\\nprint(\\n    f\\\"Final dataset: {one_ykl_class_df.shape[0]} rows with YKL-CLASS={item_ykl_class}\\\"\\n)\\nprint(f\\\"Number of keywords in final dataset is: {one_ykl_class_df.shape[1] - 6}\\\")\\nprint(f\\\"*****\\\\n\\\")\\none_ykl_class_df.head()\";\n",
       "                var nbb_formatted_code = \"# Restrict the trainingset size if necessary\\nMAX_TRAININGSET_SIZE = 3000\\n\\n# Select a random YKL-class get all rows matching it\\nsample_row = df2.sample(frac=1)\\nitem_ykl_class = sample_row.iloc[0][\\\"084\\\"]\\none_ykl_class_df = df2.loc[df2[\\\"084\\\"] == item_ykl_class]\\none_ykl_class_df = one_ykl_class_df.reset_index(drop=True)\\n\\n# Read all distinct keywords into an array\\nkeywords = []\\nfor i in range(len(one_ykl_class_df)):\\n    item_keywords_str = (str)(one_ykl_class_df.iloc[(i), 6])\\n    item_keywords_lst = item_keywords_str.split(\\\",\\\")\\n    for word in item_keywords_lst:\\n        word = word.strip().lower()\\n        if word not in keywords:\\n            keywords.append(word)\\n\\n# Sanity check printing\\nprint(f\\\"\\\\n*****\\\")\\nprint(f\\\"Found {one_ykl_class_df.shape[0]} rows with YKL-CLASS={item_ykl_class}\\\")\\nprint(f\\\"Number of keywords: {len(keywords)}\\\")\\nprint(f\\\"*****\\\\n\\\")\\n\\n# Restrict the training/validation set MAX_TRAININGSET_SIZE rows\\nif one_ykl_class_df.shape[0] < MAX_TRAININGSET_SIZE:\\n    MAX_TRAININGSET_SIZE = one_ykl_class_df.shape[0]\\none_ykl_class_df = one_ykl_class_df.sample(MAX_TRAININGSET_SIZE)\\n\\n# Add keyword columns with keyword as a title (value will be 0 or 1 depending if the keyword belongs to the volume or not)\\n# NOTE: \\\"A Pandas Series is like a column in a table\\\" (https://www.w3schools.com/python/pandas/pandas_series.asp)\\nfor i in range(len(keywords)):\\n    one_ykl_class_df[keywords[i]] = pd.Series([], dtype=\\\"int64\\\")\\none_ykl_class_df = one_ykl_class_df.reset_index(drop=True)\\n\\n# Create\\nfor i in range(len(one_ykl_class_df)):\\n    item_keywords_str = (str)(one_ykl_class_df.iloc[(i), 6])\\n    item_keywords_lst = item_keywords_str.split(\\\",\\\")\\n    for word in item_keywords_lst:\\n        word = word.strip().lower()\\n        one_ykl_class_df.at[i, word] = 1\\n\\n# Drop column with comma-separated keywords and fill NaN with 0\\none_ykl_class_df = one_ykl_class_df.drop([\\\"650\\\"], axis=1)\\none_ykl_class_df = one_ykl_class_df.fillna(0)\\n\\n# Sanity check: printing final dataset for training and validating the model\\nprint(f\\\"\\\\n*****\\\")\\nprint(\\n    f\\\"Final dataset: {one_ykl_class_df.shape[0]} rows with YKL-CLASS={item_ykl_class}\\\"\\n)\\nprint(f\\\"Number of keywords in final dataset is: {one_ykl_class_df.shape[1] - 6}\\\")\\nprint(f\\\"*****\\\\n\\\")\\none_ykl_class_df.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Restrict the trainingset size if necessary\n",
    "MAX_TRAININGSET_SIZE = 3000\n",
    "\n",
    "# Select a random YKL-class get all rows matching it\n",
    "sample_row = df2.sample(frac=1)\n",
    "item_ykl_class = sample_row.iloc[0][\"084\"]\n",
    "one_ykl_class_df = df2.loc[df2[\"084\"] == item_ykl_class]\n",
    "one_ykl_class_df = one_ykl_class_df.reset_index(drop=True)\n",
    "\n",
    "# Read all distinct keywords into an array\n",
    "keywords = []\n",
    "for i in range(len(one_ykl_class_df)):\n",
    "    item_keywords_str = (str)(one_ykl_class_df.iloc[(i), 6])\n",
    "    item_keywords_lst = item_keywords_str.split(\",\")\n",
    "    for word in item_keywords_lst:\n",
    "        word = word.strip().lower()\n",
    "        if word not in keywords:\n",
    "            keywords.append(word)\n",
    "\n",
    "# Sanity check printing\n",
    "print(f\"\\n*****\")\n",
    "print(f\"Found {one_ykl_class_df.shape[0]} rows with YKL-CLASS={item_ykl_class}\")\n",
    "print(f\"Number of keywords: {len(keywords)}\")\n",
    "print(f\"*****\\n\")\n",
    "\n",
    "# Restrict the training/validation set MAX_TRAININGSET_SIZE rows\n",
    "if one_ykl_class_df.shape[0] < MAX_TRAININGSET_SIZE:\n",
    "    MAX_TRAININGSET_SIZE = one_ykl_class_df.shape[0]\n",
    "one_ykl_class_df = one_ykl_class_df.sample(MAX_TRAININGSET_SIZE)\n",
    "\n",
    "# Add keyword columns with keyword as a title (value will be 0 or 1 depending if the keyword belongs to the volume or not)\n",
    "# NOTE: \"A Pandas Series is like a column in a table\" (https://www.w3schools.com/python/pandas/pandas_series.asp)\n",
    "for i in range(len(keywords)):\n",
    "    one_ykl_class_df[keywords[i]] = pd.Series([], dtype=\"int64\")\n",
    "one_ykl_class_df = one_ykl_class_df.reset_index(drop=True)\n",
    "\n",
    "# Create\n",
    "for i in range(len(one_ykl_class_df)):\n",
    "    item_keywords_str = (str)(one_ykl_class_df.iloc[(i), 6])\n",
    "    item_keywords_lst = item_keywords_str.split(\",\")\n",
    "    for word in item_keywords_lst:\n",
    "        word = word.strip().lower()\n",
    "        one_ykl_class_df.at[i, word] = 1\n",
    "\n",
    "# Drop column with comma-separated keywords and fill NaN with 0\n",
    "one_ykl_class_df = one_ykl_class_df.drop([\"650\"], axis=1)\n",
    "one_ykl_class_df = one_ykl_class_df.fillna(0)\n",
    "\n",
    "# Sanity check: printing final dataset for training and validating the model\n",
    "print(f\"\\n*****\")\n",
    "print(\n",
    "    f\"Final dataset: {one_ykl_class_df.shape[0]} rows with YKL-CLASS={item_ykl_class}\"\n",
    ")\n",
    "print(f\"Number of keywords in final dataset is: {one_ykl_class_df.shape[1] - 6}\")\n",
    "print(f\"*****\\n\")\n",
    "one_ykl_class_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "    **HUOM!**\n",
    "\n",
    "    Template on testattu tähän soluun asti. Loppu, lukuunottamatta lopussa olevia soluja on vielä siivoamatta/refaktoroimatta.\n",
    "\n",
    "    DataFrame jota voi käyttää tämän solun jälkeen: \"df2\" tai \"one_ykl_class_df\", joka on lopullinen Training/Validation datasetti, ja jossa on mukana vain yksi yhden YKL luokan rivit.\n",
    "    \n",
    "    Notebookin lopussa on siis toimivia ja  testattuja soluja, joissa vielä viimeitellään lopullinen dataset ja toy dataset ja tallennetaan ne csv-muodossa tiedostoon. \n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT USE THIS CELL - IT IS NOT WORKING!!!!!\n",
    "\n",
    "##df2 = df1[:]\n",
    "# df2 = df_cut[:]\n",
    "# df2 = df_uncut[:]\n",
    "df2 = df_F[:]\n",
    "# df2['084'] = pd.to_numeric(df2[\"084\"], errors='coerce')\n",
    "# df2['084'] = df2['084'].astype(str)\n",
    "c1 = df2.sort_values(by=[\"084\"])\n",
    "c2 = c1[\"084\"].unique()\n",
    "# c1.sort_values()\n",
    "# \"\"\"\n",
    "print(len(c2))\n",
    "# print(c2)\n",
    "# \"\"\"\n",
    "\n",
    "print(\"NV: \", notations.values)\n",
    "\"\"\"\n",
    "for row in c2[:10]:\n",
    "    # print(type(row))\n",
    "    if row not in notations.values:\n",
    "        print(row)\n",
    "\"\"\"\n",
    "\n",
    "c3 = c1[~c1[\"084\"].isin(notations)]\n",
    "\n",
    "print(c3[\"084\"].isnull().sum())\n",
    "\n",
    "measurer = np.vectorize(len)\n",
    "res1 = measurer(df2.values.astype(str)).max(axis=0)\n",
    "print(\"Measures: \", res1)\n",
    "\n",
    "df2 = df2[(df2[\"084\"].notnull())]\n",
    "\n",
    "df2[\"084_length\"] = df2[\"084\"].str.len()\n",
    "# s = df2[\"084\"].str.len().sort_values()\n",
    "# df2.sort_values(\"084_length\", ascending=True, inplace=True)\n",
    "# print(df2.head(2000))\n",
    "\n",
    "luokat_lkm = df2[\"084\"].value_counts().nlargest(10)\n",
    "print(luokat_lkm)\n",
    "luokat_lkm.plot(kind=\"bar\")\n",
    "\n",
    "\"\"\"\n",
    "# s_temp = luokat_lkm\n",
    "s_temp = df2[\"084_length\"].sort_values(ascending=False)\n",
    "\n",
    "# Count how many rows are not in the top ten\n",
    "not_top_ten = len(s_temp) - 10\n",
    "print(not_top_ten)\n",
    "\n",
    "# Sum the values not in the top ten\n",
    "not_top_ten_sum = s_temp.tail(not_top_ten).sum()\n",
    "print(not_top_ten_sum)\n",
    "\n",
    "# Get the top ten values\n",
    "s_top = s_temp.head(10)\n",
    "# s_top = s_temp[:10]\n",
    "print(s_top)\n",
    "\n",
    "# Append the sum of not-top-ten values to the Series\n",
    "s_top[10] = not_top_ten_sum\n",
    "# s_top.append(not_top_ten_sum)\n",
    "\n",
    "# Plot pie chart\n",
    "_ = s_top.plot.pie()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# hist = df2.hist(column=\"084_length\", bins=7)\n",
    "# hist = luokat_lkm.hist()\n",
    "# plot = luokat_lkm.plot.pie()\n",
    "\n",
    "# df2.to_csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "31032\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"# Read all distinct keywoards into an array\\nkeywords = []\\n# for i in range(len(df1[:10])):\\nfor index, row in df2.iterrows():\\n    # print(i)\\n    # k = df1.loc[i+1, \\\"650\\\"]\\n    try:\\n        kwords = row[\\\"650\\\"].split(\\\",\\\")\\n    except Exception as e:\\n        # print(row[\\\"650\\\"])\\n        kwords = None\\n    if kwords:\\n        for k in kwords:\\n            if k not in keywords:\\n                keywords.append(k)\\n        # print(index, row[\\\"650\\\"])\\n    # if index > 500000:\\n    #    break\\nprint(\\\"---\\\")\\n# print(keywords)\\nprint(len(keywords))\";\n",
       "                var nbb_formatted_code = \"# Read all distinct keywoards into an array\\nkeywords = []\\n# for i in range(len(df1[:10])):\\nfor index, row in df2.iterrows():\\n    # print(i)\\n    # k = df1.loc[i+1, \\\"650\\\"]\\n    try:\\n        kwords = row[\\\"650\\\"].split(\\\",\\\")\\n    except Exception as e:\\n        # print(row[\\\"650\\\"])\\n        kwords = None\\n    if kwords:\\n        for k in kwords:\\n            if k not in keywords:\\n                keywords.append(k)\\n        # print(index, row[\\\"650\\\"])\\n    # if index > 500000:\\n    #    break\\nprint(\\\"---\\\")\\n# print(keywords)\\nprint(len(keywords))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read all distinct keywoards into an array\n",
    "keywords = []\n",
    "# for i in range(len(df1[:10])):\n",
    "for index, row in df2.iterrows():\n",
    "    # print(i)\n",
    "    # k = df1.loc[i+1, \"650\"]\n",
    "    try:\n",
    "        kwords = row[\"650\"].split(\",\")\n",
    "    except Exception as e:\n",
    "        # print(row[\"650\"])\n",
    "        kwords = None\n",
    "    if kwords:\n",
    "        for k in kwords:\n",
    "            if k not in keywords:\n",
    "                keywords.append(k)\n",
    "        # print(index, row[\"650\"])\n",
    "    # if index > 500000:\n",
    "    #    break\n",
    "print(\"---\")\n",
    "# print(keywords)\n",
    "print(len(keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, all columns that contain 'object' variables, contain elements that can not be described in sigle datatype.\n",
    "These may be nested structures or mixed datatypes. We saw earlier that there were a lot of question marks in the dataset.\n",
    "Let's try to replace these with np.nan, meaning a missing value, and then converting the columns with 'object' datatype to numericals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace '?' with np nan\n",
    "df.replace(\"?\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert columns with 'object' type to float\n",
    "df = df.astype(\n",
    "    {\n",
    "        \"x4trestbps\": float,\n",
    "        \"x6fbs\": float,\n",
    "        \"x7restecg\": float,\n",
    "        \"x8thalach\": float,\n",
    "        \"x9exang\": float,\n",
    "        \"x10oldpeak\": float,\n",
    "        \"x11slope\": float,\n",
    "        \"x12ca\": float,\n",
    "        \"x13thal\": float,\n",
    "    }\n",
    ")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now there are no 'object' datatypes - if there were, we would know that there are still some malicius values in the cells that would have to be cleaned off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, we want to omit all categorical features in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the features \"x2sex\", \"x3cp\", \"x5chol\", \"x7restecg\" and \"x9exang\" only contain limited number of values,\n",
    " so we assume them categorical. These, we can drop from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns by name\n",
    "df = df.drop(\n",
    "    [\"x2sex\", \"x3cp\", \"x5chol\", \"x7restecg\", \"x9exang\"],\n",
    "    axis=1,  # drop columns with categorical variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how much of the data is missing in each variable left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count nan values per column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok,  there are many missing variables.\n",
    "\n",
    "Now we have two alternatives: we can either get rid of the missing values, or use such robust methods that allow missing values in the data.\n",
    "\n",
    "For removing the missing values we can either drop the columns with most missing data or try to impute the missing values from the data.\n",
    "\n",
    "If there was very little data, we would most likely want to impute the missing data,\n",
    "for example with use the Scikit-learn imputer: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.\n",
    "If you do imputing of missing data, please consider the effects on data quality.\n",
    "\n",
    "In this example, we just drop the columns with most na data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.drop(\n",
    "        [\"x6fbs\", \"x11slope\", \"x12ca\", \"x13thal\"],  # drop columns with most na values\n",
    "        axis=1,\n",
    "    )\n",
    "    .dropna()\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data\n",
    "\n",
    "Once the dataset is looking clean, it is time to visualize it.\n",
    "Visualization is probably the most powerful tool of data science.\n",
    "\n",
    "Visualization depends highly on the data, but usually you should begin by looking at two things: distribution and correlation.\n",
    "\n",
    "Let's make histograms of the variables, and a trellis of scatterplots for visualizing correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# ^(this cell is exported to ml_project_template/ml_project_template/data.py)\n",
    "# you could also define another module to export to.\n",
    "# however, all modules that you export to, must have a notebook with the same name and header!\n",
    "\n",
    "# function for drawing histograms of a dataframe\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_histogram(df):\n",
    "    \"\"\"\n",
    "    Plot histograms of a dataframe\n",
    "    \"\"\"\n",
    "    n_cols = len(df.columns)\n",
    "    col_names = df.columns.values\n",
    "    fig, axs = plt.subplots(\n",
    "        1, n_cols, figsize=(3 * n_cols, 3), constrained_layout=True, sharey=\"row\"\n",
    "    )\n",
    "    for i in range(n_cols):\n",
    "        ax = axs[i]\n",
    "        col_name = col_names[i]\n",
    "        x = df[col_name]\n",
    "        x.plot(ax=ax, kind=\"hist\")\n",
    "        xmin, xmax = min(x), max(x)\n",
    "    for i in range(n_cols):  # to have equal tick lines in each plot\n",
    "        ax = axs[i]\n",
    "        col_name = col_names[i]\n",
    "        x = df[col_name]\n",
    "        xmin, xmax = min(x), max(x)\n",
    "        ax.hlines(\n",
    "            y=axs[df.apply(lambda x: x.max()).argmin()].get_yticks()[\n",
    "                1:-1\n",
    "            ],  # select ticks from the fig with smallest max value\n",
    "            xmin=xmin,\n",
    "            xmax=xmax,\n",
    "            colors=\"white\",\n",
    "            alpha=1,\n",
    "            linewidth=2,\n",
    "        )\n",
    "        ax.set_xlabel(col_name)\n",
    "        ax.set_title(f\"{i})\", loc=\"left\")\n",
    "        ax.spines[\"left\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histogram of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_histogram(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot 4) we see the distribution of different heart disease diagnosis.\n",
    "Remember, that the diagnosis 0 is negative, meaning no heart disease, and the rest are positive, meaning different heart diagnoses.\n",
    "For sake of simplicity, we will replace all different positive diagnoses with a one positive indicator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace different heart diagnoses with value 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.y1num.replace([1, 2, 3, 4], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the distributions again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_histogram(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a trellis plot of the features. Now we can visualize the diagnosis result with the glyph (color and shape of the marker).\n",
    "\n",
    "But wait, don't we just want to plot each of the features against the label?\n",
    "You could do that, too, but in the initial analysis phase it is a good practice to compare all variables against each other.\n",
    "It can reveal all kinds of interesting correlations that may affect your choises later.\n",
    "\n",
    "This will make quite a plot to digest, I agree.\n",
    "For a raport or a presentation you might want to focus on just a few features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# Make a multiple of scatter plots\n",
    "def plot_trellis(df, legend_title=\"y\", true_label=\"True\", false_label=\"False\"):\n",
    "    \"\"\"\n",
    "    Make a trellis plot of a dataframe against a binary y value in last column\n",
    "    \"\"\"\n",
    "    n_cols = df.shape[1] - 1\n",
    "    col_names = df.columns.values\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        n_cols, n_cols, figsize=(n_cols * 3, n_cols * 3), constrained_layout=True\n",
    "    )\n",
    "    for i in range(n_cols):\n",
    "        for j in range(n_cols):\n",
    "            ax = axs[i, j]\n",
    "            if i != j:\n",
    "                # first plot negative cases\n",
    "                df[df.iloc[:, -1] == 0].plot(\n",
    "                    ax=ax,\n",
    "                    x=col_names[j],\n",
    "                    y=col_names[i],\n",
    "                    kind=\"scatter\",\n",
    "                    color=\"b\",\n",
    "                    marker=\"o\",\n",
    "                    alpha=0.5,\n",
    "                    label=false_label,\n",
    "                )\n",
    "                # then positive cases\n",
    "                df[df.iloc[:, -1] != 0].plot(\n",
    "                    ax=ax,\n",
    "                    x=col_names[j],\n",
    "                    y=col_names[i],\n",
    "                    kind=\"scatter\",\n",
    "                    color=\"r\",\n",
    "                    marker=\"x\",\n",
    "                    alpha=0.5,\n",
    "                    label=true_label,\n",
    "                )\n",
    "                # Hide the right and top spines\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.set_title(f\"{i*n_cols+j})\", loc=\"left\")\n",
    "                ax.legend(title=legend_title)\n",
    "            else:\n",
    "                ax.annotate(xy=(0, 0.5), text=col_names[i], fontsize=20)\n",
    "                ax.axis(\"off\")  # hide the box\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_trellis(\n",
    "    df, legend_title=\"diagnose\", true_label=\"positive\", false_label=\"negative\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "    **HUOM!**\n",
    "\n",
    "    Tästä eteenpäin taas toimivaa koodia. Viimeistellään lopullinen training/validation datasetti ja toy datasetti ja tallennetaan sisältö seuraaviin tiedostoihin:\n",
    "    - \"data/preprocessed_data/dataset_clean_hki_lib_book_classification.csv\"\n",
    "    - \"data/preprocessed_data/dataset_toy_hki_lib_book_classification.csv\"\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some intermediate conclusions based on data visualization\n",
    "\n",
    "We are interested in science-classifications mostly (how to separate them?).\n",
    "\n",
    "The trellis plotting of features can probably be interpreted so that there is very strong correlation between classifications.\n",
    "\n",
    "NOTE: We can also see that there are some outliers. We should find these values and maybe get rid of them (perhaps replace them with NaN or zero). Or is it possible, that these outliers are the ones that ML-algorithm classification is of most help?\n",
    "\n",
    " - [Outliers with scikit-learn](http://napitupulu-jon.appspot.com/posts/outliers-ud120.html)\n",
    " - [How should outliers be dealt with in linear regression analysis?](https://stats.stackexchange.com/questions/175/how-should-outliers-be-dealt-with-in-linear-regression-analysis)\n",
    " - [What is the difference between linearly dependent and linearly correlated?](https://stats.stackexchange.com/questions/31270/what-is-the-difference-between-linearly-dependent-and-linearly-correlated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suffle Dataset\n",
    "\n",
    "Before saving the dataset for further use, it is a good practice to suffle it.\n",
    "This is, of course, assuming the order of the data is not meaningful (as with for example time series data).\n",
    "\n",
    "Random samplin large datasets consumes time and resources, so if you have a pre-suffled dataset you can just read in more data that is already randomized.\n",
    "Also, quite often datasets are intentionally or carelessly saved with some obvious or latent order that might include odd biases to your further analysis.\n",
    "Suffling helps you to get rid of these. Suffling a large dataset may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>084</th>\n",
       "      <th>092</th>\n",
       "      <th>093</th>\n",
       "      <th>094</th>\n",
       "      <th>095</th>\n",
       "      <th>rock</th>\n",
       "      <th>popmusiikki</th>\n",
       "      <th>kokoelmat</th>\n",
       "      <th>tabulatuurinuottikirjoitus</th>\n",
       "      <th>...</th>\n",
       "      <th>heavy metal</th>\n",
       "      <th>säveltäminen</th>\n",
       "      <th>huuliharppu</th>\n",
       "      <th>rockfestivaalit</th>\n",
       "      <th>ruisrock</th>\n",
       "      <th>ilosaarirock</th>\n",
       "      <th>tapahtumat</th>\n",
       "      <th>elävä musiikki</th>\n",
       "      <th>tyylit</th>\n",
       "      <th>hakuteokset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>420908696117</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>788.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>420908412353</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>788.335</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>420907962293</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>788.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>420908458859</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>0</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>788.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>420909057565</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>788.335</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      record_id      084      092      093      094      095  rock  \\\n",
       "0  420908696117  78.8911        0        0        0   788.33   1.0   \n",
       "1  420908412353  78.8911  78.8911        0        0  788.335   1.0   \n",
       "2  420907962293  78.8911  78.8911  78.8911  78.8911   788.33   1.0   \n",
       "3  420908458859  78.8911  78.8911        0  78.8911   788.33   1.0   \n",
       "4  420909057565  78.8911        0        0  78.8911  788.335   1.0   \n",
       "\n",
       "   popmusiikki  kokoelmat  tabulatuurinuottikirjoitus  ...  heavy metal  \\\n",
       "0          0.0        1.0                         0.0  ...          0.0   \n",
       "1          0.0        1.0                         0.0  ...          0.0   \n",
       "2          1.0        0.0                         0.0  ...          0.0   \n",
       "3          0.0        0.0                         0.0  ...          0.0   \n",
       "4          0.0        0.0                         0.0  ...          0.0   \n",
       "\n",
       "   säveltäminen  huuliharppu  rockfestivaalit  ruisrock  ilosaarirock  \\\n",
       "0           0.0          0.0              0.0       0.0           0.0   \n",
       "1           0.0          0.0              0.0       0.0           0.0   \n",
       "2           0.0          0.0              0.0       0.0           0.0   \n",
       "3           0.0          0.0              0.0       0.0           0.0   \n",
       "4           0.0          0.0              0.0       0.0           0.0   \n",
       "\n",
       "   tapahtumat  elävä musiikki  tyylit  hakuteokset  \n",
       "0         0.0             0.0     0.0          0.0  \n",
       "1         0.0             0.0     0.0          0.0  \n",
       "2         0.0             0.0     0.0          0.0  \n",
       "3         0.0             0.0     0.0          0.0  \n",
       "4         0.0             0.0     0.0          0.0  \n",
       "\n",
       "[5 rows x 371 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 50;\n",
       "                var nbb_unformatted_code = \"# to suffle the dataset sample a fraction of 1 from it\\none_ykl_class_df = one_ykl_class_df.sample(frac=1).reset_index(\\n    drop=True\\n)  # suffle and re-index dataset\\none_ykl_class_df.head()\";\n",
       "                var nbb_formatted_code = \"# to suffle the dataset sample a fraction of 1 from it\\none_ykl_class_df = one_ykl_class_df.sample(frac=1).reset_index(\\n    drop=True\\n)  # suffle and re-index dataset\\none_ykl_class_df.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to suffle the dataset sample a fraction of 1 from it\n",
    "one_ykl_class_df = one_ykl_class_df.sample(frac=1).reset_index(\n",
    "    drop=True\n",
    ")  # suffle and re-index dataset\n",
    "one_ykl_class_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save clean and tidy data for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 51;\n",
       "                var nbb_unformatted_code = \"# save the dataset to output path defined in the beginning of this notebook\\none_ykl_class_df.to_csv(output_clean_filepath)\";\n",
       "                var nbb_formatted_code = \"# save the dataset to output path defined in the beginning of this notebook\\none_ykl_class_df.to_csv(output_clean_filepath)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save the dataset to output path defined in the beginning of this notebook\n",
    "one_ykl_class_df.to_csv(output_clean_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create small toy dataset for developing and testing the ML methods\n",
    "\n",
    "Best way to test your ML methods in development is with data as close to the real data as possible.\n",
    "However, to save you from frustrating long runtimes in development, it is better to create a small sample dataset.\n",
    "You can also create unit tests with this sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****\n",
      "Toy dataset size: 1000 rows and 371 columns\n",
      "*****\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>084</th>\n",
       "      <th>092</th>\n",
       "      <th>093</th>\n",
       "      <th>094</th>\n",
       "      <th>095</th>\n",
       "      <th>rock</th>\n",
       "      <th>popmusiikki</th>\n",
       "      <th>kokoelmat</th>\n",
       "      <th>tabulatuurinuottikirjoitus</th>\n",
       "      <th>...</th>\n",
       "      <th>heavy metal</th>\n",
       "      <th>säveltäminen</th>\n",
       "      <th>huuliharppu</th>\n",
       "      <th>rockfestivaalit</th>\n",
       "      <th>ruisrock</th>\n",
       "      <th>ilosaarirock</th>\n",
       "      <th>tapahtumat</th>\n",
       "      <th>elävä musiikki</th>\n",
       "      <th>tyylit</th>\n",
       "      <th>hakuteokset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>420909179575</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>0</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>788.335</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>420908618411</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>788.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>420908728550</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>78.89113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>788.334</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>420908846341</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>788.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2735</th>\n",
       "      <td>420908440456</td>\n",
       "      <td>78.8911</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>788.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         record_id      084       092 093      094      095  rock  \\\n",
       "1396  420909179575  78.8911   78.8911   0  78.8911  788.335   1.0   \n",
       "349   420908618411  78.8911   78.8911   0        0   788.33   1.0   \n",
       "1378  420908728550  78.8911  78.89113   0        0  788.334   1.0   \n",
       "630   420908846341  78.8911   78.8911   0        0   788.33   1.0   \n",
       "2735  420908440456  78.8911         0   0        0   788.33   1.0   \n",
       "\n",
       "      popmusiikki  kokoelmat  tabulatuurinuottikirjoitus  ...  heavy metal  \\\n",
       "1396          0.0        0.0                         0.0  ...          0.0   \n",
       "349           0.0        1.0                         0.0  ...          0.0   \n",
       "1378          0.0        0.0                         0.0  ...          0.0   \n",
       "630           0.0        0.0                         0.0  ...          0.0   \n",
       "2735          0.0        0.0                         0.0  ...          0.0   \n",
       "\n",
       "      säveltäminen  huuliharppu  rockfestivaalit  ruisrock  ilosaarirock  \\\n",
       "1396           0.0          0.0              0.0       0.0           0.0   \n",
       "349            0.0          0.0              0.0       0.0           0.0   \n",
       "1378           0.0          0.0              0.0       0.0           0.0   \n",
       "630            0.0          0.0              0.0       0.0           0.0   \n",
       "2735           0.0          0.0              0.0       0.0           0.0   \n",
       "\n",
       "      tapahtumat  elävä musiikki  tyylit  hakuteokset  \n",
       "1396         0.0             0.0     0.0          0.0  \n",
       "349          0.0             0.0     0.0          0.0  \n",
       "1378         0.0             0.0     0.0          0.0  \n",
       "630          0.0             0.0     0.0          0.0  \n",
       "2735         0.0             0.0     0.0          0.0  \n",
       "\n",
       "[5 rows x 371 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 52;\n",
       "                var nbb_unformatted_code = \"# create simple stratified sample of the notebook\\nn_toy = 1000  # number of samples\\nn_toy = min(one_ykl_class_df.shape[0], n_toy)\\n\\n# toy_df = (\\n#    one_ykl_class_df.groupby(\\\"record_id\\\", group_keys=False)\\n#    .apply(\\n#        lambda x: x.sample(int(np.rint(n_toy * x.shape[0] / one_ykl_class_df.shape[0])))\\n#    )\\n#    .sample(frac=1)\\n#    .reset_index(drop=True)\\n# )\\n\\n# alternatively:\\ntoy_df = one_ykl_class_df.sample(\\n    frac=n_toy / one_ykl_class_df.shape[0]\\n)  # if simple stratification is not applicable\\n# toy_df = df.iloc[:n_toy,:] # with time series data. consider the size of n_toy\\n\\n\\nprint(f\\\"\\\\n*****\\\")\\nprint(f\\\"Toy dataset size: {toy_df.shape[0]} rows and {toy_df.shape[1]} columns\\\")\\nprint(f\\\"*****\\\\n\\\")\\ntoy_df.head()\";\n",
       "                var nbb_formatted_code = \"# create simple stratified sample of the notebook\\nn_toy = 1000  # number of samples\\nn_toy = min(one_ykl_class_df.shape[0], n_toy)\\n\\n# toy_df = (\\n#    one_ykl_class_df.groupby(\\\"record_id\\\", group_keys=False)\\n#    .apply(\\n#        lambda x: x.sample(int(np.rint(n_toy * x.shape[0] / one_ykl_class_df.shape[0])))\\n#    )\\n#    .sample(frac=1)\\n#    .reset_index(drop=True)\\n# )\\n\\n# alternatively:\\ntoy_df = one_ykl_class_df.sample(\\n    frac=n_toy / one_ykl_class_df.shape[0]\\n)  # if simple stratification is not applicable\\n# toy_df = df.iloc[:n_toy,:] # with time series data. consider the size of n_toy\\n\\n\\nprint(f\\\"\\\\n*****\\\")\\nprint(f\\\"Toy dataset size: {toy_df.shape[0]} rows and {toy_df.shape[1]} columns\\\")\\nprint(f\\\"*****\\\\n\\\")\\ntoy_df.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create simple stratified sample of the notebook\n",
    "n_toy = 1000  # number of samples\n",
    "n_toy = min(one_ykl_class_df.shape[0], n_toy)\n",
    "\n",
    "# toy_df = (\n",
    "#    one_ykl_class_df.groupby(\"record_id\", group_keys=False)\n",
    "#    .apply(\n",
    "#        lambda x: x.sample(int(np.rint(n_toy * x.shape[0] / one_ykl_class_df.shape[0])))\n",
    "#    )\n",
    "#    .sample(frac=1)\n",
    "#    .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "# alternatively:\n",
    "toy_df = one_ykl_class_df.sample(\n",
    "    frac=n_toy / one_ykl_class_df.shape[0]\n",
    ")  # if simple stratification is not applicable\n",
    "# toy_df = df.iloc[:n_toy,:] # with time series data. consider the size of n_toy\n",
    "\n",
    "\n",
    "print(f\"\\n*****\")\n",
    "print(f\"Toy dataset size: {toy_df.shape[0]} rows and {toy_df.shape[1]} columns\")\n",
    "print(f\"*****\\n\")\n",
    "toy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 53;\n",
       "                var nbb_unformatted_code = \"# save toy dataset\\ntoy_df.to_csv(output_toy_filepath)\";\n",
       "                var nbb_formatted_code = \"# save toy dataset\\ntoy_df.to_csv(output_toy_filepath)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save toy dataset\n",
    "toy_df.to_csv(output_toy_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can now move on to the model notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (ml-sami-libclas)",
   "language": "python",
   "name": "ml-sami-libclas"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
